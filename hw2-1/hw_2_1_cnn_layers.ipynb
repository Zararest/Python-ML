{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ3DJzu7FDX_"
      },
      "source": [
        "# Домашнее задание 2.1. Сверточные сети\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQVARDpwNsOa"
      },
      "source": [
        "В этом задании вы должны:\n",
        "1. Написать слой Conv2d на Numpy и определить в нем forward-backward методы\n",
        "2. Определить слой MaxPool2d\n",
        "3. Написать всю необходимую обвязку для обучения: оптимизатор с адаптивным шагом и класс, позволяющий изменять расписание для learning rate'а\n",
        "\n",
        "\n",
        "\n",
        "> Обратите внимание, что в этом задании больше нет тестов.\n",
        "> Вы должны сами проверять свой код.  \n",
        "> Это можно сделать так:\n",
        "> 1. Написать юнит-тесты с помощью Pytorch. То есть, ваш модудь должен повторять поведение torch'а\n",
        "> 2. Проверять архитектуру не на всем датасете, а на подвыборке: при наивной имплементации слоев одна эпоха на всем датасете будет занимать около двух часов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMVkqpoEOoD3",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Numpy-имплементация сверточной нейронной сети\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmTxOp7KOoEG",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Вставьте сюда имплементацию из первого домашнего задания.\n",
        "\n",
        "\n",
        "\n",
        "> Обратите внимание, что обновление весов теперь производится с помощью специального класса **Optimizer**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lmfLBp4tOoEN",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "def load_mnist(flatten=False):\n",
        "    \"\"\"taken from https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py\"\"\"\n",
        "    # We first define a download function, supporting both Python 2 and 3.\n",
        "    if sys.version_info[0] == 2:\n",
        "        from urllib import urlretrieve\n",
        "    else:\n",
        "        from urllib.request import urlretrieve\n",
        "\n",
        "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
        "        print(\"Downloading %s\" % filename)\n",
        "        urlretrieve(source + filename, filename)\n",
        "\n",
        "    # We then define functions for loading MNIST images and labels.\n",
        "    # For convenience, they also download the requested files if needed.\n",
        "    import gzip\n",
        "\n",
        "    def load_mnist_images(filename):\n",
        "        if not os.path.exists(filename):\n",
        "            download(filename)\n",
        "        # Read the inputs in Yann LeCun's binary format.\n",
        "        with gzip.open(filename, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
        "        # following the shape convention: (examples, channels, rows, columns)\n",
        "        data = data.reshape(-1, 1, 28, 28)\n",
        "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
        "        # (Actually to range [0, 255/256], for compatibility to the version\n",
        "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
        "        return data / np.float32(256)\n",
        "\n",
        "    def load_mnist_labels(filename):\n",
        "        if not os.path.exists(filename):\n",
        "            download(filename)\n",
        "        # Read the labels in Yann LeCun's binary format.\n",
        "        with gzip.open(filename, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "        # The labels are vectors of integers now, that's exactly what we want.\n",
        "        return data\n",
        "\n",
        "    # We can now download and read the training and test set images and labels.\n",
        "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
        "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
        "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
        "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "    # We reserve the last 10000 training examples for validation.\n",
        "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
        "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
        "\n",
        "    if flatten:\n",
        "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
        "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
        "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
        "\n",
        "    # We just return all the arrays in order, as expected in main().\n",
        "    # (It doesn't matter how we do this as long as we can read them again.)\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jYZ3ptaiOoER",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    \"\"\"\n",
        "    A building block. Each layer is capable of performing two things:\n",
        "\n",
        "    - Process input to get output:           output = layer.forward(input)\n",
        "\n",
        "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
        "\n",
        "    Some layers also have learnable parameters which they update during layer.backward.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"Here you can initialize layer parameters (if any) and auxiliary stuff.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n",
        "        \"\"\"\n",
        "        return input\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the layer, with respect to the given input.\n",
        "\n",
        "        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n",
        "\n",
        "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
        "\n",
        "        Luckily, you already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
        "\n",
        "        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss / d layer\n",
        "        \"\"\"\n",
        "        input_dim = input.shape[1]\n",
        "\n",
        "        d_layer_d_input = np.eye(input_dim)\n",
        "\n",
        "        return np.dot(grad_output, d_layer_d_input) # chain rule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Qe246l61OoEe",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
        "        pass\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Apply elementwise ReLU to [batch, input_units] matrix\"\"\"\n",
        "        output = np.array(input)\n",
        "        output[output < 0] = 0\n",
        "        return output\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"Compute gradient of loss w.r.t. ReLU input\"\"\"\n",
        "        relu_grad_mask = np.zeros_like(input)\n",
        "        relu_grad_mask[input > 0] = 1\n",
        "        return grad_output * relu_grad_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g22Gzs_2OoEl",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class Dense(Layer):\n",
        "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        A dense layer is a layer which performs a learned affine transformation:\n",
        "        f(x) = <W*x> + b\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # initialize weights with small random numbers from normal distribution\n",
        "        # you can change the intializtion method\n",
        "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
        "        self.biases = np.zeros(output_units)\n",
        "\n",
        "    def forward(self,input):\n",
        "        \"\"\"\n",
        "        Perform an affine transformation:\n",
        "        f(x) = <W*x> + b\n",
        "\n",
        "        input shape: [batch, input_units]\n",
        "        output shape: [batch, output_units]\n",
        "        \"\"\"\n",
        "        return np.dot(input, self.weights) + self.biases\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "\n",
        "        # compute d f / d x = d f / d dense * d dense / d x\n",
        "        # where d dense/ d x = weights transposed\n",
        "        # grad_output is a derivative of the next (in forward) layer of prediction\n",
        "        # this result is needed for the next layer\n",
        "        grad_input = np.dot(grad_output, np.transpose(self.weights))\n",
        "\n",
        "        # compute gradient w.r.t. weights and biases\n",
        "        grad_weights = np.dot(np.transpose(input), grad_output)\n",
        "        grad_biases = np.sum(grad_output, axis=0)\n",
        "\n",
        "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
        "        # Here we perform a stochastic gradient descent step.\n",
        "        # or step of another gradient method\n",
        "        self.weights = self.weights - self.learning_rate * grad_weights\n",
        "        self.biases = self.biases - self.learning_rate * grad_biases\n",
        "\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "A1vN8h41ILY9"
      },
      "outputs": [],
      "source": [
        "class Conv2d(Layer):\n",
        "    def __init__(self, input_channels, output_channels, kernel_size, learning_rate=0.1):\n",
        "        self.weights = np.random.randn(input_channels, output_channels, kernel_size, kernel_size)*0.01\n",
        "        self.biases = np.zeros(output_channels)\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        self.grad_weights = None\n",
        "        self.grad_biases = None\n",
        "\n",
        "    def _conv(input, weights, biases):\n",
        "        kernel_height = np.shape(weights)[2]\n",
        "        kernel_width = np.shape(weights)[3]\n",
        "        output_chanels = np.shape(weights)[1]\n",
        "        # Output dimention is determined by the number of filters in the layer.\n",
        "        # Each filter is a tensor with the size (input_channels, kernel_size, kernel_size)\n",
        "        # (In this example second axis is an index of filter)\n",
        "        # Such a filter produces one number per convolution \n",
        "        #   and this convolution is performed for each part of the input tensor.\n",
        "        batch_size = np.shape(input)[0]\n",
        "        output_height = np.shape(input)[2] - kernel_height + 1\n",
        "        output_width = np.shape(input)[3] - kernel_width + 1\n",
        "\n",
        "        # This is the result of convollution of each filter\n",
        "        filters = np.empty((output_chanels, output_height, output_width, batch_size))\n",
        "        for filter_idx in range(output_chanels):\n",
        "            for i in range(output_height):\n",
        "                for j in range(output_width):\n",
        "                    filters[filter_idx, i, j] = \\\n",
        "                        np.tensordot(input[:, :, i:i+kernel_height, j:j+kernel_width],\n",
        "                                     weights[:, filter_idx, :, :],\n",
        "                                     axes=([1, 2, 3], [0, 1, 2]))\n",
        "        output = np.rollaxis(filters, 3, 0)\n",
        "        if biases is not None:\n",
        "        # Here values from biases are broadcasted\n",
        "            output += np.reshape(biases, (1, output_chanels, 1, 1))\n",
        "        return output\n",
        "\n",
        "    def _pad(tensor, val, new_height, new_width):\n",
        "        pad_h = (new_height - np.shape(tensor)[2]) // 2\n",
        "        pad_w = (new_width - np.shape(tensor)[3]) // 2\n",
        "        padding = ((0, 0), (0, 0), (pad_h, pad_h), (pad_w, pad_w))\n",
        "        res = np.pad(tensor, padding, 'constant', constant_values=(val, val))\n",
        "        assert np.shape(res)[2] == new_height and np.shape(res)[3] == new_width\n",
        "        return res\n",
        "    \n",
        "    def _flip_transpose(tensor):\n",
        "        tensor = np.flip(tensor, (2, 3))\n",
        "        tensor = np.moveaxis(tensor, 2, 3) # this should work\n",
        "        return tensor\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Perform an convolution:\n",
        "\n",
        "        output_height = input_height - kernel_size + 1\n",
        "        output_width = input_width - kernel_size + 1\n",
        "\n",
        "        input shape: [batch, input_channels, input_height, input_width]\n",
        "        output shape: [batch, output_channels, output_height, output_width]\n",
        "        \"\"\"\n",
        "        return Conv2d._conv(input, self.weights, self.biases)\n",
        "        \n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        # Grad output has the shape [batch, output_channels, output_height, output_width]\n",
        "        # I don't know how this is possible, but if has these axises, convolution just works\n",
        "        input = np.moveaxis(input, 0, 1)\n",
        "        grad_weights = Conv2d._conv(input, grad_output, None)\n",
        "        input = np.moveaxis(input, 0, 1)\n",
        "        # dldb = dldz * dzdb \n",
        "        # dzdb = 1\n",
        "        # dldz = grad_output\n",
        "        # Each biases' component is applied to each element with the fixed output_chanels axis,\n",
        "        #   so input gradient is the sum of all output gradients\n",
        "        grad_biases = np.sum(grad_output, axis=(0, 2, 3))\n",
        "\n",
        "        kernel_size = np.shape(self.weights)[2]\n",
        "        # Probably I shouldn't change input tensor, but whatever\n",
        "        grad_padded = Conv2d._pad(grad_output, 0,\n",
        "                                   np.shape(input)[2] + kernel_size - 1,\n",
        "                                   np.shape(input)[3] + kernel_size - 1)\n",
        "        tmp_weights = np.copy(self.weights)\n",
        "        tmp_weights = Conv2d._flip_transpose(tmp_weights)\n",
        "        tmp_weights = np.moveaxis(tmp_weights, 0, 1)\n",
        "        grad_input = Conv2d._conv(grad_padded, tmp_weights, None)\n",
        "\n",
        "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
        "\n",
        "        self.weights = self.weights - self.learning_rate * grad_weights\n",
        "        self.biases = self.biases - self.learning_rate * grad_biases\n",
        "\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test shapes:\n",
            "torch.Size([10, 2, 3, 4])\n",
            "(10, 2, 3, 4)\n"
          ]
        }
      ],
      "source": [
        "# Conv2D test\n",
        "from torch import nn\n",
        "import torch\n",
        "test_batch_size = 10\n",
        "test_input_chanels = 3\n",
        "test_output_chanels = 2\n",
        "test_input_height = 4\n",
        "test_input_width = 5\n",
        "test_kernel_size = 2\n",
        "\n",
        "test_conv = Conv2d(input_channels=test_input_chanels, \n",
        "                   output_channels=test_output_chanels, \n",
        "                   kernel_size=test_kernel_size)\n",
        "test_input = np.random.randn(test_batch_size, \n",
        "                             test_input_chanels, \n",
        "                             test_input_height,\n",
        "                             test_input_width)\n",
        "test_output = test_conv.forward(test_input)\n",
        "test_layer_torch = nn.Conv2d(in_channels=test_input_chanels,\n",
        "                     out_channels=test_output_chanels,\n",
        "                     kernel_size=(test_kernel_size, test_kernel_size))\n",
        "test_ref = test_layer_torch(torch.tensor(test_input, dtype=torch.float))\n",
        "print(\"Test shapes:\")\n",
        "print(test_ref.shape)\n",
        "print(test_output.shape)\n",
        "#assert torch.equal(torch.tensor(test_output, dtype=torch.float), test_ref)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shapes (10, 3, 4, 5), (10, 2, 3, 4)\n",
            "grad_output shape before pad (10, 2, 3, 4)\n",
            "input (10, 2, 3, 4), 5, 6\n",
            "grad_output shape after pad (10, 2, 5, 6)\n",
            "weights shape before flip (3, 2, 2, 2)\n",
            "weights shape after flip (3, 2, 2, 2)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[[[-5.11001667e-04, -6.31551106e-04, -8.68758445e-04,\n",
              "          -1.01711396e-03,  1.83748802e-04],\n",
              "         [ 1.11976809e-04, -1.68039207e-03, -1.35932624e-03,\n",
              "          -6.67949680e-05, -1.28401713e-04],\n",
              "         [-2.22100701e-03, -7.35242328e-04,  2.83525726e-04,\n",
              "          -7.78658135e-05, -1.39136373e-04],\n",
              "         [ 5.84763765e-04,  5.07679891e-04,  4.18121853e-04,\n",
              "           9.94755519e-05, -4.94088685e-05]],\n",
              "\n",
              "        [[-4.72519659e-04, -8.48570891e-04, -2.36693272e-04,\n",
              "           8.96231402e-04,  6.02728103e-04],\n",
              "         [-1.74804067e-03, -1.77697210e-03, -1.47159473e-03,\n",
              "          -1.83497416e-03, -3.69348485e-04],\n",
              "         [-5.47330468e-04,  7.56472350e-04,  4.49689504e-04,\n",
              "           1.93732843e-04,  4.94583064e-04],\n",
              "         [-1.73263845e-04,  6.47217413e-04, -1.56549619e-04,\n",
              "           7.43344269e-05, -1.29729237e-04]],\n",
              "\n",
              "        [[ 5.96201736e-04,  2.17971130e-04, -1.46795449e-04,\n",
              "          -1.30643561e-04, -2.49134161e-04],\n",
              "         [ 9.57775149e-04,  9.18301863e-04,  9.89609644e-04,\n",
              "           1.63947983e-03,  4.82619431e-04],\n",
              "         [ 3.20759029e-04, -2.08454143e-03,  7.03187303e-04,\n",
              "           6.35022745e-04,  1.02751370e-03],\n",
              "         [ 1.32929791e-04,  9.33285185e-04, -6.02407009e-04,\n",
              "          -9.20283794e-06, -3.72183509e-04]]],\n",
              "\n",
              "\n",
              "       [[[-1.90697031e-03,  8.93962451e-05,  2.34087863e-04,\n",
              "          -1.73291779e-05,  7.57933170e-04],\n",
              "         [ 2.30341604e-03,  1.61949740e-03, -4.21193589e-04,\n",
              "           3.89641336e-04, -2.38211732e-05],\n",
              "         [ 2.17939108e-03,  1.60732973e-03, -3.50622847e-04,\n",
              "          -1.36498960e-03, -8.84166538e-04],\n",
              "         [-1.49923394e-03, -9.56888298e-04,  2.50273228e-04,\n",
              "          -4.81118617e-04, -3.93402777e-04]],\n",
              "\n",
              "        [[-5.33012653e-04,  5.19147913e-04, -1.06148538e-03,\n",
              "           6.92867400e-04,  5.06111671e-04],\n",
              "         [ 2.84995765e-03,  3.43037225e-03,  3.94107829e-04,\n",
              "          -1.48356223e-04, -7.26837760e-04],\n",
              "         [-3.55970015e-04, -1.75576785e-03, -5.64872183e-04,\n",
              "          -1.13669063e-03, -3.34714654e-04],\n",
              "         [-4.17208018e-04, -1.71334160e-04, -1.73187258e-04,\n",
              "           1.55950011e-04,  4.91968971e-05]],\n",
              "\n",
              "        [[ 1.44031652e-03, -1.80665298e-03,  1.27445522e-03,\n",
              "          -1.50426031e-03,  6.54712831e-04],\n",
              "         [-2.47099544e-03,  3.43162094e-03, -1.34129988e-03,\n",
              "           5.11479086e-04, -4.99680431e-04],\n",
              "         [ 1.09844791e-03,  2.58117680e-04,  1.84513824e-03,\n",
              "          -5.94934906e-04, -6.46987558e-04],\n",
              "         [-2.20178394e-04, -6.85678847e-04,  5.65282598e-04,\n",
              "           9.46568670e-04, -2.56160083e-04]]],\n",
              "\n",
              "\n",
              "       [[[ 1.16842401e-03,  2.11924625e-03,  6.56642608e-04,\n",
              "          -1.34561981e-03,  5.02549525e-04],\n",
              "         [ 1.97046377e-03, -6.91473475e-04,  1.25894790e-05,\n",
              "           4.28683074e-03,  2.40783287e-03],\n",
              "         [-8.48054081e-04,  4.30105464e-04,  9.10574098e-04,\n",
              "          -4.89600292e-04, -4.51970025e-04],\n",
              "         [ 6.02412958e-04, -9.55945551e-04, -2.38351743e-03,\n",
              "          -1.01061083e-03, -5.59620572e-05]],\n",
              "\n",
              "        [[ 2.00564372e-03,  1.85804605e-03,  5.90721037e-04,\n",
              "           1.71293745e-03,  1.13101695e-03],\n",
              "         [-1.73760364e-05, -1.93747499e-03,  1.00265977e-03,\n",
              "           2.23052240e-03,  8.72948752e-04],\n",
              "         [ 1.57563058e-03, -1.33523254e-03, -1.01322304e-03,\n",
              "          -1.86481445e-03, -6.25972202e-04],\n",
              "         [ 6.31851822e-04, -6.46851987e-04, -4.04823416e-04,\n",
              "          -5.67870741e-04,  4.66890399e-04]],\n",
              "\n",
              "        [[-1.95325265e-03,  2.32251125e-05,  6.48254875e-04,\n",
              "          -1.56522916e-04, -2.41738226e-04],\n",
              "         [-1.47571005e-04,  1.03663062e-03, -2.96570905e-03,\n",
              "           2.93015261e-04,  3.00548854e-03],\n",
              "         [-1.89118023e-03, -3.44066011e-04,  1.93603517e-03,\n",
              "           1.24014557e-03, -1.15412228e-03],\n",
              "         [ 2.34643465e-05, -8.71062365e-04, -1.16694746e-03,\n",
              "          -8.31285153e-04,  1.11410765e-03]]],\n",
              "\n",
              "\n",
              "       [[[-4.53903232e-04, -1.44266705e-03, -1.19882297e-04,\n",
              "           1.73867862e-03,  6.28932655e-04],\n",
              "         [-1.79389457e-03,  8.02312285e-05,  1.59094227e-03,\n",
              "          -9.53920971e-05,  8.53205061e-04],\n",
              "         [ 1.03943684e-03,  1.01105038e-03, -2.20061132e-03,\n",
              "           1.36772192e-03,  2.25760605e-03],\n",
              "         [ 8.63442983e-04,  1.50739306e-04, -2.84523791e-05,\n",
              "           1.16104931e-03,  6.65943244e-04]],\n",
              "\n",
              "        [[-1.71700418e-03, -1.42764982e-03,  1.01967859e-03,\n",
              "           1.35922453e-03,  1.79885952e-04],\n",
              "         [ 4.44798091e-05,  8.45045525e-04, -5.31784485e-04,\n",
              "           1.33714472e-03,  5.52887902e-04],\n",
              "         [ 9.30639451e-04,  6.91431067e-04, -9.12764099e-04,\n",
              "           2.53873216e-03,  7.06368961e-04],\n",
              "         [ 2.34164913e-04, -4.32326245e-05, -2.19480620e-04,\n",
              "           9.34468186e-04, -4.45220505e-04]],\n",
              "\n",
              "        [[ 1.35687320e-03, -7.20109575e-05, -1.39835284e-03,\n",
              "          -5.57244468e-04,  7.47269878e-04],\n",
              "         [-1.51010226e-04, -8.37222352e-04,  1.42494738e-03,\n",
              "          -1.37247104e-03, -4.55146299e-04],\n",
              "         [-2.91307985e-04,  1.12626921e-03, -6.14472811e-04,\n",
              "          -2.98255245e-03,  1.62804891e-03],\n",
              "         [ 1.27662031e-04,  3.53514441e-04, -3.24734827e-04,\n",
              "           6.13705300e-04, -4.71873509e-04]]],\n",
              "\n",
              "\n",
              "       [[[-3.49289001e-04,  8.22364111e-04,  2.62328501e-04,\n",
              "           5.76818691e-04,  3.16170941e-04],\n",
              "         [ 2.07300164e-03,  2.31230398e-04, -5.62742252e-04,\n",
              "           1.11401127e-04,  5.84249838e-04],\n",
              "         [ 1.72185669e-04, -2.13757856e-04,  1.05309343e-03,\n",
              "           2.11756245e-03,  7.01843427e-04],\n",
              "         [-5.97581031e-05,  1.85496290e-04,  1.04912156e-04,\n",
              "          -1.78221985e-04, -1.11468578e-04]],\n",
              "\n",
              "        [[ 4.76810292e-04, -2.87606432e-04,  1.23760463e-04,\n",
              "           7.39612285e-04,  2.36234003e-04],\n",
              "         [ 1.49227048e-03,  1.95408291e-04,  7.48860939e-05,\n",
              "           9.23645889e-04,  4.49524521e-04],\n",
              "         [ 6.64616236e-04, -5.77115637e-04,  1.32187362e-03,\n",
              "           4.16951006e-04, -1.72715151e-05],\n",
              "         [ 1.95130852e-05, -4.91338969e-05,  6.04454077e-04,\n",
              "          -4.47428113e-04, -4.07345067e-05]],\n",
              "\n",
              "        [[-1.02510815e-04, -1.87593092e-05, -4.78198787e-04,\n",
              "          -5.40275468e-05,  2.51777797e-04],\n",
              "         [-1.54774781e-03,  1.73279289e-03, -1.32240598e-03,\n",
              "          -5.83297335e-04,  3.45660189e-05],\n",
              "         [-1.33079385e-04, -1.09723558e-03, -1.45843232e-03,\n",
              "           9.31408412e-04,  8.45683324e-04],\n",
              "         [-1.38373791e-05, -5.75274578e-05,  1.92943301e-04,\n",
              "          -1.17073632e-03, -2.09364133e-04]]],\n",
              "\n",
              "\n",
              "       [[[ 3.76924997e-04, -9.19175840e-04,  4.66739926e-04,\n",
              "          -1.07561830e-03, -8.40271206e-04],\n",
              "         [-4.27872867e-04, -2.14054436e-04, -3.28250526e-04,\n",
              "          -1.53182395e-03, -5.87900874e-04],\n",
              "         [ 2.72318640e-04,  4.43361566e-04, -7.16111953e-04,\n",
              "          -2.10925613e-04,  3.67254420e-04],\n",
              "         [-1.44497584e-04, -1.64593381e-04,  6.76492961e-04,\n",
              "           1.25314182e-03,  4.95119274e-04]],\n",
              "\n",
              "        [[-4.86032574e-05,  1.25876480e-04, -1.92054011e-04,\n",
              "          -1.62564713e-03, -4.33187005e-04],\n",
              "         [-7.69174243e-04, -8.37814816e-04, -4.72046739e-06,\n",
              "          -1.35584879e-03,  2.39572978e-04],\n",
              "         [ 5.40195631e-04, -1.08673602e-04,  5.62763033e-04,\n",
              "           7.61293034e-04,  6.33104466e-04],\n",
              "         [ 4.00177775e-04, -4.38449686e-04,  1.26496207e-04,\n",
              "           2.60601715e-04, -1.04270874e-05]],\n",
              "\n",
              "        [[-1.86508209e-04,  5.88693123e-04, -4.06410593e-04,\n",
              "           1.44287340e-03, -8.34513430e-04],\n",
              "         [ 4.76201590e-04, -6.39764416e-05,  8.10330213e-04,\n",
              "          -1.76698795e-04,  3.19787076e-04],\n",
              "         [-1.15297160e-03,  7.94193395e-04,  4.28908581e-04,\n",
              "          -7.67286217e-04,  7.03879075e-04],\n",
              "         [-8.28912792e-05, -9.66953827e-04,  1.93829496e-04,\n",
              "           7.34813923e-04,  4.51207994e-04]]],\n",
              "\n",
              "\n",
              "       [[[-3.10635154e-04, -6.68442679e-04, -4.62486934e-04,\n",
              "           2.86742577e-03,  3.95116845e-04],\n",
              "         [ 8.75683765e-05, -2.29302317e-03, -1.17426563e-04,\n",
              "           1.59853802e-03, -9.72703130e-04],\n",
              "         [-1.49869803e-03,  6.22280928e-04,  1.16992157e-03,\n",
              "          -8.66839712e-04, -7.42287522e-04],\n",
              "         [ 1.53783821e-03,  1.04025067e-03,  1.15121117e-03,\n",
              "           1.13109566e-03,  3.05567933e-04]],\n",
              "\n",
              "        [[-8.66691592e-04, -1.22294982e-03,  2.11580607e-03,\n",
              "           1.39646943e-03, -2.90265805e-04],\n",
              "         [-3.27740897e-04, -2.25152409e-03,  1.29800077e-03,\n",
              "           1.76074998e-05, -1.13929374e-03],\n",
              "         [ 8.68571950e-04,  9.35097694e-04,  1.39695036e-03,\n",
              "           8.66541722e-04,  2.28502889e-05],\n",
              "         [ 5.48018252e-05,  8.49937273e-04, -6.36587452e-04,\n",
              "           4.61408502e-04, -3.13101741e-05]],\n",
              "\n",
              "        [[ 7.31948359e-04,  2.09767958e-04, -1.93984283e-03,\n",
              "          -2.04865110e-04,  8.12105545e-04],\n",
              "         [-3.78175882e-04,  2.43703978e-03, -1.49920977e-03,\n",
              "           5.42415391e-04, -1.69884172e-03],\n",
              "         [-5.20352889e-05, -2.56753293e-03,  3.01511522e-03,\n",
              "           9.18519273e-05, -1.45782116e-03],\n",
              "         [ 2.78102143e-04,  1.42182685e-03, -9.19613373e-04,\n",
              "           1.47357379e-03,  2.16235994e-04]]],\n",
              "\n",
              "\n",
              "       [[[-3.62240768e-05,  1.08865659e-03, -1.65440113e-03,\n",
              "          -6.56817443e-05, -1.13132565e-05],\n",
              "         [ 1.65360031e-05, -1.82889456e-03,  1.80246233e-04,\n",
              "          -2.70267593e-04, -5.01276776e-04],\n",
              "         [ 1.55116377e-04, -1.01768628e-03,  9.90716271e-04,\n",
              "           3.30491477e-03,  1.22561381e-03],\n",
              "         [-5.40624800e-04,  4.49233781e-04,  1.82203120e-03,\n",
              "           9.44285698e-04,  7.66350832e-05]],\n",
              "\n",
              "        [[ 1.14199063e-03, -9.84446418e-04, -1.31354555e-03,\n",
              "          -1.60403646e-04, -1.61261975e-04],\n",
              "         [-1.08658002e-03, -6.28174193e-04,  1.16277550e-03,\n",
              "           8.81493000e-04,  9.66445834e-05],\n",
              "         [-7.02018332e-04,  4.69427939e-04,  2.52360065e-03,\n",
              "           1.47387859e-03,  6.22145982e-04],\n",
              "         [ 5.79936763e-05, -2.76906284e-04,  8.89454220e-04,\n",
              "           1.87287656e-04, -3.70787751e-04]],\n",
              "\n",
              "        [[-7.07355579e-04,  1.04193999e-03,  5.71378173e-04,\n",
              "          -6.81675739e-04,  1.20825647e-04],\n",
              "         [ 1.65160629e-03, -1.20396743e-03, -2.01877206e-03,\n",
              "           2.27833382e-03, -9.80336373e-04],\n",
              "         [-2.04000256e-04,  1.48184119e-03, -2.14350456e-03,\n",
              "          -5.20661766e-04,  1.98560658e-03],\n",
              "         [-1.08585475e-04, -5.17516783e-04,  1.23302959e-03,\n",
              "           6.59978427e-05, -8.53751632e-04]]],\n",
              "\n",
              "\n",
              "       [[[-5.07414015e-04,  6.77106332e-04,  3.42222269e-04,\n",
              "          -2.14116166e-03, -1.07450998e-04],\n",
              "         [-3.69135879e-04, -6.84054270e-04, -7.40129374e-04,\n",
              "           1.05044767e-03,  6.66540378e-04],\n",
              "         [-1.81741518e-03, -1.29899895e-03, -2.06250943e-05,\n",
              "          -6.75078792e-04, -4.08781554e-04],\n",
              "         [ 1.14352826e-03,  9.86198547e-04, -9.50503831e-04,\n",
              "          -1.14897354e-03, -2.81444156e-04]],\n",
              "\n",
              "        [[-4.36019481e-05,  1.25587787e-03, -8.69361002e-04,\n",
              "          -1.94971598e-04,  4.94635528e-04],\n",
              "         [-1.36569237e-03, -9.49581650e-04, -1.14334958e-03,\n",
              "          -3.81417576e-04,  2.04730299e-04],\n",
              "         [-5.82333020e-04,  4.80738040e-04, -2.33561341e-04,\n",
              "          -1.32157758e-03, -1.68719436e-04],\n",
              "         [-2.64959552e-04,  5.00208622e-04, -1.23251673e-04,\n",
              "          -2.56316898e-04,  3.86837735e-05]],\n",
              "\n",
              "        [[ 3.20606502e-04, -1.47933825e-03,  1.83596265e-03,\n",
              "          -5.57612946e-05, -5.74049825e-04],\n",
              "         [ 1.34514932e-03,  8.16669147e-04, -1.62698531e-03,\n",
              "           1.09465081e-03,  1.65936459e-03],\n",
              "         [ 8.28711394e-04, -3.55367694e-04,  3.13882805e-04,\n",
              "          -2.20169894e-04, -6.48814048e-05],\n",
              "         [ 2.49605663e-04,  1.77228796e-03, -2.74862538e-05,\n",
              "          -1.00822691e-03, -1.74533619e-04]]],\n",
              "\n",
              "\n",
              "       [[[ 1.72256324e-05, -1.69040717e-03,  5.61039814e-04,\n",
              "           1.48454612e-03, -2.10965307e-04],\n",
              "         [-1.42406650e-03,  1.19164782e-03,  2.31455521e-03,\n",
              "          -6.40000647e-04, -3.83757025e-04],\n",
              "         [-3.28450880e-04,  2.18613579e-03,  1.24463148e-03,\n",
              "           1.54035947e-03,  9.21661042e-04],\n",
              "         [ 1.32810705e-03,  5.46812545e-04,  3.95657583e-05,\n",
              "           1.84166788e-04,  5.72936485e-05]],\n",
              "\n",
              "        [[-8.20950837e-04, -3.69968798e-04,  2.14342940e-03,\n",
              "           2.10122754e-04, -3.67053915e-04],\n",
              "         [-2.98723684e-04,  1.88831332e-03,  1.21027251e-03,\n",
              "          -5.45183943e-04,  2.98479400e-05],\n",
              "         [ 4.89957790e-04,  1.69586370e-03,  5.32391246e-05,\n",
              "           1.65141710e-03,  3.20224318e-04],\n",
              "         [ 2.94602554e-05,  4.08007159e-04,  7.21619808e-05,\n",
              "           3.32659584e-04, -4.24922377e-04]],\n",
              "\n",
              "        [[ 5.13588351e-04,  4.57755430e-04, -2.25642357e-03,\n",
              "           1.10916847e-03,  9.94118255e-06],\n",
              "         [ 3.21381620e-04, -2.02116466e-03,  3.34309878e-03,\n",
              "          -1.14323821e-03, -5.62947468e-04],\n",
              "         [ 2.19257042e-04,  1.79791906e-04, -4.99661651e-04,\n",
              "          -5.64331380e-04,  9.02529338e-04],\n",
              "         [ 2.42676549e-04,  1.24664146e-03, -6.60544844e-04,\n",
              "          -4.27343755e-04, -1.00782943e-03]]]])"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"Input shapes {np.shape(test_input)}, {np.shape(test_output)}\")\n",
        "test_conv.backward(test_input, test_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "util.py.1           100%[===================>]   3,97K  --.-KB/s    in 0s      \n"
          ]
        }
      ],
      "source": [
        "!wget --quiet --show-progress \"https://raw.githubusercontent.com/mryab/dl-hse-ami/master/week01_intro/util.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "grad_output shape before pad (10, 2, 3, 4)\n",
            "input (10, 2, 3, 4), 5, 6\n",
            "grad_output shape after pad (10, 2, 5, 6)\n",
            "weights shape before flip (3, 2, 2, 2)\n",
            "weights shape after flip (3, 2, 2, 2)\n",
            "my grads [[[[ 0.02970716  0.02257574]\n",
            "   [-0.0030515   0.05990383]]\n",
            "\n",
            "  [[ 0.02672301  0.03915883]\n",
            "   [ 0.09213424  0.07432515]]]\n",
            "\n",
            "\n",
            " [[[ 0.106251   -0.0375867 ]\n",
            "   [-0.13800736  0.00326983]]\n",
            "\n",
            "  [[ 0.03884403 -0.03407668]\n",
            "   [-0.12116685 -0.13928758]]]]\n",
            "real grads [[[[-0.0012761   0.00652251]\n",
            "   [ 0.0001263   0.00961945]]\n",
            "\n",
            "  [[ 0.00737544  0.00829457]\n",
            "   [ 0.00986708  0.01001397]]]\n",
            "\n",
            "\n",
            " [[[-0.0012761   0.00652251]\n",
            "   [ 0.0001263   0.00961945]]\n",
            "\n",
            "  [[ 0.00737544  0.00829457]\n",
            "   [ 0.00986708  0.01001397]]]]\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "gradient returned by your layer does not match the numerically computed gradient",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[151], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy grads \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrads[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal grads \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_grads[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(grads, numeric_grads, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient returned by your layer does not match the numerically computed gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;31mAssertionError\u001b[0m: gradient returned by your layer does not match the numerically computed gradient"
          ]
        }
      ],
      "source": [
        "# some tests\n",
        "from util import eval_numerical_gradient\n",
        "\n",
        "grads = test_conv.backward(test_input, test_output)\n",
        "numeric_grads = eval_numerical_gradient(lambda x: test_conv.forward(x).mean(), x=test_input)\n",
        "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0),\\\n",
        "    \"gradient returned by your layer does not match the numerically computed gradient\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1iE85PpMm0Z"
      },
      "outputs": [],
      "source": [
        "class MaxPool2d(Layer):\n",
        "    def __init__(self, kernel_size):\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Perform an pooling:\n",
        "\n",
        "        output_height = input_height // kernel_size\n",
        "        output_width = input_width // kernel_size\n",
        "\n",
        "        input shape: [batch, input_channels, input_height, input_width]\n",
        "        output shape: [batch, input_channels, output_height, output_width]\n",
        "        \"\"\"\n",
        "        #<your code here>\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        #grad_input = <your code here>\n",
        "\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W-GM45xSqJD"
      },
      "outputs": [],
      "source": [
        "class Flatten(Layer):\n",
        "    def forward(self, input):\n",
        "          \"\"\"\n",
        "          Perform an flatten operation:\n",
        "\n",
        "          input shape: [batch, input_channels, input_height, input_width]\n",
        "          output shape: [batch, input_channels * output_height * output_width]\n",
        "          \"\"\"\n",
        "          #<your code here>\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        #grad_input = <your code here>\n",
        "\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCuNsUikOoE1",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def softmax_crossentropy_with_logits(logits,reference_answers):\n",
        "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
        "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
        "\n",
        "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
        "\n",
        "    return xentropy\n",
        "\n",
        "def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n",
        "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
        "    ones_for_answers = np.zeros_like(logits)\n",
        "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
        "\n",
        "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
        "\n",
        "    return (- ones_for_answers + softmax) / logits.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muokQIA3UcnL"
      },
      "source": [
        "## Имплементация оптимизатора и изменения learning rate'a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBzMVan_XzFM"
      },
      "source": [
        "В имплементации этих двух классов есть небольшие неточности.\n",
        "Посмотрите, как сделана имплементация метода моментов в Pytorch и добавьте пропущенное.\n",
        "\n",
        "> Добавлять моменты Нестерова не нужно!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4hyzuIcUqLd"
      },
      "outputs": [],
      "source": [
        "class SGDOptimizer:\n",
        "    def __init__(self, momentum=0.9, dampening=0.0, weight_decay=0.0):\n",
        "        \"\"\"\n",
        "        Wrapper which perfoms weights update\n",
        "        \"\"\"\n",
        "        self.momentum = momentum\n",
        "        self.dampening = dampening\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        # здесь будем копить моментумы\n",
        "        self.momentum_buffer = 0\n",
        "\n",
        "    def step(self, weights, grad_weights, lr=0.1):\n",
        "      \"\"\"\n",
        "      Update weights\n",
        "      \"\"\"\n",
        "      self.momentum_buffer = self.momentum * self.momentum_buffer + (1 - self.dampening) * grad_weights\n",
        "\n",
        "      #<your code here>\n",
        "\n",
        "      return weights - lr * self.momentum_buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DX-leurYblp"
      },
      "outputs": [],
      "source": [
        "class LRScheduler:\n",
        "    def __init__(self, lr):\n",
        "        \"\"\"\n",
        "        Wrapper which perfoms learning rate updates\n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "        self.current_step = 0\n",
        "\n",
        "    def get_lr(self, step):\n",
        "      \"\"\"\n",
        "      Update learing rate for current iteration\n",
        "      \"\"\"\n",
        "      current_lr = self.lr / (self.current_step + 1)\n",
        "      #<your code here>\n",
        "      self.current_step += 1\n",
        "      return current_lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlEURTE9OoE5",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Итоговая нейросеть\n",
        "\n",
        "Все готово для запуска нейросети. Нейросеть будем тестировать на классическом датасете MNIST. Код ниже визуализирует несколько примеров из этого датасета."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KKQ81e0OoE6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist(flatten=False)\n",
        "\n",
        "plt.figure(figsize=[6,6])\n",
        "for i in range(4):\n",
        "    plt.subplot(2,2,i+1)\n",
        "    plt.title(\"Label: %i\"%y_train[i])\n",
        "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGXtnvX4OoE7",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "В нашей реализации сеть - просто список (Python-list) слоев.\n",
        "\n",
        "\n",
        "\n",
        "> Обратите внимание, что у нас нет глобального пулинга. При изменении архитектуры сети вы должны поменять входую размерность в последнем Dense слое\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VicezF_TOoE8",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "network = []\n",
        "network.append(Conv2d(1, 4, 5))\n",
        "network.append(MaxPool2d(2))\n",
        "network.append(ReLU())\n",
        "network.append(Conv2d(4, 8, 5))\n",
        "network.append(MaxPool2d(2))\n",
        "network.append(ReLU())\n",
        "network.append(Flatten())\n",
        "network.append(Dense(5 * 5 * 8, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sUrd667ZDuK"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "\n",
        "optimizer = SGDOptimizer()\n",
        "scheduler = LRScheduler(learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "482hIf_UOoE9",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Реализуйте прямой проход по целой сети, последовательно вызывая .forward() для каждого слоя."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKRyUyj5OoE9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def forward(network, X):\n",
        "    \"\"\"\n",
        "    Compute activations of all network layers by applying them sequentially.\n",
        "    Return a list of activations for each layer.\n",
        "    Make sure last activation corresponds to network logits.\n",
        "    \"\"\"\n",
        "    activations = []\n",
        "    input = X\n",
        "\n",
        "    # <your code here>\n",
        "\n",
        "    assert len(activations) == len(network)\n",
        "    return activations\n",
        "\n",
        "def predict(network, X):\n",
        "    \"\"\"\n",
        "    Use network to predict the most likely class for each sample.\n",
        "    \"\"\"\n",
        "    logits = forward(network, X)[-1]\n",
        "    return logits.argmax(axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfUyHKPyOoE-",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def train(network,X,y):\n",
        "    \"\"\"\n",
        "    Train your network on a given batch of X and y.\n",
        "    You first need to run forward to get all layer activations.\n",
        "    Then you can run layer.backward going from last to first layer.\n",
        "\n",
        "    After you called backward for all layers, all Dense layers have already made one gradient step.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the layer activations\n",
        "    layer_activations = forward(network,X)\n",
        "    layer_inputs = [X] + layer_activations  #layer_input[i] is an input for network[i]\n",
        "    logits = layer_activations[-1]\n",
        "\n",
        "    # Compute the loss and the initial gradient\n",
        "    loss = softmax_crossentropy_with_logits(logits,y)\n",
        "    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
        "\n",
        "    # propagate gradients through network layers using .backward\n",
        "    # hint: start from last layer and move to earlier layers\n",
        "    #<YOUR CODE>\n",
        "\n",
        "    # update weights and biases with optimizer\n",
        "    #<YOUR CODE>\n",
        "\n",
        "    # update learning rate\n",
        "    #<YOUR CODE>\n",
        "\n",
        "    return np.mean(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJG4VMsROoE_",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Все готово для запуска обучения. Если все реализовано корректно, то точность классификации на валидационном множестве **должна быть около** 99%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kq5XTDNNOoE_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
        "    assert len(inputs) == len(targets)\n",
        "    if shuffle:\n",
        "        indices = np.random.permutation(len(inputs))\n",
        "    for start_idx in tqdm(range(0, len(inputs) - batchsize + 1, batchsize)):\n",
        "        if shuffle:\n",
        "            excerpt = indices[start_idx:start_idx + batchsize]\n",
        "        else:\n",
        "            excerpt = slice(start_idx, start_idx + batchsize)\n",
        "        yield inputs[excerpt], targets[excerpt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q2KcwkTOoFA",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "train_log = []\n",
        "val_log = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaxQu9WsOoFB",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "for epoch in range(15):\n",
        "\n",
        "    for x_batch,y_batch in iterate_minibatches(X_train, y_train, batchsize=32, shuffle=True):\n",
        "        train(network, x_batch, y_batch)\n",
        "\n",
        "    train_log.append(np.mean(predict(network, X_train) == y_train))\n",
        "    val_log.append(np.mean(predict(network, X_val) == y_val))\n",
        "\n",
        "    clear_output()\n",
        "    print(\"Epoch\",epoch)\n",
        "    print(\"Train accuracy:\",train_log[-1])\n",
        "    print(\"Val accuracy:\",val_log[-1])\n",
        "    plt.plot(train_log,label='train accuracy')\n",
        "    plt.plot(val_log,label='val accuracy')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
