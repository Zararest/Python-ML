{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ3DJzu7FDX_"
      },
      "source": [
        "# Домашнее задание 2.1. Сверточные сети\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQVARDpwNsOa"
      },
      "source": [
        "В этом задании вы должны:\n",
        "1. Написать слой Conv2d на Numpy и определить в нем forward-backward методы\n",
        "2. Определить слой MaxPool2d\n",
        "3. Написать всю необходимую обвязку для обучения: оптимизатор с адаптивным шагом и класс, позволяющий изменять расписание для learning rate'а\n",
        "\n",
        "\n",
        "\n",
        "> Обратите внимание, что в этом задании больше нет тестов.\n",
        "> Вы должны сами проверять свой код.  \n",
        "> Это можно сделать так:\n",
        "> 1. Написать юнит-тесты с помощью Pytorch. То есть, ваш модудь должен повторять поведение torch'а\n",
        "> 2. Проверять архитектуру не на всем датасете, а на подвыборке: при наивной имплементации слоев одна эпоха на всем датасете будет занимать около двух часов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMVkqpoEOoD3",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Numpy-имплементация сверточной нейронной сети\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmTxOp7KOoEG",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Вставьте сюда имплементацию из первого домашнего задания.\n",
        "\n",
        "\n",
        "\n",
        "> Обратите внимание, что обновление весов теперь производится с помощью специального класса **Optimizer**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 373,
      "metadata": {
        "id": "lmfLBp4tOoEN",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "def load_mnist(flatten=False):\n",
        "    \"\"\"taken from https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py\"\"\"\n",
        "    # We first define a download function, supporting both Python 2 and 3.\n",
        "    if sys.version_info[0] == 2:\n",
        "        from urllib import urlretrieve\n",
        "    else:\n",
        "        from urllib.request import urlretrieve\n",
        "\n",
        "    def download(filename, source='https://ossci-datasets.s3.amazonaws.com/mnist/'):\n",
        "        print(\"Downloading %s\" % filename)\n",
        "        urlretrieve(source + filename, filename)\n",
        "\n",
        "    # We then define functions for loading MNIST images and labels.\n",
        "    # For convenience, they also download the requested files if needed.\n",
        "    import gzip\n",
        "\n",
        "    def load_mnist_images(filename):\n",
        "        if not os.path.exists(filename):\n",
        "            download(filename)\n",
        "        # Read the inputs in Yann LeCun's binary format.\n",
        "        with gzip.open(filename, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
        "        # following the shape convention: (examples, channels, rows, columns)\n",
        "        data = data.reshape(-1, 1, 28, 28)\n",
        "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
        "        # (Actually to range [0, 255/256], for compatibility to the version\n",
        "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
        "        return data / np.float32(256)\n",
        "\n",
        "    def load_mnist_labels(filename):\n",
        "        if not os.path.exists(filename):\n",
        "            download(filename)\n",
        "        # Read the labels in Yann LeCun's binary format.\n",
        "        with gzip.open(filename, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "        # The labels are vectors of integers now, that's exactly what we want.\n",
        "        return data\n",
        "\n",
        "    # We can now download and read the training and test set images and labels.\n",
        "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
        "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
        "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
        "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "    # We reserve the last 10000 training examples for validation.\n",
        "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
        "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
        "\n",
        "    if flatten:\n",
        "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
        "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
        "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
        "\n",
        "    # We just return all the arrays in order, as expected in main().\n",
        "    # (It doesn't matter how we do this as long as we can read them again.)\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 374,
      "metadata": {
        "id": "jYZ3ptaiOoER",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    \"\"\"\n",
        "    A building block. Each layer is capable of performing two things:\n",
        "\n",
        "    - Process input to get output:           output = layer.forward(input)\n",
        "\n",
        "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
        "\n",
        "    Some layers also have learnable parameters which they update during layer.backward.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"Here you can initialize layer parameters (if any) and auxiliary stuff.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def get_grad_weights(self):\n",
        "        return None\n",
        "\n",
        "    def get_weights(self):\n",
        "        return None\n",
        "    \n",
        "    def set_weights(self, weights):\n",
        "        pass\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n",
        "        \"\"\"\n",
        "        return input\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the layer, with respect to the given input.\n",
        "\n",
        "        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n",
        "\n",
        "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
        "\n",
        "        Luckily, you already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
        "\n",
        "        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss / d layer\n",
        "        \"\"\"\n",
        "        input_dim = input.shape[1]\n",
        "\n",
        "        d_layer_d_input = np.eye(input_dim)\n",
        "\n",
        "        return np.dot(grad_output, d_layer_d_input) # chain rule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {
        "id": "Qe246l61OoEe",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
        "        pass\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Apply elementwise ReLU to [batch, input_units] matrix\"\"\"\n",
        "        output = np.array(input)\n",
        "        output[output < 0] = 0\n",
        "        return output\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"Compute gradient of loss w.r.t. ReLU input\"\"\"\n",
        "        relu_grad_mask = np.zeros_like(input)\n",
        "        relu_grad_mask[input > 0] = 1\n",
        "        return grad_output * relu_grad_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "metadata": {
        "id": "g22Gzs_2OoEl",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class Dense(Layer):\n",
        "    def __init__(self, input_units, output_units):\n",
        "        \"\"\"\n",
        "        A dense layer is a layer which performs a learned affine transformation:\n",
        "        f(x) = <W*x> + b\n",
        "        \"\"\"\n",
        "        # initialize weights with small random numbers from normal distribution\n",
        "        # you can change the intializtion method\n",
        "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
        "        self.biases = np.zeros(output_units)\n",
        "\n",
        "        self.grad_weights = None\n",
        "        self.grad_biases = None\n",
        "\n",
        "    def get_weights(self):\n",
        "        return np.concatenate((self.weights, self.biases), axis=None)\n",
        "    \n",
        "    def get_grad_weights(self):\n",
        "        return np.concatenate((self.grad_weights, self.grad_biases), axis=None)\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        weights_size = np.size(self.weights)\n",
        "        biases_size = np.size(self.biases)\n",
        "        all_weights_size = weights_size + biases_size\n",
        "        assert np.shape(weights)[0] == all_weights_size\n",
        "        self.weights = np.reshape(weights[:weights_size], np.shape(self.weights))\n",
        "        self.biases = np.reshape(weights[weights_size:], np.shape(self.biases))\n",
        "\n",
        "    def forward(self,input):\n",
        "        \"\"\"\n",
        "        Perform an affine transformation:\n",
        "        f(x) = <W*x> + b\n",
        "\n",
        "        input shape: [batch, input_units]\n",
        "        output shape: [batch, output_units]\n",
        "        \"\"\"\n",
        "        return np.dot(input, self.weights) + self.biases\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "\n",
        "        # compute d f / d x = d f / d dense * d dense / d x\n",
        "        # where d dense/ d x = weights transposed\n",
        "        # grad_output is a derivative of the next (in forward) layer of prediction\n",
        "        # this result is needed for the next layer\n",
        "        grad_input = np.dot(grad_output, np.transpose(self.weights))\n",
        "\n",
        "        # compute gradient w.r.t. weights and biases\n",
        "        self.grad_weights = np.dot(np.transpose(input), grad_output)\n",
        "        self.grad_biases = np.sum(grad_output, axis=0)\n",
        "\n",
        "        # Gradient step should be performed with the help of separate optimizer\n",
        "        assert self.grad_weights.shape == self.weights.shape and self.grad_biases.shape == self.biases.shape\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 515,
      "metadata": {
        "id": "A1vN8h41ILY9"
      },
      "outputs": [],
      "source": [
        "class Conv2d(Layer):\n",
        "    def __init__(self, input_channels, output_channels, kernel_size, learning_rate=0.1):\n",
        "        self.weights = np.random.randn(input_channels, output_channels, kernel_size, kernel_size)*0.01\n",
        "        self.biases = np.zeros(output_channels)\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        self.grad_weights = None\n",
        "        self.grad_biases = None\n",
        "\n",
        "    def _conv(input, weights, biases):\n",
        "        kernel_height = np.shape(weights)[2]\n",
        "        kernel_width = np.shape(weights)[3]\n",
        "        output_chanels = np.shape(weights)[1]\n",
        "        # Output dimention is determined by the number of filters in the layer.\n",
        "        # Each filter is a tensor with the size (input_channels, kernel_size, kernel_size)\n",
        "        # (In this example second axis is an index of filter)\n",
        "        # Such a filter produces one number per convolution \n",
        "        #   and this convolution is performed for each part of the input tensor.\n",
        "        batch_size = np.shape(input)[0]\n",
        "        output_height = np.shape(input)[2] - kernel_height + 1\n",
        "        output_width = np.shape(input)[3] - kernel_width + 1\n",
        "\n",
        "        # This is the result of convollution of each filter\n",
        "        filters = np.empty((output_chanels, output_height, output_width, batch_size))\n",
        "        for filter_idx in range(output_chanels):\n",
        "            for i in range(output_height):\n",
        "                for j in range(output_width):\n",
        "                    filters[filter_idx, i, j] = \\\n",
        "                        np.tensordot(input[:, :, i:i+kernel_height, j:j+kernel_width],\n",
        "                                     weights[:, filter_idx, :, :],\n",
        "                                     axes=([1, 2, 3], [0, 1, 2]))\n",
        "        output = np.rollaxis(filters, 3, 0)\n",
        "        if biases is not None:\n",
        "        # Here values from biases are broadcasted\n",
        "            output += np.reshape(biases, (1, output_chanels, 1, 1))\n",
        "        return output\n",
        "\n",
        "    def _pad(tensor, val, new_height, new_width):\n",
        "        pad_h = (new_height - np.shape(tensor)[2]) // 2\n",
        "        pad_w = (new_width - np.shape(tensor)[3]) // 2\n",
        "        padding = ((0, 0), (0, 0), (pad_h, pad_h), (pad_w, pad_w))\n",
        "        res = np.pad(tensor, padding, 'constant', constant_values=(val, val))\n",
        "        assert np.shape(res)[2] == new_height and np.shape(res)[3] == new_width\n",
        "        return res\n",
        "    \n",
        "    def _flip_transpose(tensor):\n",
        "        tensor = np.flip(tensor, (2, 3))\n",
        "        tensor = np.moveaxis(tensor, 2, 3) # this should work\n",
        "        return tensor\n",
        "    \n",
        "    def get_grad_weights(self):\n",
        "        return np.concatenate((self.grad_weights, self.grad_biases), axis=None)\n",
        "    \n",
        "    def get_weights(self):\n",
        "        return np.concatenate((self.weights, self.biases), axis=None)\n",
        "    \n",
        "    def set_weights(self, weights):\n",
        "        weights_size = np.size(self.weights)\n",
        "        biases_size = np.size(self.biases)\n",
        "        all_weights_size = weights_size + biases_size\n",
        "        assert np.shape(weights)[0] == all_weights_size\n",
        "        self.weights = np.reshape(weights[:weights_size], np.shape(self.weights))\n",
        "        self.biases = np.reshape(weights[weights_size:], np.shape(self.biases))\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Perform an convolution:\n",
        "\n",
        "        output_height = input_height - kernel_size + 1\n",
        "        output_width = input_width - kernel_size + 1\n",
        "\n",
        "        input shape: [batch, input_channels, input_height, input_width]\n",
        "        output shape: [batch, output_channels, output_height, output_width]\n",
        "        \"\"\"\n",
        "        return Conv2d._conv(input, self.weights, self.biases)\n",
        "        \n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        # Grad output has the shape [batch, output_channels, output_height, output_width]\n",
        "        # I don't know how this is possible, but if I move these axises, convolution just works\n",
        "        input = np.moveaxis(input, 0, 1)\n",
        "        self.grad_weights = Conv2d._conv(input, grad_output, None)\n",
        "        input = np.moveaxis(input, 0, 1)\n",
        "        # dldb = dldz * dzdb \n",
        "        # dzdb = 1\n",
        "        # dldz = grad_output\n",
        "        # Each biases' component is applied to each element with the fixed output_chanels axis,\n",
        "        #   so input gradient is the sum of all output gradients\n",
        "        self.grad_biases = np.sum(grad_output, axis=(0, 2, 3))\n",
        "\n",
        "        kernel_size = np.shape(self.weights)[2]\n",
        "        # Probably I shouldn't change input tensor, but whatever\n",
        "        grad_padded = Conv2d._pad(grad_output, 0,\n",
        "                                   np.shape(input)[2] + kernel_size - 1,\n",
        "                                   np.shape(input)[3] + kernel_size - 1) # Should be right\n",
        "        tmp_weights = np.copy(self.weights)\n",
        "        tmp_weights = Conv2d._flip_transpose(tmp_weights)\n",
        "        tmp_weights = np.moveaxis(tmp_weights, 0, 1)\n",
        "        grad_input = Conv2d._conv(grad_padded, tmp_weights, None)\n",
        "\n",
        "        # Gradient step is in a separate optimizer\n",
        "        assert self.grad_weights.shape == self.weights.shape and \\\n",
        "            self.grad_biases.shape == self.biases.shape\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 516,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conv2D test\n",
        "from torch import nn\n",
        "import torch\n",
        "test_batch_size = 10\n",
        "test_input_chanels = 3\n",
        "test_output_chanels = 2\n",
        "test_input_height = 4\n",
        "test_input_width = 5\n",
        "test_kernel_size = 2\n",
        "\n",
        "test_conv = Conv2d(input_channels=test_input_chanels, \n",
        "                   output_channels=test_output_chanels, \n",
        "                   kernel_size=test_kernel_size)\n",
        "test_input = np.random.randn(test_batch_size, \n",
        "                             test_input_chanels, \n",
        "                             test_input_height,\n",
        "                             test_input_width)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 517,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_forward(input, conv_layer):\n",
        "  torch_weights = torch.tensor(conv_layer.weights, dtype=torch.float64)\n",
        "  torch_weights = torch.moveaxis(torch_weights, 0, 1)\n",
        "  torch_bias = torch.tensor(conv_layer.biases, dtype=torch.float64)\n",
        "  weights_shape = torch_weights.shape\n",
        "  #np.random.randn(input_channels, output_channels, kernel_size, kernel_size)v\n",
        "  torch_layer = nn.Conv2d(in_channels=weights_shape[1], \n",
        "                          out_channels=weights_shape[0], \n",
        "                          kernel_size=(weights_shape[2], weights_shape[3]),\n",
        "                          bias=True)\n",
        "  assert torch_layer.weight.data.shape == torch_weights.shape, \\\n",
        "    f\"{torch_layer.weight.data.shape} vs {torch_weights.shape}\"\n",
        "  assert torch_layer.bias.data.shape == torch_bias.shape\n",
        "  torch_layer.weight.data = torch_weights\n",
        "  torch_layer.bias.data = torch_bias\n",
        "\n",
        "  input_torch = torch.tensor(input, dtype=torch.float64)\n",
        "  output_torch = torch_layer(input_torch)\n",
        "  output = torch.tensor(conv_layer.forward(input))\n",
        "  \n",
        "  assert torch.allclose(output_torch, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 518,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_forward_backward(input, conv_layer):\n",
        "  torch_weights = torch.tensor(conv_layer.weights, dtype=torch.float64)\n",
        "  torch_weights = torch.moveaxis(torch_weights, 0, 1)\n",
        "  torch_bias = torch.tensor(conv_layer.biases, dtype=torch.float64)\n",
        "  weights_shape = torch_weights.shape\n",
        "  #np.random.randn(input_channels, output_channels, kernel_size, kernel_size)v\n",
        "  torch_layer = nn.Conv2d(in_channels=weights_shape[1], \n",
        "                          out_channels=weights_shape[0], \n",
        "                          kernel_size=(weights_shape[2], weights_shape[3]),\n",
        "                          bias=True)\n",
        "  assert torch_layer.weight.data.shape == torch_weights.shape, \\\n",
        "    f\"{torch_layer.weight.data.shape} vs {torch_weights.shape}\"\n",
        "  assert torch_layer.bias.data.shape == torch_bias.shape, \\\n",
        "    f\"{torch_layer.bias.data.shape} vs {torch_bias.shape}\"\n",
        "  torch_layer.weight.data = torch_weights\n",
        "  torch_layer.bias.data = torch_bias\n",
        "\n",
        "  input_torch = torch.tensor(input, dtype=torch.float64)\n",
        "  output_torch = torch_layer(input_torch)\n",
        "  output = torch.tensor(conv_layer.forward(input))\n",
        "  \n",
        "  assert torch.allclose(output_torch, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 519,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_forward(test_input, test_conv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 520,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_input: [[[[-0.34684817 -0.30095996  0.47312227 -0.62475388 -1.15468839]\n",
            "   [-1.58399781  1.08904639  0.47824314  0.38111637  0.18424932]\n",
            "   [-0.05582002 -1.53952863  0.0475645  -0.34929211  2.06039877]\n",
            "   [-1.23538066 -0.19646768  0.57744411 -1.50372514  0.23136769]]\n",
            "\n",
            "  [[-0.07067682 -0.86512453  1.16928108  0.09670819  0.09698619]\n",
            "   [ 0.53434696 -0.4514494   0.10570858  0.90779031  0.44230828]\n",
            "   [ 0.29505433  0.63284878  0.13110665  0.8929546   0.61450229]\n",
            "   [-1.70832847 -1.2328668   0.94000501  0.32740124  0.0039272 ]]\n",
            "\n",
            "  [[-1.52173351 -0.62847767  1.45074686  0.42137873  0.42252007]\n",
            "   [ 1.29829819 -1.20193603 -1.08414318 -2.5650369  -0.18129772]\n",
            "   [ 1.37066407 -0.042622    0.27686214 -0.30265044  0.8222425 ]\n",
            "   [-0.1795455   0.71729322 -1.84618087 -1.6823786   0.33753531]]]\n",
            "\n",
            "\n",
            " [[[ 0.60942677 -1.16567143 -0.55523641 -0.94688002  0.01921002]\n",
            "   [ 0.42617415 -0.57970064  0.41872027 -0.50671351  0.91878301]\n",
            "   [ 0.54708318 -1.20545416  0.05533356 -0.26875594  0.790425  ]\n",
            "   [ 0.43723492 -0.56794593  0.47725385 -0.45946334  3.15268646]]\n",
            "\n",
            "  [[-0.3989117  -1.16240778 -0.06026398  0.18733366 -0.7278087 ]\n",
            "   [-1.8058913   0.98903649 -1.1423536  -0.19568778  0.49051269]\n",
            "   [ 0.42541621 -0.426273   -1.00634123 -0.10828656 -0.33015244]\n",
            "   [-1.50905008 -1.09847633 -0.6147088   2.12769421 -0.42698827]]\n",
            "\n",
            "  [[-0.25834132 -0.55775121 -0.27535679  0.02753259  0.10716319]\n",
            "   [ 0.1218273  -0.32045666  0.1680738   0.6256123  -1.14038798]\n",
            "   [ 1.80511636 -0.01020746 -0.79656141  1.20959749 -0.84573526]\n",
            "   [-1.91501931  0.16433819 -0.27562615  2.59867792 -1.24894648]]]]\n",
            "test_input: [[[[-0.34684817 -0.30095996  0.47312227 -0.62475388 -1.15468839]\n",
            "   [-1.58399781  1.08904639  0.47824314  0.38111637  0.18424932]\n",
            "   [-0.05582002 -1.53952863  0.0475645  -0.34929211  2.06039877]\n",
            "   [-1.23538066 -0.19646768  0.57744411 -1.50372514  0.23136769]]\n",
            "\n",
            "  [[-0.07067682 -0.86512453  1.16928108  0.09670819  0.09698619]\n",
            "   [ 0.53434696 -0.4514494   0.10570858  0.90779031  0.44230828]\n",
            "   [ 0.29505433  0.63284878  0.13110665  0.8929546   0.61450229]\n",
            "   [-1.70832847 -1.2328668   0.94000501  0.32740124  0.0039272 ]]\n",
            "\n",
            "  [[-1.52173351 -0.62847767  1.45074686  0.42137873  0.42252007]\n",
            "   [ 1.29829819 -1.20193603 -1.08414318 -2.5650369  -0.18129772]\n",
            "   [ 1.37066407 -0.042622    0.27686214 -0.30265044  0.8222425 ]\n",
            "   [-0.1795455   0.71729322 -1.84618087 -1.6823786   0.33753531]]]\n",
            "\n",
            "\n",
            " [[[ 0.60942677 -1.16567143 -0.55523641 -0.94688002  0.01921002]\n",
            "   [ 0.42617415 -0.57970064  0.41872027 -0.50671351  0.91878301]\n",
            "   [ 0.54708318 -1.20545416  0.05533356 -0.26875594  0.790425  ]\n",
            "   [ 0.43723492 -0.56794593  0.47725385 -0.45946334  3.15268646]]\n",
            "\n",
            "  [[-0.3989117  -1.16240778 -0.06026398  0.18733366 -0.7278087 ]\n",
            "   [-1.8058913   0.98903649 -1.1423536  -0.19568778  0.49051269]\n",
            "   [ 0.42541621 -0.426273   -1.00634123 -0.10828656 -0.33015244]\n",
            "   [-1.50905008 -1.09847633 -0.6147088   2.12769421 -0.42698827]]\n",
            "\n",
            "  [[-0.25834132 -0.55775121 -0.27535679  0.02753259  0.10716319]\n",
            "   [ 0.1218273  -0.32045666  0.1680738   0.6256123  -1.14038798]\n",
            "   [ 1.80511636 -0.01020746 -0.79656141  1.20959749 -0.84573526]\n",
            "   [-1.91501931  0.16433819 -0.27562615  2.59867792 -1.24894648]]]]\n",
            "test_input: [[[[-4.71737750e-04 -2.50883918e-04  2.70360808e-05  6.61344483e-04\n",
            "     1.15541232e-04]\n",
            "   [ 5.66660395e-05 -3.71347517e-05 -1.20962551e-03  4.93964569e-04\n",
            "     3.24975914e-04]\n",
            "   [ 1.21913473e-03 -2.00899695e-04  5.75269655e-04  1.37615066e-03\n",
            "     4.82599569e-05]\n",
            "   [-9.39973598e-04 -4.69967430e-04  9.09666669e-04  1.07436758e-03\n",
            "    -3.89691171e-04]]\n",
            "\n",
            "  [[ 2.10014233e-05  2.16032568e-04  4.10118863e-04  4.84482797e-05\n",
            "    -5.39482004e-06]\n",
            "   [-1.94675732e-04  1.14068047e-03  1.64745957e-03  1.14265116e-04\n",
            "    -8.45591660e-06]\n",
            "   [-3.69337760e-04 -1.29981180e-04  4.26179373e-04  4.04364649e-04\n",
            "     8.24498307e-04]\n",
            "   [ 8.16341056e-04 -2.39848920e-04  1.33918115e-03 -1.06332125e-03\n",
            "     4.21415408e-04]]\n",
            "\n",
            "  [[-3.90417844e-04 -3.83444350e-05  5.33107430e-04  5.30933789e-04\n",
            "     6.42042554e-05]\n",
            "   [-8.34975158e-04 -1.21194305e-03 -1.90360753e-03 -1.84522128e-03\n",
            "    -3.24961363e-04]\n",
            "   [ 5.66897969e-04 -8.08665613e-05  2.87659247e-04 -1.43885205e-03\n",
            "    -8.93966141e-04]\n",
            "   [ 8.12283114e-04 -3.97278840e-04 -1.39942124e-03 -6.47666219e-04\n",
            "     5.21683912e-04]]]\n",
            "\n",
            "\n",
            " [[[-8.49510474e-05 -1.76190571e-04  9.44832061e-05 -5.27610886e-04\n",
            "     3.07463696e-04]\n",
            "   [ 7.56829882e-04  1.26387378e-04  7.41674891e-05 -8.42804564e-04\n",
            "     1.08096497e-03]\n",
            "   [-6.85424626e-06 -1.05953819e-04  9.56072640e-05 -2.43403784e-03\n",
            "     1.61577660e-03]\n",
            "   [-5.46586451e-04 -7.69492083e-05  9.36535717e-04 -1.39347599e-03\n",
            "     1.19434390e-03]]\n",
            "\n",
            "  [[-1.37339134e-04 -3.52353479e-05  5.52979207e-05  1.22896233e-04\n",
            "    -4.80201455e-05]\n",
            "   [-3.67357846e-04  1.64452901e-04  1.40608229e-05  2.44143197e-04\n",
            "     5.94975131e-04]\n",
            "   [ 8.20097994e-04 -1.34308082e-03 -9.87415032e-04  8.19633351e-04\n",
            "     2.75932770e-04]\n",
            "   [ 2.32141717e-04 -1.45050328e-03 -1.02363338e-03  1.83901606e-03\n",
            "     2.83636633e-04]]\n",
            "\n",
            "  [[-2.85970038e-04 -2.40109380e-04  1.72149238e-04 -2.46826999e-04\n",
            "     2.62121317e-05]\n",
            "   [ 4.15311144e-04  3.46198787e-04  1.19786221e-04 -7.94415169e-04\n",
            "    -3.66331892e-04]\n",
            "   [ 1.10513049e-03 -4.06813514e-04  3.73910790e-05  2.85027563e-05\n",
            "    -2.94191587e-04]\n",
            "   [ 6.88054816e-04  2.05889449e-04 -7.13869940e-05  6.13933883e-04\n",
            "    -1.41037571e-03]]]]\n",
            "grad_weights: [[[[ 0.23515265 -0.1803296 ]\n",
            "   [-0.0130581  -0.49619615]]\n",
            "\n",
            "  [[ 0.07644738 -0.28463703]\n",
            "   [ 0.18371609 -0.04474137]]]\n",
            "\n",
            "\n",
            " [[[-0.17361174 -0.07918409]\n",
            "   [-0.2878075  -0.16228482]]\n",
            "\n",
            "  [[-0.03757894  0.15618499]\n",
            "   [-0.06149358 -0.27053571]]]\n",
            "\n",
            "\n",
            " [[[ 0.04589665  0.22453264]\n",
            "   [-0.34523898  0.7480495 ]]\n",
            "\n",
            "  [[ 0.46293343  0.10280008]\n",
            "   [-0.01203683 -0.19955456]]]]\n"
          ]
        }
      ],
      "source": [
        "small_input = test_input[:2]\n",
        "print(f\"test_input: {small_input}\")\n",
        "grad_input = test_conv.backward(small_input, test_conv.forward(small_input))\n",
        "print(f\"test_input: {small_input}\")\n",
        "print(f\"test_input: {grad_input}\")\n",
        "print(f\"grad_weights: {test_conv.grad_weights}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 521,
      "metadata": {
        "id": "p1iE85PpMm0Z"
      },
      "outputs": [],
      "source": [
        "class MaxPool2d(Layer):\n",
        "    def __init__(self, kernel_size):\n",
        "        self.kernel_size = kernel_size\n",
        "        self.max_elem_mask = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Perform an pooling:\n",
        "\n",
        "        output_height = input_height // kernel_size\n",
        "        output_width = input_width // kernel_size\n",
        "\n",
        "        input shape: [batch, input_channels, input_height, input_width]\n",
        "        output shape: [batch, input_channels, output_height, output_width]\n",
        "        \"\"\"\n",
        "        kernel_size = self.kernel_size\n",
        "        batch = np.shape(input)[0]\n",
        "        input_chanels = np.shape(input)[1]\n",
        "        output_height = np.shape(input)[2] // kernel_size\n",
        "        output_width = np.shape(input)[3] // kernel_size\n",
        "        output = np.empty((output_height, output_width, batch, input_chanels))\n",
        "        self.max_elem_mask = np.zeros((batch, input_chanels, np.shape(input)[2], np.shape(input)[3]))\n",
        "        for i in range(output_height):\n",
        "            for j in range(output_width):\n",
        "                i_pos = i * kernel_size\n",
        "                j_pos = j * kernel_size\n",
        "                input_slice = input[:, :, i_pos:i_pos+kernel_size, j_pos:j_pos+kernel_size]\n",
        "                max_tensor = np.max(input_slice, axis=(2, 3))\n",
        "# How should I handlel such indexes convertions???????\n",
        "                output[i, j] = max_tensor\n",
        "                self.max_elem_mask[:, :, i_pos:i_pos+kernel_size, j_pos:j_pos+kernel_size] = \\\n",
        "                    np.equal(input_slice, np.reshape(max_tensor, \n",
        "                                                     (np.shape(max_tensor)[0],\n",
        "                                                      np.shape(max_tensor)[1],\n",
        "                                                     1, 1)))\n",
        "        return np.moveaxis(output, [0, 1], [2, 3])\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        type = input.dtype\n",
        "        kernel_size = self.kernel_size\n",
        "        grad_input = self.max_elem_mask\n",
        "        for i in range(np.shape(grad_output)[2]):\n",
        "            for j in range(np.shape(grad_output)[3]):\n",
        "                i_pos = i * kernel_size\n",
        "                j_pos = j * kernel_size\n",
        "                grad_input[:, :, i_pos:i_pos+kernel_size, j_pos:j_pos+kernel_size] = \\\n",
        "                    np.multiply(grad_output[:, :, i:i+1, j:j+1], \n",
        "                                self.max_elem_mask[:, :, i_pos:i_pos+kernel_size, j_pos:j_pos+kernel_size],\n",
        "                                dtype=type)\n",
        "        self.max_elem_mask = None\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 522,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input: [[[[-0.34684817 -0.30095996  0.47312227 -0.62475388 -1.15468839]\n",
            "   [-1.58399781  1.08904639  0.47824314  0.38111637  0.18424932]\n",
            "   [-0.05582002 -1.53952863  0.0475645  -0.34929211  2.06039877]\n",
            "   [-1.23538066 -0.19646768  0.57744411 -1.50372514  0.23136769]]\n",
            "\n",
            "  [[-0.07067682 -0.86512453  1.16928108  0.09670819  0.09698619]\n",
            "   [ 0.53434696 -0.4514494   0.10570858  0.90779031  0.44230828]\n",
            "   [ 0.29505433  0.63284878  0.13110665  0.8929546   0.61450229]\n",
            "   [-1.70832847 -1.2328668   0.94000501  0.32740124  0.0039272 ]]\n",
            "\n",
            "  [[-1.52173351 -0.62847767  1.45074686  0.42137873  0.42252007]\n",
            "   [ 1.29829819 -1.20193603 -1.08414318 -2.5650369  -0.18129772]\n",
            "   [ 1.37066407 -0.042622    0.27686214 -0.30265044  0.8222425 ]\n",
            "   [-0.1795455   0.71729322 -1.84618087 -1.6823786   0.33753531]]]]\n",
            "output: [[[[ 1.08904639  0.47824314]\n",
            "   [-0.05582002  0.57744411]]\n",
            "\n",
            "  [[ 0.53434696  1.16928108]\n",
            "   [ 0.63284878  0.94000501]]\n",
            "\n",
            "  [[ 1.29829819  1.45074686]\n",
            "   [ 1.37066407  0.27686214]]]\n",
            "\n",
            "\n",
            " [[[ 0.60942677  0.41872027]\n",
            "   [ 0.54708318  0.47725385]]\n",
            "\n",
            "  [[ 0.98903649  0.18733366]\n",
            "   [ 0.42541621  2.12769421]]\n",
            "\n",
            "  [[ 0.1218273   0.6256123 ]\n",
            "   [ 1.80511636  2.59867792]]]\n",
            "\n",
            "\n",
            " [[[ 0.51466144  1.99188196]\n",
            "   [ 0.93846819 -0.31082275]]\n",
            "\n",
            "  [[ 1.79995072  1.67910407]\n",
            "   [ 0.83373042  0.36380456]]\n",
            "\n",
            "  [[ 1.83885172  1.0163071 ]\n",
            "   [ 0.17586255  0.56181987]]]\n",
            "\n",
            "\n",
            " [[[ 0.51450149  2.52649414]\n",
            "   [ 0.84937687  0.8323543 ]]\n",
            "\n",
            "  [[ 0.91131666  0.61024353]\n",
            "   [ 2.0897489   1.44934841]]\n",
            "\n",
            "  [[ 0.14911939  1.44386786]\n",
            "   [ 0.69475755  1.96245875]]]\n",
            "\n",
            "\n",
            " [[[ 0.44856212  0.6285583 ]\n",
            "   [ 2.38238189  1.05008494]]\n",
            "\n",
            "  [[ 1.01688875  1.55527905]\n",
            "   [ 1.2422994   1.58032719]]\n",
            "\n",
            "  [[ 2.71974532  1.96704734]\n",
            "   [ 1.72661701  1.24660494]]]\n",
            "\n",
            "\n",
            " [[[ 1.25313272  0.79247087]\n",
            "   [ 0.6137563   0.60529646]]\n",
            "\n",
            "  [[-0.01102436  1.19371493]\n",
            "   [ 1.00440855  3.06348499]]\n",
            "\n",
            "  [[ 0.40589581  1.71125154]\n",
            "   [ 1.64676651  0.9849087 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.77998956  0.29094921]\n",
            "   [ 0.12906892  0.54184881]]\n",
            "\n",
            "  [[ 1.2310379   0.8642693 ]\n",
            "   [ 1.35554579  0.0307123 ]]\n",
            "\n",
            "  [[ 0.57448209  0.75094947]\n",
            "   [ 0.85169591  1.00737684]]]\n",
            "\n",
            "\n",
            " [[[ 0.23588196  0.96483003]\n",
            "   [ 2.60725788  1.94413093]]\n",
            "\n",
            "  [[ 0.67001153  1.14199661]\n",
            "   [ 0.91915765  1.19320094]]\n",
            "\n",
            "  [[ 0.26617442  0.80556141]\n",
            "   [ 0.96212439  1.68363215]]]\n",
            "\n",
            "\n",
            " [[[ 1.36038085  1.22953606]\n",
            "   [ 1.59574174  1.00848306]]\n",
            "\n",
            "  [[ 1.48532554  1.35260481]\n",
            "   [ 1.32387901  0.75991785]]\n",
            "\n",
            "  [[ 0.862248    0.26372811]\n",
            "   [ 1.98275234  1.7502806 ]]]\n",
            "\n",
            "\n",
            " [[[ 1.40534646 -0.3120857 ]\n",
            "   [ 1.51659486  0.6535202 ]]\n",
            "\n",
            "  [[ 0.84703691  1.62594609]\n",
            "   [ 0.68692168  1.08729548]]\n",
            "\n",
            "  [[ 0.42492394  0.39208731]\n",
            "   [ 0.92396242  1.06646109]]]]\n",
            "mask: [[[[0. 0. 0. 0. 0.]\n",
            "   [0. 1. 1. 0. 0.]\n",
            "   [1. 0. 0. 0. 0.]\n",
            "   [0. 0. 1. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 1. 0. 0.]\n",
            "   [1. 0. 0. 0. 0.]\n",
            "   [0. 1. 0. 0. 0.]\n",
            "   [0. 0. 1. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 1. 0. 0.]\n",
            "   [1. 0. 0. 0. 0.]\n",
            "   [1. 0. 1. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[1. 0. 0. 0. 0.]\n",
            "   [0. 0. 1. 0. 0.]\n",
            "   [1. 0. 0. 0. 0.]\n",
            "   [0. 0. 1. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. 1. 0.]\n",
            "   [0. 1. 0. 0. 0.]\n",
            "   [1. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 1. 0.]]\n",
            "\n",
            "  [[0. 0. 0. 0. 0.]\n",
            "   [1. 0. 0. 1. 0.]\n",
            "   [1. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 1. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. 1. 0.]\n",
            "   [0. 1. 0. 0. 0.]\n",
            "   [1. 0. 0. 1. 0.]\n",
            "   [0. 0. 0. 0. 0.]]\n",
            "\n",
            "  [[1. 0. 0. 0. 0.]\n",
            "   [0. 0. 1. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 1. 1. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. 1. 0.]\n",
            "   [1. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 1. 1. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 1. 0. 0.]\n",
            "   [1. 0. 0. 0. 0.]\n",
            "   [0. 1. 1. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. 0. 0.]\n",
            "   [1. 0. 0. 1. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 1. 1. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 1. 0. 0.]\n",
            "   [0. 1. 0. 0. 0.]\n",
            "   [0. 0. 1. 0. 0.]\n",
            "   [0. 1. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[1. 0. 1. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 1. 1. 0. 0.]]\n",
            "\n",
            "  [[0. 1. 1. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [1. 0. 0. 1. 0.]\n",
            "   [0. 0. 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. 1. 0.]\n",
            "   [1. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [1. 0. 1. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 1. 1. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [1. 0. 0. 0. 0.]\n",
            "   [0. 0. 1. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 1. 0. 0.]\n",
            "   [0. 1. 0. 0. 0.]\n",
            "   [0. 1. 1. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. 0. 0.]\n",
            "   [1. 0. 1. 0. 0.]\n",
            "   [0. 1. 1. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[1. 0. 0. 1. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 1. 1. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 1. 0. 0.]\n",
            "   [1. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 1. 0.]\n",
            "   [0. 1. 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 1. 0. 0.]\n",
            "   [1. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [1. 0. 1. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. 1. 0.]\n",
            "   [1. 0. 0. 0. 0.]\n",
            "   [1. 0. 0. 1. 0.]\n",
            "   [0. 0. 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. 1. 0.]\n",
            "   [1. 0. 0. 0. 0.]\n",
            "   [1. 0. 0. 0. 0.]\n",
            "   [0. 0. 1. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. 1. 0.]\n",
            "   [0. 1. 0. 0. 0.]\n",
            "   [0. 1. 0. 1. 0.]\n",
            "   [0. 0. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 1. 0. 0.]\n",
            "   [1. 0. 0. 0. 0.]\n",
            "   [1. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 1. 0.]]\n",
            "\n",
            "  [[1. 0. 1. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 1. 0. 0.]\n",
            "   [1. 0. 0. 0. 0.]]\n",
            "\n",
            "  [[0. 1. 0. 0. 0.]\n",
            "   [0. 0. 0. 1. 0.]\n",
            "   [0. 1. 0. 1. 0.]\n",
            "   [0. 0. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. 0. 0.]\n",
            "   [1. 0. 1. 0. 0.]\n",
            "   [1. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 1. 0.]]\n",
            "\n",
            "  [[0. 1. 1. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 1. 0. 0. 0.]\n",
            "   [0. 0. 1. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. 1. 0.]\n",
            "   [1. 0. 0. 0. 0.]\n",
            "   [0. 0. 1. 0. 0.]\n",
            "   [1. 0. 0. 0. 0.]]]]\n",
            "grad shape: [[[[ 0.          0.          0.          0.          0.        ]\n",
            "   [ 0.          1.08904639  0.47824314  0.          0.        ]\n",
            "   [-0.05582002 -0.          0.          0.          0.        ]\n",
            "   [-0.         -0.          0.57744411  0.          0.        ]]\n",
            "\n",
            "  [[ 0.          0.          1.16928108  0.          0.        ]\n",
            "   [ 0.53434696  0.          0.          0.          0.        ]\n",
            "   [ 0.          0.63284878  0.          0.          0.        ]\n",
            "   [ 0.          0.          0.94000501  0.          0.        ]]\n",
            "\n",
            "  [[ 0.          0.          1.45074686  0.          0.        ]\n",
            "   [ 1.29829819  0.          0.          0.          0.        ]\n",
            "   [ 1.37066407  0.          0.27686214  0.          0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]]]\n",
            "\n",
            "\n",
            " [[[ 0.60942677  0.          0.          0.          0.        ]\n",
            "   [ 0.          0.          0.41872027  0.          0.        ]\n",
            "   [ 0.54708318  0.          0.          0.          0.        ]\n",
            "   [ 0.          0.          0.47725385  0.          0.        ]]\n",
            "\n",
            "  [[ 0.          0.          0.          0.18733366  0.        ]\n",
            "   [ 0.          0.98903649  0.          0.          0.        ]\n",
            "   [ 0.42541621  0.          0.          0.          0.        ]\n",
            "   [ 0.          0.          0.          2.12769421  0.        ]]\n",
            "\n",
            "  [[ 0.          0.          0.          0.          0.        ]\n",
            "   [ 0.1218273   0.          0.          0.6256123   0.        ]\n",
            "   [ 1.80511636  0.          0.          0.          0.        ]\n",
            "   [ 0.          0.          0.          2.59867792  0.        ]]]\n",
            "\n",
            "\n",
            " [[[ 0.          0.          0.          1.99188196  0.        ]\n",
            "   [ 0.          0.51466144  0.          0.          0.        ]\n",
            "   [ 0.93846819  0.         -0.         -0.31082275  0.        ]\n",
            "   [ 0.          0.         -0.         -0.          0.        ]]\n",
            "\n",
            "  [[ 1.79995072  0.          0.          0.          0.        ]\n",
            "   [ 0.          0.          1.67910407  0.          0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]\n",
            "   [ 0.          0.83373042  0.36380456  0.          0.        ]]\n",
            "\n",
            "  [[ 0.          0.          0.          1.0163071   0.        ]\n",
            "   [ 1.83885172  0.          0.          0.          0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]\n",
            "   [ 0.          0.17586255  0.56181987  0.          0.        ]]]\n",
            "\n",
            "\n",
            " [[[ 0.          0.          2.52649414  0.          0.        ]\n",
            "   [ 0.51450149  0.          0.          0.          0.        ]\n",
            "   [ 0.          0.84937687  0.8323543   0.          0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]]\n",
            "\n",
            "  [[ 0.          0.          0.          0.          0.        ]\n",
            "   [ 0.91131666  0.          0.          0.61024353  0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]\n",
            "   [ 0.          2.0897489   1.44934841  0.          0.        ]]\n",
            "\n",
            "  [[ 0.          0.          1.44386786  0.          0.        ]\n",
            "   [ 0.          0.14911939  0.          0.          0.        ]\n",
            "   [ 0.          0.          1.96245875  0.          0.        ]\n",
            "   [ 0.          0.69475755  0.          0.          0.        ]]]\n",
            "\n",
            "\n",
            " [[[ 0.44856212  0.          0.6285583   0.          0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]\n",
            "   [ 0.          2.38238189  1.05008494  0.          0.        ]]\n",
            "\n",
            "  [[ 0.          1.01688875  1.55527905  0.          0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]\n",
            "   [ 1.2422994   0.          0.          1.58032719  0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]]\n",
            "\n",
            "  [[ 0.          0.          0.          1.96704734  0.        ]\n",
            "   [ 2.71974532  0.          0.          0.          0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]\n",
            "   [ 1.72661701  0.          1.24660494  0.          0.        ]]]\n",
            "\n",
            "\n",
            " [[[ 0.          1.25313272  0.79247087  0.          0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]\n",
            "   [ 0.6137563   0.          0.          0.          0.        ]\n",
            "   [ 0.          0.          0.60529646  0.          0.        ]]\n",
            "\n",
            "  [[-0.         -0.          1.19371493  0.          0.        ]\n",
            "   [-0.         -0.01102436  0.          0.          0.        ]\n",
            "   [ 0.          1.00440855  3.06348499  0.          0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]]\n",
            "\n",
            "  [[ 0.          0.          0.          0.          0.        ]\n",
            "   [ 0.40589581  0.          1.71125154  0.          0.        ]\n",
            "   [ 0.          1.64676651  0.9849087   0.          0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]]]\n",
            "\n",
            "\n",
            " [[[ 1.77998956  0.          0.          0.29094921  0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]\n",
            "   [ 0.          0.12906892  0.54184881  0.          0.        ]]\n",
            "\n",
            "  [[ 0.          0.          0.8642693   0.          0.        ]\n",
            "   [ 1.2310379   0.          0.          0.          0.        ]\n",
            "   [ 0.          0.          0.          0.0307123   0.        ]\n",
            "   [ 0.          1.35554579  0.          0.          0.        ]]\n",
            "\n",
            "  [[ 0.          0.          0.75094947  0.          0.        ]\n",
            "   [ 0.57448209  0.          0.          0.          0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]\n",
            "   [ 0.85169591  0.          1.00737684  0.          0.        ]]]\n",
            "\n",
            "\n",
            " [[[ 0.          0.          0.          0.96483003  0.        ]\n",
            "   [ 0.23588196  0.          0.          0.          0.        ]\n",
            "   [ 2.60725788  0.          0.          1.94413093  0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]]\n",
            "\n",
            "  [[ 0.          0.          0.          1.14199661  0.        ]\n",
            "   [ 0.67001153  0.          0.          0.          0.        ]\n",
            "   [ 0.91915765  0.          0.          0.          0.        ]\n",
            "   [ 0.          0.          1.19320094  0.          0.        ]]\n",
            "\n",
            "  [[ 0.          0.          0.          0.80556141  0.        ]\n",
            "   [ 0.          0.26617442  0.          0.          0.        ]\n",
            "   [ 0.          0.96212439  0.          1.68363215  0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]]]\n",
            "\n",
            "\n",
            " [[[ 0.          0.          1.22953606  0.          0.        ]\n",
            "   [ 1.36038085  0.          0.          0.          0.        ]\n",
            "   [ 1.59574174  0.          0.          0.          0.        ]\n",
            "   [ 0.          0.          0.          1.00848306  0.        ]]\n",
            "\n",
            "  [[ 1.48532554  0.          1.35260481  0.          0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]\n",
            "   [ 0.          0.          0.75991785  0.          0.        ]\n",
            "   [ 1.32387901  0.          0.          0.          0.        ]]\n",
            "\n",
            "  [[ 0.          0.862248    0.          0.          0.        ]\n",
            "   [ 0.          0.          0.          0.26372811  0.        ]\n",
            "   [ 0.          1.98275234  0.          1.7502806   0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]]]\n",
            "\n",
            "\n",
            " [[[ 0.          0.         -0.         -0.          0.        ]\n",
            "   [ 1.40534646  0.         -0.3120857  -0.          0.        ]\n",
            "   [ 1.51659486  0.          0.          0.          0.        ]\n",
            "   [ 0.          0.          0.          0.6535202   0.        ]]\n",
            "\n",
            "  [[ 0.          0.84703691  1.62594609  0.          0.        ]\n",
            "   [ 0.          0.          0.          0.          0.        ]\n",
            "   [ 0.          0.68692168  0.          0.          0.        ]\n",
            "   [ 0.          0.          1.08729548  0.          0.        ]]\n",
            "\n",
            "  [[ 0.          0.          0.          0.39208731  0.        ]\n",
            "   [ 0.42492394  0.          0.          0.          0.        ]\n",
            "   [ 0.          0.          1.06646109  0.          0.        ]\n",
            "   [ 0.92396242  0.          0.          0.          0.        ]]]]\n"
          ]
        }
      ],
      "source": [
        "test_maxpool = MaxPool2d(2)\n",
        "print(f\"input: {test_input[:1]}\")\n",
        "test_output = test_maxpool.forward(test_input)\n",
        "print(f\"output: {test_output}\")\n",
        "print(f\"mask: {test_maxpool.max_elem_mask}\")\n",
        "test_backward = test_maxpool.backward(test_input, test_output)\n",
        "print(f\"grad shape: {test_backward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 523,
      "metadata": {
        "id": "3W-GM45xSqJD"
      },
      "outputs": [],
      "source": [
        "class Flatten(Layer):\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Perform an flatten operation:\n",
        "\n",
        "        input shape: [batch, input_channels, input_height, input_width]\n",
        "        output shape: [batch, input_channels * output_height * output_width]\n",
        "        \"\"\"\n",
        "        flattened_size = np.shape(input)[1] * np.shape(input)[2] * np.shape(input)[3]\n",
        "        return np.reshape(input, (np.shape(input)[0], flattened_size))\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        return np.reshape(grad_output, np.shape(input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 524,
      "metadata": {
        "id": "iCuNsUikOoE1",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def softmax_crossentropy_with_logits(logits,reference_answers):\n",
        "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
        "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
        "\n",
        "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
        "\n",
        "    return xentropy\n",
        "\n",
        "def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n",
        "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
        "    ones_for_answers = np.zeros_like(logits)\n",
        "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
        "\n",
        "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
        "\n",
        "    return (- ones_for_answers + softmax) / logits.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muokQIA3UcnL"
      },
      "source": [
        "## Имплементация оптимизатора и изменения learning rate'a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBzMVan_XzFM"
      },
      "source": [
        "В имплементации этих двух классов есть небольшие неточности.\n",
        "Посмотрите, как сделана имплементация метода моментов в Pytorch и добавьте пропущенное.\n",
        "\n",
        "> Добавлять моменты Нестерова не нужно!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 525,
      "metadata": {
        "id": "n4hyzuIcUqLd"
      },
      "outputs": [],
      "source": [
        "class SGDOptimizer:\n",
        "    def __init__(self, model, momentum=0.9, dampening=0.0, weight_decay=0.0):\n",
        "        \"\"\"\n",
        "        Wrapper which perfoms weights update\n",
        "        \"\"\"\n",
        "        self.momentum = momentum\n",
        "        self.dampening = dampening\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        self.model = model\n",
        "        self.momentum_buffer = [None] * len(model)\n",
        "        \n",
        "\n",
        "    def step(self, grad_weights, lr=0.1):\n",
        "        \"\"\"\n",
        "        Update weights\n",
        "        \"\"\"\n",
        "        # For no there is no input parameter\n",
        "        #if self.weight_decay != 0:\n",
        "        #  grad_weights = grad_weights + self.weight_decay * input\n",
        "\n",
        "        for layer, layer_moment, layer_grad_weights in zip(self.model, \n",
        "                                                           self.momentum_buffer,\n",
        "                                                           grad_weights):    \n",
        "            layer_weights = layer.get_weights()\n",
        "            if layer_weights is None:\n",
        "                continue\n",
        "\n",
        "            if self.momentum != 0:\n",
        "                if layer_moment is None:\n",
        "                    layer_moment = layer_grad_weights\n",
        "                else:\n",
        "                    layer_moment = self.momentum * layer_moment + \\\n",
        "                                        (1 - self.dampening) * layer_grad_weights\n",
        "                layer.set_weights(layer_weights - lr * layer_moment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 526,
      "metadata": {
        "id": "4DX-leurYblp"
      },
      "outputs": [],
      "source": [
        "class LRScheduler:\n",
        "    def __init__(self, lr):\n",
        "        \"\"\"\n",
        "        Wrapper which perfoms learning rate updates\n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "        self.current_step = 0\n",
        "\n",
        "    def get_lr(self):\n",
        "      \"\"\"\n",
        "      Update learing rate for current iteration\n",
        "      \"\"\"\n",
        "      current_lr = self.lr / (self.current_step + 1)\n",
        "      #<your code here>\n",
        "      self.current_step += 1\n",
        "      return current_lr\n",
        "\n",
        "    def reset(self):\n",
        "       self.current_step = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlEURTE9OoE5",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Итоговая нейросеть\n",
        "\n",
        "Все готово для запуска нейросети. Нейросеть будем тестировать на классическом датасете MNIST. Код ниже визуализирует несколько примеров из этого датасета."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 535,
      "metadata": {
        "id": "2KKQ81e0OoE6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x600 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist(flatten=False)\n",
        "\n",
        "plt.figure(figsize=[6,6])\n",
        "for i in range(4):\n",
        "    plt.subplot(2,2,i+1)\n",
        "    plt.title(\"Label: %i\"%y_train[i])\n",
        "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGXtnvX4OoE7",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "В нашей реализации сеть - просто список (Python-list) слоев.\n",
        "\n",
        "\n",
        "\n",
        "> Обратите внимание, что у нас нет глобального пулинга. При изменении архитектуры сети вы должны поменять входую размерность в последнем Dense слое\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 536,
      "metadata": {
        "id": "VicezF_TOoE8",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "network = []\n",
        "network.append(Conv2d(1, 4, 5))\n",
        "#network.append(MaxPool2d(2))\n",
        "network.append(ReLU())\n",
        "network.append(Conv2d(4, 8, 5))\n",
        "#network.append(MaxPool2d(2))\n",
        "network.append(ReLU())\n",
        "network.append(Flatten())\n",
        "network.append(Dense(3200, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 537,
      "metadata": {
        "id": "1sUrd667ZDuK"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "\n",
        "optimizer = SGDOptimizer(network)\n",
        "scheduler = LRScheduler(learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "482hIf_UOoE9",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Реализуйте прямой проход по целой сети, последовательно вызывая .forward() для каждого слоя."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 538,
      "metadata": {
        "id": "OKRyUyj5OoE9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def forward(network, X):\n",
        "    \"\"\"\n",
        "    Compute activations of all network layers by applying them sequentially.\n",
        "    Return a list of activations for each layer.\n",
        "    Make sure last activation corresponds to network logits.\n",
        "    \"\"\"\n",
        "    activations = []\n",
        "    input = X\n",
        "\n",
        "    for layer in network:\n",
        "        input = layer.forward(input)\n",
        "        activations.append(input)\n",
        "\n",
        "    assert len(activations) == len(network)\n",
        "    return activations\n",
        "\n",
        "def predict(network, X):\n",
        "    \"\"\"\n",
        "    Use network to predict the most likely class for each sample.\n",
        "    \"\"\"\n",
        "    logits = forward(network, X)[-1]\n",
        "    return logits.argmax(axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 539,
      "metadata": {
        "id": "UfUyHKPyOoE-",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def train(network, X, y, optimizer, scheduler):\n",
        "   \"\"\"\n",
        "   Train your network on a given batch of X and y.\n",
        "   You first need to run forward to get all layer activations.\n",
        "   Then you can run layer.backward going from last to first layer.\n",
        "\n",
        "   After you called backward for all layers, all Dense layers have already made one gradient step.\n",
        "   \"\"\"\n",
        "\n",
        "   # Get the layer activations\n",
        "   layer_activations = forward(network,X)\n",
        "   layer_inputs = [X] + layer_activations  #layer_input[i] is an input for network[i]\n",
        "   logits = layer_activations[-1]\n",
        "\n",
        "   # Compute the loss and the initial gradient\n",
        "   loss = softmax_crossentropy_with_logits(logits,y)\n",
        "   loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
        "\n",
        "   grad_weights = []\n",
        "   for i in range(0, len(network)):\n",
        "      cur_pos = len(network) - 1 - i\n",
        "      loss_grad = network[cur_pos].backward(layer_inputs[cur_pos], loss_grad)\n",
        "      grad_weights.insert(0, network[cur_pos].get_grad_weights())\n",
        "\n",
        "   lr = lr = scheduler.get_lr()\n",
        "   # Optimizer performs step for all the weights captured before\n",
        "   optimizer.step(grad_weights, lr)\n",
        "   \n",
        "   return np.mean(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJG4VMsROoE_",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Все готово для запуска обучения. Если все реализовано корректно, то точность классификации на валидационном множестве **должна быть около** 99%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 540,
      "metadata": {
        "id": "kq5XTDNNOoE_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
        "    assert len(inputs) == len(targets)\n",
        "    if shuffle:\n",
        "        indices = np.random.permutation(len(inputs))\n",
        "    for start_idx in tqdm(range(0, len(inputs) - batchsize + 1, batchsize)):\n",
        "        if shuffle:\n",
        "            excerpt = indices[start_idx:start_idx + batchsize]\n",
        "        else:\n",
        "            excerpt = slice(start_idx, start_idx + batchsize)\n",
        "        yield inputs[excerpt], targets[excerpt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 541,
      "metadata": {
        "id": "7q2KcwkTOoFA",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "train_log = []\n",
        "val_log = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 542,
      "metadata": {
        "id": "kaxQu9WsOoFB",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Train accuracy: 0.11356\n",
            "Val accuracy: 0.1064\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06f55e38fa024b3c8c6b3b86986552cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1562 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[542], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x_batch,y_batch \u001b[38;5;129;01min\u001b[39;00m iterate_minibatches(X_train, y_train, batchsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m----> 4\u001b[0m         train(network, x_batch, y_batch, optimizer, scheduler)\n\u001b[1;32m      6\u001b[0m     train_log\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(predict(network, X_train) \u001b[38;5;241m==\u001b[39m y_train))\n\u001b[1;32m      7\u001b[0m     val_log\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(predict(network, X_val) \u001b[38;5;241m==\u001b[39m y_val))\n",
            "Cell \u001b[0;32mIn[539], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(network, X, y, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(network)):\n\u001b[1;32m     21\u001b[0m    cur_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(network) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m i\n\u001b[0;32m---> 22\u001b[0m    loss_grad \u001b[38;5;241m=\u001b[39m network[cur_pos]\u001b[38;5;241m.\u001b[39mbackward(layer_inputs[cur_pos], loss_grad)\n\u001b[1;32m     23\u001b[0m    grad_weights\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, network[cur_pos]\u001b[38;5;241m.\u001b[39mget_grad_weights())\n\u001b[1;32m     25\u001b[0m lr \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39mget_lr()\n",
            "Cell \u001b[0;32mIn[515], line 100\u001b[0m, in \u001b[0;36mConv2d.backward\u001b[0;34m(self, input, grad_output)\u001b[0m\n\u001b[1;32m     98\u001b[0m tmp_weights \u001b[38;5;241m=\u001b[39m Conv2d\u001b[38;5;241m.\u001b[39m_flip_transpose(tmp_weights)\n\u001b[1;32m     99\u001b[0m tmp_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmoveaxis(tmp_weights, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m grad_input \u001b[38;5;241m=\u001b[39m Conv2d\u001b[38;5;241m.\u001b[39m_conv(grad_padded, tmp_weights, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Gradient step is in a separate optimizer\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_weights\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_biases\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases\u001b[38;5;241m.\u001b[39mshape\n",
            "Cell \u001b[0;32mIn[515], line 30\u001b[0m, in \u001b[0;36mConv2d._conv\u001b[0;34m(input, weights, biases)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(output_height):\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(output_width):\n\u001b[1;32m     29\u001b[0m             filters[filter_idx, i, j] \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m---> 30\u001b[0m                 np\u001b[38;5;241m.\u001b[39mtensordot(\u001b[38;5;28minput\u001b[39m[:, :, i:i\u001b[38;5;241m+\u001b[39mkernel_height, j:j\u001b[38;5;241m+\u001b[39mkernel_width],\n\u001b[1;32m     31\u001b[0m                              weights[:, filter_idx, :, :],\n\u001b[1;32m     32\u001b[0m                              axes\u001b[38;5;241m=\u001b[39m([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m]))\n\u001b[1;32m     33\u001b[0m output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrollaxis(filters, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m biases \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Here values from biases are broadcasted\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/ML/lib/python3.11/site-packages/numpy/core/numeric.py:1119\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(a, b, axes)\u001b[0m\n\u001b[1;32m   1116\u001b[0m newshape_b \u001b[38;5;241m=\u001b[39m (N2, \u001b[38;5;28mint\u001b[39m(multiply\u001b[38;5;241m.\u001b[39mreduce([bs[ax] \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m notin])))\n\u001b[1;32m   1117\u001b[0m oldb \u001b[38;5;241m=\u001b[39m [bs[axis] \u001b[38;5;28;01mfor\u001b[39;00m axis \u001b[38;5;129;01min\u001b[39;00m notin]\n\u001b[0;32m-> 1119\u001b[0m at \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mtranspose(newaxes_a)\u001b[38;5;241m.\u001b[39mreshape(newshape_a)\n\u001b[1;32m   1120\u001b[0m bt \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mtranspose(newaxes_b)\u001b[38;5;241m.\u001b[39mreshape(newshape_b)\n\u001b[1;32m   1121\u001b[0m res \u001b[38;5;241m=\u001b[39m dot(at, bt)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(15):\n",
        "\n",
        "    for x_batch,y_batch in iterate_minibatches(X_train, y_train, batchsize=32, shuffle=True):\n",
        "        train(network, x_batch, y_batch, optimizer, scheduler)\n",
        "\n",
        "    train_log.append(np.mean(predict(network, X_train) == y_train))\n",
        "    val_log.append(np.mean(predict(network, X_val) == y_val))\n",
        "\n",
        "    clear_output()\n",
        "    print(\"Epoch\",epoch)\n",
        "    print(\"Train accuracy:\",train_log[-1])\n",
        "    print(\"Val accuracy:\",val_log[-1])\n",
        "    plt.plot(train_log,label='train accuracy')\n",
        "    plt.plot(val_log,label='val accuracy')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
