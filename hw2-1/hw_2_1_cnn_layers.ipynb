{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ3DJzu7FDX_"
      },
      "source": [
        "# Домашнее задание 2.1. Сверточные сети\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQVARDpwNsOa"
      },
      "source": [
        "В этом задании вы должны:\n",
        "1. Написать слой Conv2d на Numpy и определить в нем forward-backward методы\n",
        "2. Определить слой MaxPool2d\n",
        "3. Написать всю необходимую обвязку для обучения: оптимизатор с адаптивным шагом и класс, позволяющий изменять расписание для learning rate'а\n",
        "\n",
        "\n",
        "\n",
        "> Обратите внимание, что в этом задании больше нет тестов.\n",
        "> Вы должны сами проверять свой код.  \n",
        "> Это можно сделать так:\n",
        "> 1. Написать юнит-тесты с помощью Pytorch. То есть, ваш модудь должен повторять поведение torch'а\n",
        "> 2. Проверять архитектуру не на всем датасете, а на подвыборке: при наивной имплементации слоев одна эпоха на всем датасете будет занимать около двух часов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMVkqpoEOoD3",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Numpy-имплементация сверточной нейронной сети\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmTxOp7KOoEG",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Вставьте сюда имплементацию из первого домашнего задания.\n",
        "\n",
        "\n",
        "\n",
        "> Обратите внимание, что обновление весов теперь производится с помощью специального класса **Optimizer**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lmfLBp4tOoEN",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "def load_mnist(flatten=False):\n",
        "    \"\"\"taken from https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py\"\"\"\n",
        "    # We first define a download function, supporting both Python 2 and 3.\n",
        "    if sys.version_info[0] == 2:\n",
        "        from urllib import urlretrieve\n",
        "    else:\n",
        "        from urllib.request import urlretrieve\n",
        "\n",
        "    def download(filename, source='https://ossci-datasets.s3.amazonaws.com/mnist/'):\n",
        "        print(\"Downloading %s\" % filename)\n",
        "        urlretrieve(source + filename, filename)\n",
        "\n",
        "    # We then define functions for loading MNIST images and labels.\n",
        "    # For convenience, they also download the requested files if needed.\n",
        "    import gzip\n",
        "\n",
        "    def load_mnist_images(filename):\n",
        "        if not os.path.exists(filename):\n",
        "            download(filename)\n",
        "        # Read the inputs in Yann LeCun's binary format.\n",
        "        with gzip.open(filename, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
        "        # following the shape convention: (examples, channels, rows, columns)\n",
        "        data = data.reshape(-1, 1, 28, 28)\n",
        "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
        "        # (Actually to range [0, 255/256], for compatibility to the version\n",
        "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
        "        return data / np.float32(256)\n",
        "\n",
        "    def load_mnist_labels(filename):\n",
        "        if not os.path.exists(filename):\n",
        "            download(filename)\n",
        "        # Read the labels in Yann LeCun's binary format.\n",
        "        with gzip.open(filename, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "        # The labels are vectors of integers now, that's exactly what we want.\n",
        "        return data\n",
        "\n",
        "    # We can now download and read the training and test set images and labels.\n",
        "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
        "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
        "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
        "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "    # We reserve the last 10000 training examples for validation.\n",
        "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
        "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
        "\n",
        "    if flatten:\n",
        "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
        "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
        "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
        "\n",
        "    # We just return all the arrays in order, as expected in main().\n",
        "    # (It doesn't matter how we do this as long as we can read them again.)\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jYZ3ptaiOoER",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    \"\"\"\n",
        "    A building block. Each layer is capable of performing two things:\n",
        "\n",
        "    - Process input to get output:           output = layer.forward(input)\n",
        "\n",
        "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
        "\n",
        "    Some layers also have learnable parameters which they update during layer.backward.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"Here you can initialize layer parameters (if any) and auxiliary stuff.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n",
        "        \"\"\"\n",
        "        return input\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the layer, with respect to the given input.\n",
        "\n",
        "        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n",
        "\n",
        "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
        "\n",
        "        Luckily, you already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
        "\n",
        "        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss / d layer\n",
        "        \"\"\"\n",
        "        input_dim = input.shape[1]\n",
        "\n",
        "        d_layer_d_input = np.eye(input_dim)\n",
        "\n",
        "        return np.dot(grad_output, d_layer_d_input) # chain rule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Qe246l61OoEe",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
        "        pass\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Apply elementwise ReLU to [batch, input_units] matrix\"\"\"\n",
        "        output = np.array(input)\n",
        "        output[output < 0] = 0\n",
        "        return output\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"Compute gradient of loss w.r.t. ReLU input\"\"\"\n",
        "        relu_grad_mask = np.zeros_like(input)\n",
        "        relu_grad_mask[input > 0] = 1\n",
        "        return grad_output * relu_grad_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g22Gzs_2OoEl",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class Dense(Layer):\n",
        "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        A dense layer is a layer which performs a learned affine transformation:\n",
        "        f(x) = <W*x> + b\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # initialize weights with small random numbers from normal distribution\n",
        "        # you can change the intializtion method\n",
        "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
        "        self.biases = np.zeros(output_units)\n",
        "\n",
        "    def forward(self,input):\n",
        "        \"\"\"\n",
        "        Perform an affine transformation:\n",
        "        f(x) = <W*x> + b\n",
        "\n",
        "        input shape: [batch, input_units]\n",
        "        output shape: [batch, output_units]\n",
        "        \"\"\"\n",
        "        return np.dot(input, self.weights) + self.biases\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "\n",
        "        # compute d f / d x = d f / d dense * d dense / d x\n",
        "        # where d dense/ d x = weights transposed\n",
        "        # grad_output is a derivative of the next (in forward) layer of prediction\n",
        "        # this result is needed for the next layer\n",
        "        grad_input = np.dot(grad_output, np.transpose(self.weights))\n",
        "\n",
        "        # compute gradient w.r.t. weights and biases\n",
        "        grad_weights = np.dot(np.transpose(input), grad_output)\n",
        "        grad_biases = np.sum(grad_output, axis=0)\n",
        "\n",
        "        # Gradient step should be performed with the help of separate optimizer\n",
        "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "A1vN8h41ILY9"
      },
      "outputs": [],
      "source": [
        "class Conv2d(Layer):\n",
        "    def __init__(self, input_channels, output_channels, kernel_size, learning_rate=0.1):\n",
        "        self.weights = np.random.randn(input_channels, output_channels, kernel_size, kernel_size)*0.01\n",
        "        self.biases = np.zeros(output_channels)\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        self.grad_weights = None\n",
        "        self.grad_biases = None\n",
        "\n",
        "    def _conv(input, weights, biases):\n",
        "        kernel_height = np.shape(weights)[2]\n",
        "        kernel_width = np.shape(weights)[3]\n",
        "        output_chanels = np.shape(weights)[1]\n",
        "        # Output dimention is determined by the number of filters in the layer.\n",
        "        # Each filter is a tensor with the size (input_channels, kernel_size, kernel_size)\n",
        "        # (In this example second axis is an index of filter)\n",
        "        # Such a filter produces one number per convolution \n",
        "        #   and this convolution is performed for each part of the input tensor.\n",
        "        batch_size = np.shape(input)[0]\n",
        "        output_height = np.shape(input)[2] - kernel_height + 1\n",
        "        output_width = np.shape(input)[3] - kernel_width + 1\n",
        "\n",
        "        # This is the result of convollution of each filter\n",
        "        filters = np.empty((output_chanels, output_height, output_width, batch_size))\n",
        "        for filter_idx in range(output_chanels):\n",
        "            for i in range(output_height):\n",
        "                for j in range(output_width):\n",
        "                    filters[filter_idx, i, j] = \\\n",
        "                        np.tensordot(input[:, :, i:i+kernel_height, j:j+kernel_width],\n",
        "                                     weights[:, filter_idx, :, :],\n",
        "                                     axes=([1, 2, 3], [0, 1, 2]))\n",
        "        output = np.rollaxis(filters, 3, 0)\n",
        "        if biases is not None:\n",
        "        # Here values from biases are broadcasted\n",
        "            output += np.reshape(biases, (1, output_chanels, 1, 1))\n",
        "        return output\n",
        "\n",
        "    def _pad(tensor, val, new_height, new_width):\n",
        "        pad_h = (new_height - np.shape(tensor)[2]) // 2\n",
        "        pad_w = (new_width - np.shape(tensor)[3]) // 2\n",
        "        padding = ((0, 0), (0, 0), (pad_h, pad_h), (pad_w, pad_w))\n",
        "        res = np.pad(tensor, padding, 'constant', constant_values=(val, val))\n",
        "        assert np.shape(res)[2] == new_height and np.shape(res)[3] == new_width\n",
        "        return res\n",
        "    \n",
        "    def _flip_transpose(tensor):\n",
        "        tensor = np.flip(tensor, (2, 3))\n",
        "        tensor = np.moveaxis(tensor, 2, 3) # this should work\n",
        "        return tensor\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Perform an convolution:\n",
        "\n",
        "        output_height = input_height - kernel_size + 1\n",
        "        output_width = input_width - kernel_size + 1\n",
        "\n",
        "        input shape: [batch, input_channels, input_height, input_width]\n",
        "        output shape: [batch, output_channels, output_height, output_width]\n",
        "        \"\"\"\n",
        "        return Conv2d._conv(input, self.weights, self.biases)\n",
        "        \n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        # Grad output has the shape [batch, output_channels, output_height, output_width]\n",
        "        # I don't know how this is possible, but if has these axises, convolution just works\n",
        "        input = np.moveaxis(input, 0, 1)\n",
        "        grad_weights = Conv2d._conv(input, grad_output, None)\n",
        "        input = np.moveaxis(input, 0, 1)\n",
        "        # dldb = dldz * dzdb \n",
        "        # dzdb = 1\n",
        "        # dldz = grad_output\n",
        "        # Each biases' component is applied to each element with the fixed output_chanels axis,\n",
        "        #   so input gradient is the sum of all output gradients\n",
        "        grad_biases = np.sum(grad_output, axis=(0, 2, 3))\n",
        "\n",
        "        kernel_size = np.shape(self.weights)[2]\n",
        "        # Probably I shouldn't change input tensor, but whatever\n",
        "        grad_padded = Conv2d._pad(grad_output, 0,\n",
        "                                   np.shape(input)[2] + kernel_size - 1,\n",
        "                                   np.shape(input)[3] + kernel_size - 1)\n",
        "        tmp_weights = np.copy(self.weights)\n",
        "        tmp_weights = Conv2d._flip_transpose(tmp_weights)\n",
        "        tmp_weights = np.moveaxis(tmp_weights, 0, 1)\n",
        "        grad_input = Conv2d._conv(grad_padded, tmp_weights, None)\n",
        "\n",
        "        # Gradient step is in a separate optimizer\n",
        "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test shapes:\n",
            "torch.Size([10, 2, 3, 4])\n",
            "(10, 2, 3, 4)\n"
          ]
        }
      ],
      "source": [
        "# Conv2D test\n",
        "from torch import nn\n",
        "import torch\n",
        "test_batch_size = 10\n",
        "test_input_chanels = 3\n",
        "test_output_chanels = 2\n",
        "test_input_height = 4\n",
        "test_input_width = 5\n",
        "test_kernel_size = 2\n",
        "\n",
        "test_conv = Conv2d(input_channels=test_input_chanels, \n",
        "                   output_channels=test_output_chanels, \n",
        "                   kernel_size=test_kernel_size)\n",
        "test_input = np.random.randn(test_batch_size, \n",
        "                             test_input_chanels, \n",
        "                             test_input_height,\n",
        "                             test_input_width)\n",
        "test_output = test_conv.forward(test_input)\n",
        "test_layer_torch = nn.Conv2d(in_channels=test_input_chanels,\n",
        "                     out_channels=test_output_chanels,\n",
        "                     kernel_size=(test_kernel_size, test_kernel_size))\n",
        "test_ref = test_layer_torch(torch.tensor(test_input, dtype=torch.float))\n",
        "print(\"Test shapes:\")\n",
        "print(test_ref.shape)\n",
        "print(test_output.shape)\n",
        "#assert torch.equal(torch.tensor(test_output, dtype=torch.float), test_ref)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shapes (10, 3, 4, 5), (10, 2, 3, 4)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[[[-9.51866913e-04, -7.50838179e-04, -1.08698607e-03,\n",
              "          -4.29527551e-04, -1.69690940e-04],\n",
              "         [ 8.12572149e-04,  1.89395250e-04, -5.97774487e-04,\n",
              "          -1.63375913e-03, -5.05891268e-04],\n",
              "         [ 3.12043490e-04, -3.52872669e-04, -4.84598210e-04,\n",
              "           8.52907876e-04,  6.44280115e-04],\n",
              "         [-3.03319331e-04, -1.98457861e-04, -4.44630827e-04,\n",
              "          -4.88709446e-04, -8.38695337e-05]],\n",
              "\n",
              "        [[ 3.33523910e-04, -4.54835279e-04,  2.81424518e-04,\n",
              "          -1.77954190e-04,  2.68280151e-05],\n",
              "         [ 6.14554804e-04,  1.18382402e-04,  1.01605212e-03,\n",
              "           5.73653102e-04, -2.70644959e-04],\n",
              "         [-4.74880466e-04, -1.48189113e-04, -8.30828126e-05,\n",
              "           6.22928985e-04, -3.30373493e-04],\n",
              "         [-5.46758854e-05,  5.98579951e-04,  3.27410142e-04,\n",
              "           1.42447359e-04,  7.25808815e-05]],\n",
              "\n",
              "        [[ 1.36393793e-04,  8.69183435e-05,  3.50690931e-04,\n",
              "           2.16972803e-04,  1.19644437e-04],\n",
              "         [-3.21596617e-04, -6.74621171e-04, -3.85647789e-04,\n",
              "           6.40037448e-04,  7.38833712e-04],\n",
              "         [ 3.07781522e-04,  1.21797404e-05, -3.63457699e-04,\n",
              "          -2.71764688e-04, -5.80269474e-05],\n",
              "         [-1.23329056e-04, -4.85461113e-04, -5.05373135e-04,\n",
              "           3.72864507e-04, -9.62389070e-05]]],\n",
              "\n",
              "\n",
              "       [[[ 8.73779505e-05, -1.03780212e-03, -2.34402826e-04,\n",
              "          -3.76419630e-04, -1.13706463e-04],\n",
              "         [-5.89611839e-05, -9.88636377e-04, -6.44738377e-04,\n",
              "           6.34034341e-04,  1.48146167e-04],\n",
              "         [-7.67784456e-04, -9.81766298e-05, -1.98874833e-04,\n",
              "          -2.65519617e-03, -9.16541875e-04],\n",
              "         [-1.29699189e-04, -2.40005231e-04, -6.55173555e-04,\n",
              "           2.10323026e-04,  3.34959637e-04]],\n",
              "\n",
              "        [[-1.75297849e-04,  3.60075534e-04, -1.85705193e-04,\n",
              "           3.38036321e-04, -2.18150883e-04],\n",
              "         [-2.10929729e-04,  9.75230740e-04, -5.94555336e-04,\n",
              "          -7.79618616e-05,  3.73396104e-05],\n",
              "         [-1.19764539e-04,  5.48577694e-04,  8.60902461e-05,\n",
              "           1.09832561e-03,  2.52291331e-04],\n",
              "         [ 3.29417793e-04, -1.55447085e-04, -2.13602282e-04,\n",
              "           1.28860030e-03, -3.59935954e-04]],\n",
              "\n",
              "        [[-2.38849993e-04,  2.52987704e-05,  6.73565906e-04,\n",
              "           1.06566319e-04,  2.92703685e-04],\n",
              "         [-2.62342863e-04, -1.09216980e-03,  5.19586174e-04,\n",
              "          -6.69256253e-04, -9.82944343e-05],\n",
              "         [-1.38705991e-04, -6.01785525e-04, -1.20671912e-03,\n",
              "           2.20267813e-04,  5.66966416e-04],\n",
              "         [-5.49085960e-04,  5.46939412e-04, -2.40248879e-04,\n",
              "          -7.36503866e-04,  2.37767430e-04]]],\n",
              "\n",
              "\n",
              "       [[[-4.77382744e-04, -1.71935306e-04,  2.99921970e-04,\n",
              "          -7.29944707e-04, -4.22795705e-04],\n",
              "         [ 6.67066655e-04,  1.59786857e-03,  1.83547835e-04,\n",
              "          -9.36909458e-04, -4.15083277e-04],\n",
              "         [ 5.48455674e-04, -3.47008249e-04, -6.62442923e-05,\n",
              "           1.82361699e-04, -1.07753944e-04],\n",
              "         [-4.38080845e-04, -6.22631084e-04, -4.47284264e-04,\n",
              "          -2.65850188e-04, -1.18127362e-04]],\n",
              "\n",
              "        [[ 1.73732434e-04,  5.53725872e-05, -2.81424686e-04,\n",
              "           1.75177582e-04, -6.49111113e-05],\n",
              "         [ 3.43297013e-04, -5.56233047e-04, -1.88720274e-04,\n",
              "           9.04813345e-04,  1.89139544e-05],\n",
              "         [-5.85473779e-04, -7.90249929e-04,  2.59378596e-04,\n",
              "           6.20180988e-04, -3.09667355e-06],\n",
              "         [-1.83289460e-04,  2.87821685e-04, -4.89680546e-05,\n",
              "           1.75026466e-04,  1.72953625e-04]],\n",
              "\n",
              "        [[ 7.85142988e-05,  5.70635058e-04, -3.31006327e-04,\n",
              "          -1.38366830e-04,  4.16690691e-04],\n",
              "         [ 1.18935242e-06, -4.77455167e-04, -5.78927691e-04,\n",
              "          -5.94485126e-04,  5.09599769e-04],\n",
              "         [ 2.37179687e-04, -2.36778358e-04, -2.43071587e-04,\n",
              "          -9.90809715e-04,  2.36825601e-04],\n",
              "         [-3.13526225e-05, -1.93863282e-04,  3.14098817e-04,\n",
              "          -3.15552932e-04,  1.24350433e-05]]],\n",
              "\n",
              "\n",
              "       [[[-4.16762778e-04, -2.12716325e-04,  3.79812966e-04,\n",
              "           1.49328363e-04, -1.60554915e-04],\n",
              "         [-4.88986244e-04, -3.36144057e-04,  5.21067312e-04,\n",
              "          -9.62839184e-04, -8.23970593e-04],\n",
              "         [ 7.55119996e-04,  1.47673041e-03,  7.98677029e-04,\n",
              "          -1.58828574e-03, -5.70627050e-04],\n",
              "         [ 2.32034008e-04,  6.39746903e-04,  1.55763762e-04,\n",
              "           6.22248822e-04,  4.14341807e-04]],\n",
              "\n",
              "        [[ 1.51702101e-04, -9.84211801e-05, -1.26937346e-04,\n",
              "          -2.70005551e-05,  6.57075061e-05],\n",
              "         [ 7.36579169e-04,  1.62889777e-04, -8.07668509e-04,\n",
              "           1.61276400e-04,  2.12069054e-04],\n",
              "         [ 5.05721323e-04, -8.86730827e-05, -1.22714492e-03,\n",
              "           1.37648197e-03,  3.34858581e-05],\n",
              "         [-3.88273843e-05, -2.61848625e-04, -6.27214689e-04,\n",
              "           8.13184329e-04, -4.77102688e-04]],\n",
              "\n",
              "        [[ 6.85925841e-05,  2.43244296e-04, -2.30151638e-06,\n",
              "          -4.43701924e-04,  7.69084111e-05],\n",
              "         [ 2.50750728e-04,  9.72848698e-04,  1.60992882e-04,\n",
              "          -8.29137361e-04,  5.67528971e-04],\n",
              "         [ 1.19260168e-04,  6.40632202e-04, -1.38889027e-05,\n",
              "          -6.04830801e-04,  7.23398327e-04],\n",
              "         [ 2.07816010e-04,  3.54084713e-04,  1.32602010e-04,\n",
              "          -5.55916862e-04,  2.27442014e-04]]],\n",
              "\n",
              "\n",
              "       [[[-2.37430617e-04, -9.64849824e-04,  3.45518191e-04,\n",
              "           6.27416213e-04,  1.72660225e-04],\n",
              "         [ 2.40427194e-04, -3.93611175e-04, -1.94496214e-04,\n",
              "          -5.59154985e-04, -9.69266291e-06],\n",
              "         [-1.95182216e-03, -1.09316074e-03, -9.87441183e-04,\n",
              "           2.59811572e-05,  1.12172334e-04],\n",
              "         [ 6.06669372e-05, -5.66781268e-04, -5.86957511e-04,\n",
              "          -6.83781187e-04, -3.13602922e-04]],\n",
              "\n",
              "        [[ 3.65975935e-06,  4.76153882e-05, -4.13175136e-04,\n",
              "           2.07318532e-04,  3.11920780e-05],\n",
              "         [-2.67435621e-04,  7.77311606e-04, -7.99461701e-04,\n",
              "           3.95948136e-04, -9.68734970e-05],\n",
              "         [ 6.07568263e-05,  4.51878917e-05, -1.04152368e-04,\n",
              "           5.19148066e-04,  2.20587442e-04],\n",
              "         [ 7.79646705e-04,  5.63827130e-05,  3.89504495e-04,\n",
              "          -1.53221448e-04,  3.82227529e-04]],\n",
              "\n",
              "        [[-9.03947189e-05, -5.02894492e-05,  3.68289869e-04,\n",
              "          -4.79450714e-04, -1.74382880e-04],\n",
              "         [-7.54142996e-04, -8.29510957e-04, -3.63673850e-06,\n",
              "           3.47771346e-05,  2.36617945e-05],\n",
              "         [-2.98038904e-04,  2.87538210e-04, -4.76495023e-04,\n",
              "          -5.38921731e-04, -2.75519095e-04],\n",
              "         [-1.05683280e-03,  5.50029328e-05, -1.37544896e-04,\n",
              "           1.50047949e-04, -1.27947580e-04]]],\n",
              "\n",
              "\n",
              "       [[[ 7.67910122e-04, -2.07001472e-04, -7.60754625e-04,\n",
              "           5.56722008e-04,  3.30224794e-04],\n",
              "         [ 3.11411159e-04,  1.55544791e-03,  1.16289077e-03,\n",
              "           6.85704574e-05,  1.87545493e-04],\n",
              "         [ 1.02261269e-03,  5.93930381e-04, -1.97091071e-04,\n",
              "           5.86434242e-04,  2.81640419e-04],\n",
              "         [ 3.50509904e-04,  9.15621726e-04,  1.16825563e-03,\n",
              "           5.06048510e-04,  1.93516935e-06]],\n",
              "\n",
              "        [[-1.19962843e-04,  6.93615971e-04, -5.29472320e-05,\n",
              "          -5.82186360e-04,  2.46794410e-04],\n",
              "         [-4.63522173e-04,  1.98695752e-04, -2.47914642e-05,\n",
              "          -4.86091160e-04,  5.12570285e-05],\n",
              "         [-1.37669109e-04,  2.56861454e-05,  2.33021270e-04,\n",
              "          -5.52945748e-04, -5.33594733e-04],\n",
              "         [-5.58491070e-04, -3.55942977e-04, -1.75533477e-04,\n",
              "          -5.40205384e-04,  8.85821565e-06]],\n",
              "\n",
              "        [[ 1.23214140e-04,  1.29049621e-04,  4.60709114e-04,\n",
              "           2.73072322e-04, -5.01956882e-04],\n",
              "         [ 7.59574212e-04, -3.62914978e-04, -4.01314716e-04,\n",
              "           1.02763381e-03, -3.55153502e-04],\n",
              "         [ 7.44660943e-04,  5.07703023e-05,  7.16477239e-04,\n",
              "           6.53024969e-04,  9.67067742e-05],\n",
              "         [ 1.01714935e-03,  5.15543019e-05,  7.94794781e-05,\n",
              "           2.82612098e-05,  2.42592426e-05]]],\n",
              "\n",
              "\n",
              "       [[[ 2.37710724e-04,  6.00423921e-04,  8.98386846e-04,\n",
              "          -3.45354162e-04, -2.00817975e-04],\n",
              "         [-1.82803555e-04,  1.28609781e-03,  1.01621270e-03,\n",
              "           2.96157850e-04,  2.68064128e-04],\n",
              "         [-1.23862757e-03, -4.87337830e-04, -4.15892835e-04,\n",
              "          -1.20405588e-04, -1.81447915e-04],\n",
              "         [ 3.05432734e-04, -3.43813162e-04, -6.21989294e-04,\n",
              "          -8.19679880e-04, -4.03765651e-04]],\n",
              "\n",
              "        [[ 4.87429623e-05,  7.98130297e-05, -2.44556339e-04,\n",
              "           3.60797836e-04, -1.52858786e-04],\n",
              "         [ 1.91034909e-05, -7.70740767e-04, -6.17411585e-04,\n",
              "           7.22581819e-04, -2.25365990e-04],\n",
              "         [ 6.09035597e-04, -1.30009470e-03, -2.78018824e-04,\n",
              "          -1.20045018e-04, -7.72888753e-05],\n",
              "         [ 8.49144357e-04, -2.21406905e-04,  4.37623525e-04,\n",
              "           3.58437870e-04,  5.59604065e-04]],\n",
              "\n",
              "        [[ 1.72483205e-04,  1.07813399e-04, -4.05204693e-04,\n",
              "          -1.62881866e-04,  3.07751993e-04],\n",
              "         [ 4.09966554e-04,  6.23614658e-04, -4.52112233e-04,\n",
              "          -4.39521349e-04,  6.71645571e-05],\n",
              "         [-1.09763539e-04,  1.30512528e-03, -2.93779437e-04,\n",
              "          -3.03915599e-04,  1.71972999e-04],\n",
              "         [-9.93013758e-04,  2.19480624e-04, -3.88907347e-04,\n",
              "          -4.08079312e-04, -2.35327286e-05]]],\n",
              "\n",
              "\n",
              "       [[[ 4.82172706e-04, -7.41964874e-04,  8.45335131e-04,\n",
              "           6.08905160e-04,  2.06228553e-04],\n",
              "         [-1.25377632e-03,  1.59832460e-04, -3.30239210e-04,\n",
              "           1.42793992e-04,  2.71634722e-04],\n",
              "         [-8.27767071e-06,  1.72346505e-04, -2.19401282e-04,\n",
              "          -3.47483455e-04,  6.06843076e-05],\n",
              "         [ 5.97163937e-04,  4.19253554e-04,  9.53754314e-05,\n",
              "           3.91177818e-04,  2.34206824e-04]],\n",
              "\n",
              "        [[-4.28096240e-04,  8.35616650e-04, -5.74637763e-04,\n",
              "           2.02993938e-04, -1.73370187e-05],\n",
              "         [-2.90195730e-04,  6.00898195e-04, -6.23797209e-04,\n",
              "          -3.16961557e-04, -1.62786541e-04],\n",
              "         [ 6.76503668e-04, -9.95833729e-05,  9.02034388e-04,\n",
              "          -2.05774743e-04, -2.77444442e-04],\n",
              "         [-9.31608638e-05, -7.89290564e-04,  2.59365953e-04,\n",
              "           2.42450221e-04, -2.93415417e-04]],\n",
              "\n",
              "        [[-4.74483806e-04,  4.18569784e-04,  2.62833445e-04,\n",
              "          -3.44618652e-04, -1.59148041e-04],\n",
              "         [-2.46168507e-04, -5.21691109e-05,  9.00467956e-04,\n",
              "           5.10759834e-04, -1.65591946e-04],\n",
              "         [-9.37447105e-05, -1.76292230e-04, -5.23489044e-04,\n",
              "           9.29248758e-04,  1.32348861e-04],\n",
              "         [ 5.25317990e-04,  2.68754638e-04, -4.42818845e-04,\n",
              "           3.15263217e-05,  7.89036710e-05]]],\n",
              "\n",
              "\n",
              "       [[[-9.01355046e-04, -8.78510240e-04, -8.91369615e-05,\n",
              "           1.88957135e-04,  1.78165143e-04],\n",
              "         [ 4.03721361e-04,  6.97849296e-05, -3.89199698e-04,\n",
              "          -1.04900966e-03, -3.70318402e-04],\n",
              "         [-2.59371957e-04,  4.66756707e-04,  7.65738600e-04,\n",
              "          -4.68475772e-04, -2.86155058e-04],\n",
              "         [ 1.88804962e-04,  8.73215772e-04,  6.77177866e-05,\n",
              "          -5.48614817e-04, -1.75755429e-04]],\n",
              "\n",
              "        [[ 3.17182722e-04,  1.94667686e-05, -2.77840542e-04,\n",
              "           2.76962878e-05,  3.56804666e-05],\n",
              "         [ 5.91476108e-04,  5.55184154e-04, -5.12001558e-04,\n",
              "           7.64199920e-05, -1.20234588e-04],\n",
              "         [ 1.09735727e-04,  5.00798372e-04, -7.81217395e-04,\n",
              "           6.27053782e-04,  2.19500861e-04],\n",
              "         [ 2.29592811e-04, -4.95470270e-05, -8.17475883e-04,\n",
              "           4.63002239e-04,  2.50109788e-04]],\n",
              "\n",
              "        [[ 1.31279678e-04,  7.18853258e-04,  1.01937419e-04,\n",
              "           5.69703199e-05, -1.83087482e-04],\n",
              "         [-4.37147441e-04,  2.92456581e-04,  1.60663283e-05,\n",
              "           3.85914994e-04,  3.47745311e-04],\n",
              "         [ 2.71826737e-04,  8.20948597e-04, -8.08038787e-04,\n",
              "          -6.94889051e-04,  2.08707160e-04],\n",
              "         [-1.98364480e-04,  4.96812733e-04, -1.06979025e-04,\n",
              "          -2.90517499e-04,  3.39725591e-06]]],\n",
              "\n",
              "\n",
              "       [[[-6.19160527e-04,  5.63131577e-04,  1.04589683e-03,\n",
              "           1.86239454e-04, -4.05872146e-04],\n",
              "         [-1.45926511e-04, -9.70666567e-04,  6.30268679e-04,\n",
              "          -7.28970928e-04, -7.04443461e-04],\n",
              "         [-6.89564472e-04,  5.20621470e-04,  1.48213271e-04,\n",
              "          -1.17657061e-03, -2.41615562e-04],\n",
              "         [ 5.95584977e-04,  8.65924067e-04, -3.98120106e-05,\n",
              "           6.40643675e-05,  1.92879884e-04]],\n",
              "\n",
              "        [[ 1.84046425e-04, -4.10832962e-04,  1.13405288e-04,\n",
              "          -1.21447946e-04,  1.07233057e-04],\n",
              "         [ 1.75173049e-04, -1.62795873e-04, -9.97346398e-04,\n",
              "           2.33667779e-04,  4.81877373e-04],\n",
              "         [ 5.42742538e-04,  6.68169250e-04, -1.09411088e-03,\n",
              "           1.15934832e-03,  1.59918017e-04],\n",
              "         [ 1.58504178e-04, -6.60305765e-04, -6.00201717e-04,\n",
              "           5.49926779e-04, -2.36908634e-04]],\n",
              "\n",
              "        [[ 3.72524243e-05,  2.98585472e-04, -4.01377302e-04,\n",
              "          -8.37491216e-04,  2.47407709e-04],\n",
              "         [-8.46394956e-04,  1.26752403e-03,  2.12426275e-05,\n",
              "          -1.15475349e-03,  3.19424677e-04],\n",
              "         [-4.27398548e-05,  4.90891018e-04, -3.89215022e-04,\n",
              "          -4.35006400e-04,  2.16632773e-04],\n",
              "         [ 1.70207881e-04,  5.83359607e-04, -1.21215999e-04,\n",
              "          -1.40940203e-04,  7.48821998e-05]]]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"Input shapes {np.shape(test_input)}, {np.shape(test_output)}\")\n",
        "test_conv.backward(test_input, test_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "p1iE85PpMm0Z"
      },
      "outputs": [],
      "source": [
        "class MaxPool2d(Layer):\n",
        "    def __init__(self, kernel_size):\n",
        "        self.kernel_size = kernel_size\n",
        "        self.max_elem_mask = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Perform an pooling:\n",
        "\n",
        "        output_height = input_height // kernel_size\n",
        "        output_width = input_width // kernel_size\n",
        "\n",
        "        input shape: [batch, input_channels, input_height, input_width]\n",
        "        output shape: [batch, input_channels, output_height, output_width]\n",
        "        \"\"\"\n",
        "        type = input.dtype\n",
        "        kernel_size = self.kernel_size\n",
        "        batch = np.shape(input)[0]\n",
        "        input_chanels = np.shape(input)[1]\n",
        "        output_height = np.shape(input)[2]\n",
        "        output_width = np.shape(input)[3]\n",
        "        output = np.empty((output_height, output_width, batch, input_chanels))\n",
        "        self.max_elem_mask = np.empty((batch, input_chanels, np.shape(input)[2], np.shape(input)[3]))\n",
        "        for i in range(output_height):\n",
        "            for j in range(output_width):\n",
        "                input_slice = input[:, :, i:i+kernel_size, j:j+kernel_size]\n",
        "                max_tensor = np.max(input_slice, axis=(2, 3))\n",
        "# How should I handlel such indexes convertions???????\n",
        "                output[i, j] = max_tensor\n",
        "                self.max_elem_mask[:, :, i:i+kernel_size, j:j+kernel_size] = \\\n",
        "                    np.equal(input_slice, np.reshape(max_tensor, \n",
        "                                                     (np.shape(max_tensor)[0],\n",
        "                                                      np.shape(max_tensor)[1],\n",
        "                                                     1, 1)))\n",
        "        return np.moveaxis(output, [0, 1], [2, 3])\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        type = input.dtype\n",
        "        kernel_size = self.kernel_size\n",
        "        grad_input = self.max_elem_mask\n",
        "        for i in range(np.shape(grad_output)[2]):\n",
        "            for j in range(np.shape(grad_output)[3]):\n",
        "                grad_input[:, :, i:i+kernel_size, j:j+kernel_size] = \\\n",
        "                    np.multiply(grad_output[:, :, i:i+1, j:j+1], \n",
        "                                self.max_elem_mask[:, :, i:i+kernel_size, j:j+kernel_size],\n",
        "                                dtype=type)\n",
        "        self.max_elem_mask = None\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10, 3, 4, 5)\n",
            "input_shape (10, 3, 4, 5) grad shape: (10, 3, 4, 5)\n"
          ]
        }
      ],
      "source": [
        "test_maxpool = MaxPool2d(2)\n",
        "test_output = test_maxpool.forward(test_input)\n",
        "print(np.shape(test_maxpool.max_elem_mask))\n",
        "test_backward = test_maxpool.backward(test_input, test_output)\n",
        "print(f\"input_shape {np.shape(test_input)} grad shape: {np.shape(test_backward)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "3W-GM45xSqJD"
      },
      "outputs": [],
      "source": [
        "class Flatten(Layer):\n",
        "    def forward(self, input):\n",
        "          \"\"\"\n",
        "          Perform an flatten operation:\n",
        "\n",
        "          input shape: [batch, input_channels, input_height, input_width]\n",
        "          output shape: [batch, input_channels * output_height * output_width]\n",
        "          \"\"\"\n",
        "          flattened_size = np.shape(input)[1] * np.shape(input)[2] * np.shape(input)[3]\n",
        "          return np.reshape(input, (np.shape(input)[0], flattened_size))\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        return np.reshape(grad_output, np.shape(input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "iCuNsUikOoE1",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def softmax_crossentropy_with_logits(logits,reference_answers):\n",
        "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
        "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
        "\n",
        "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
        "\n",
        "    return xentropy\n",
        "\n",
        "def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n",
        "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
        "    ones_for_answers = np.zeros_like(logits)\n",
        "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
        "\n",
        "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
        "\n",
        "    return (- ones_for_answers + softmax) / logits.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muokQIA3UcnL"
      },
      "source": [
        "## Имплементация оптимизатора и изменения learning rate'a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBzMVan_XzFM"
      },
      "source": [
        "В имплементации этих двух классов есть небольшие неточности.\n",
        "Посмотрите, как сделана имплементация метода моментов в Pytorch и добавьте пропущенное.\n",
        "\n",
        "> Добавлять моменты Нестерова не нужно!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "n4hyzuIcUqLd"
      },
      "outputs": [],
      "source": [
        "class SGDOptimizer:\n",
        "    def __init__(self, momentum=0.9, dampening=0.0, weight_decay=0.0):\n",
        "        \"\"\"\n",
        "        Wrapper which perfoms weights update\n",
        "        \"\"\"\n",
        "        self.momentum = momentum\n",
        "        self.dampening = dampening\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        self.momentum_buffer = None\n",
        "\n",
        "    def step(self, weights, grad_weights, lr=0.1):\n",
        "        \"\"\"\n",
        "        Update weights\n",
        "        \"\"\"\n",
        "        # For no there is no input parameter\n",
        "        #if self.weight_decay != 0:\n",
        "        #  grad_weights = grad_weights + self.weight_decay * input\n",
        "\n",
        "        if self.momentum != 0:\n",
        "            if self.momentum_buffer is None:\n",
        "                self.momentum_buffer = grad_weights\n",
        "            else:\n",
        "                self.momentum_buffer = self.momentum * self.momentum_buffer + \\\n",
        "                                    (1 - self.dampening) * grad_weights\n",
        "            grad_weights = self.momentum_buffer\n",
        "      \n",
        "        return weights - lr * self.momentum_buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "4DX-leurYblp"
      },
      "outputs": [],
      "source": [
        "class LRScheduler:\n",
        "    def __init__(self, lr):\n",
        "        \"\"\"\n",
        "        Wrapper which perfoms learning rate updates\n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "        self.current_step = 0\n",
        "\n",
        "    def get_lr(self):\n",
        "      \"\"\"\n",
        "      Update learing rate for current iteration\n",
        "      \"\"\"\n",
        "      current_lr = self.lr / (self.current_step + 1)\n",
        "      #<your code here>\n",
        "      self.current_step += 1\n",
        "      return current_lr\n",
        "\n",
        "    def reset(self):\n",
        "       self.current_step = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlEURTE9OoE5",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Итоговая нейросеть\n",
        "\n",
        "Все готово для запуска нейросети. Нейросеть будем тестировать на классическом датасете MNIST. Код ниже визуализирует несколько примеров из этого датасета."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "2KKQ81e0OoE6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf8AAAIOCAYAAAC21wSfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA90UlEQVR4nO3de3RU9b3//9cAyRS5pEYgk3AJKYK0oKAoKCIXlRzjwWpRS7GtUF0U5aJ88VIRLUGRgFqOVcArjVi1cHoEpZWqsUKwC+mBFI4WxAPHoKEkRKhMQoDEwOf3hz+mDsnemQwzmcvn+Vjrs5bZ7/3Z+51t3ryzM/viMcYYAQAAa7SKdQIAAKBl0fwBALAMzR8AAMvQ/AEAsAzNHwAAy9D8AQCwDM0fAADL0PwBALAMzR8AAMvQ/BPUiy++KI/Hoy1btkRkex6PR9OmTYvItr65zfz8/LDm7tmzRx6Pp9GxYsWKiOYJJIpkr3tJ+uqrrzR37lz17NlTXq9Xffv21VNPPRW5BCFJahPrBAA306dP10033RS0rHfv3jHKBkC0TZkyRb/97W/18MMP66KLLtLbb7+tO++8U9XV1br//vtjnV7SoPkjrvXo0UMXX3xxrNMA0AK2b9+uZcuW6ZFHHtE999wjSRo5cqQOHjyoefPm6bbbblN6enqMs0wO/Nk/iR07dkx33XWXBg4cqLS0NKWnp+uSSy7RG2+84Tjn2WefVZ8+feT1evW9732v0T+xV1RUaPLkyerWrZtSU1OVk5OjuXPnqr6+PprfDoAQJHLdv/766zLG6Gc/+1nQ8p/97Gc6evSo3nrrrYjty3ac+Sex2tpa/fOf/9Tdd9+trl27qq6uTu+++67Gjh2rwsJC3XzzzUHrr1mzRuvWrdNDDz2kdu3aaenSpRo/frzatGmjG264QdLX/wAMHjxYrVq10i9/+Uv16tVLH3zwgebNm6c9e/aosLDQNaeePXtK+voz/VAsWLBA999/v9q0aaMLLrhA9957r77//e83+1gAtkjkuv/73/+uzp07y+fzBS0/77zzAnFEiEFCKiwsNJLM5s2bQ55TX19vvvrqK3Prrbea888/PygmybRt29ZUVFQErd+3b19z9tlnB5ZNnjzZtG/f3nz22WdB8x9//HEjyWzfvj1om3PmzAlar1evXqZXr15N5rpv3z4zadIk85//+Z/m/fffN6+88oq5+OKLjSTz/PPPh/w9A8kk2et+9OjR5pxzzmk0lpqaan7+8583uQ2Ehj/7J7nf//73uvTSS9W+fXu1adNGKSkpWrZsmT7++OMG615xxRXKyMgIfN26dWuNGzdOu3fv1t69eyVJf/zjHzVq1ChlZWWpvr4+MPLy8iRJxcXFrvns3r1bu3fvbjLvzMxMPffcc7rxxhs1bNgw3XTTTdqwYYPOP/983XfffXzEALhI1LqXvr5bIJwYmofmn8RWrVqlH/7wh+ratatefvllffDBB9q8ebNuueUWHTt2rMH6p/6p7ZvLDh48KEnav3+//vCHPyglJSVo9OvXT5J04MCBqH0/KSkpGjdunA4ePKhdu3ZFbT9AIkvkuj/rrLMC+/ymmpoa1dXVcbFfBPGZfxJ7+eWXlZOTo5UrVwb9xlxbW9vo+hUVFY7LzjrrLElSp06ddN555+mRRx5pdBtZWVmnm7YrY4wkqVUrfm8FGpPIdX/uuedqxYoVqqioCPql5KOPPpIk9e/fPyL7Ac0/qXk8HqWmpgb9A1BRUeF41e+f//xn7d+/P/AnwOPHj2vlypXq1auXunXrJkkaM2aM1q5dq169eunMM8+M/jfxDV999ZVWrlypTp066eyzz27RfQOJIpHr/tprr9UDDzyg5cuX6xe/+EVg+Ysvvqi2bdvqqquuitq+bUPzT3Dvvfdeo1fQXn311RozZoxWrVqlKVOm6IYbblBZWZkefvhhZWZmNvpn806dOunyyy/Xgw8+GLjqd+fOnUG3/Tz00EMqKirS0KFDdccdd+icc87RsWPHtGfPHq1du1bPPPNM4B+Mxpxs2k19/jdz5kx99dVXuvTSS+Xz+VRWVqannnpK27ZtU2FhoVq3bh3iEQKST7LWfb9+/XTrrbdqzpw5at26tS666CK98847eu655zRv3jz+7B9Jsb7iEOE5edWv0ygtLTXGGLNgwQLTs2dP4/V6zXe/+13z/PPPmzlz5phT/9dLMlOnTjVLly41vXr1MikpKaZv377mlVdeabDvL774wtxxxx0mJyfHpKSkmPT0dDNo0CAze/Zsc/jw4aBtnnrVb3Z2tsnOzm7y+1u2bJkZPHiwSU9PN23atDFnnnmm+bd/+zfz9ttvN/tYAcki2eveGGPq6urMnDlzTI8ePUxqaqrp06ePefLJJ5t1nNA0jzH//4eoAADAClw1BQCAZWj+AABYhuYPAIBlaP4AAFiG5g8AgGVo/gAAWCbuHvJz4sQJ7du3Tx06dOAlDkCYjDGqrq5WVlZWQjwKmboHTl+z6j5aDxBYsmRJ4CETF1xwgdmwYUNI88rKylwfYsFgMEIfZWVl0SrxBsKteWOoewYjkiOUuo9K81+xYoVJSUkxzz//vNmxY4e58847Tbt27Rq8C7oxhw4divmBYzCSZRw6dCgaJd7A6dS8MdQ9gxHJEUrdR6X5Dx482Nx2221By/r27Wvuu+++Juf6/f6YHzgGI1mG3++PRok3cDo1bwx1z2BEcoRS9xH/MLCurk4lJSXKzc0NWp6bm6uNGzc2WL+2tlZVVVVBA0DiaG7NS9Q9EGsRb/4HDhzQ8ePHA6+HPCkjI6PR90YXFBQoLS0tMLp37x7plABEUXNrXqLugViL2mXAp16xa4xp9CreWbNmye/3B0ZZWVm0UgIQRaHWvETdA7EW8Vv9OnXqpNatWzf4jb+ysrLBmYEkeb1eeb3eSKcBoIU0t+Yl6h6ItYif+aempmrQoEEqKioKWl5UVKShQ4dGencAYoyaBxJQGBf2NunkbT/Lli0zO3bsMDNmzDDt2rUze/bsaXIuV/0yGJEbLXW1/+nUvDHUPYMRyRFK3UflCX/jxo3TwYMH9dBDD6m8vFz9+/fX2rVrlZ2dHY3dAYgxah5ILB5jjIl1Et9UVVWltLS0WKcBJAW/36+OHTvGOo0mUfdA5IRS9/H/0G8AABBRNH8AACxD8wcAwDI0fwAALEPzBwDAMjR/AAAsQ/MHAMAyNH8AACxD8wcAwDI0fwAALEPzBwDAMjR/AAAsQ/MHAMAyNH8AACxD8wcAwDI0fwAALEPzBwDAMjR/AAAsQ/MHAMAybWKdAAAg8Q0aNMg1Pm3aNMfYzTff7Dr3pZdecow99dRTrnP/9re/ucZtxZk/AACWofkDAGAZmj8AAJah+QMAYBmaPwAAlqH5AwBgGY8xxkRyg/n5+Zo7d27QsoyMDFVUVIQ0v6qqSmlpaZFMCQ5at27tGIvm/wO3W37OOOMM17nnnHOOY2zq1Kmucx9//HHH2Pjx413nHjt2zDG2YMEC17mn1kNL8vv96tixY9T3Q93bYeDAgY6x9957z3VutH4O/X6/a/yss86Kyn7jWSh1H5X7/Pv166d333038LVbkwGQHKh7IHFEpfm3adNGPp8vGpsGEKeoeyBxROUz/127dikrK0s5OTn60Y9+pE8//TQauwEQR6h7IHFE/Mx/yJAheumll9SnTx/t379f8+bN09ChQ7V9+/ZGP3upra1VbW1t4OuqqqpIpwQgyqh7ILFE/Mw/Ly9P119/vc4991xdeeWVevPNNyVJy5cvb3T9goICpaWlBUb37t0jnRKAKKPugcQS9Vv92rVrp3PPPVe7du1qND5r1iz5/f7AKCsri3ZKAKKMugfiW9Tf6ldbW6uPP/5Yl112WaNxr9crr9cb7TQAtCDqHohvEW/+d999t6655hr16NFDlZWVmjdvnqqqqjRhwoRI7ypp9OjRwzGWmprqOnfo0KGOsWHDhrnO/fa3v+0Yu/76613nxsrevXsdY08++aTr3B/84AeOserqate5//M//+MYKy4udp1rA+o+OQwePNg1/tprrznGmnpOg9sjZZqqv7q6OsdYU/fxX3zxxY6xpl7367bfRBfx5r93716NHz9eBw4cUOfOnXXxxRdr06ZNys7OjvSuAMQJ6h5ILBFv/itWrIj0JgHEOeoeSCw82x8AAMvQ/AEAsAzNHwAAy9D8AQCwTMRf6Xu6kvHVnm6vwZTcX4WZbMeiKSdOnHCN33LLLY6xw4cPh73f8vJy1/iXX37pGPvkk0/C3m+0tdQrfU9XMtZ9rDT1WuwLLrjAMfbyyy+7zu3WrZtjzOPxuM51azVN3XL36KOPOsaautjULa8HHnjAdW5BQYFrPF6FUvec+QMAYBmaPwAAlqH5AwBgGZo/AACWofkDAGAZmj8AAJah+QMAYJmIv9gHDX3++eeu8YMHDzrG4vXe57/+9a+OsUOHDrnOHTVqlGOsqVdo/va3v3WNA7Z79tlnXePjx49voUxC5/bsAUlq3769Y6ypV2qPHDnSMXbeeee5zk1mnPkDAGAZmj8AAJah+QMAYBmaPwAAlqH5AwBgGZo/AACW4Va/FvDPf/7TNX7PPfc4xsaMGeM6d+vWrY6xJ5980j0xF9u2bXONjx492jFWU1PjOrdfv36OsTvvvNN1LgBp0KBBjrF///d/d53b1Kt33bjdVveHP/zBde7jjz/uGNu3b5/rXLd/59xety1Jl19+uWPsdI5FouPMHwAAy9D8AQCwDM0fAADL0PwBALAMzR8AAMvQ/AEAsAzNHwAAy3iMMaY5EzZs2KDHHntMJSUlKi8v1+rVq3XdddcF4sYYzZ07V88995y+/PJLDRkyREuWLHG9t/ubqqqq4vY1trHQsWNH13h1dbVjrKlXe956662OsZ/85Ceuc3/3u9+5xhEf/H5/kz9DTYl2zUvU/akGDhzoGn/vvfccY6fz//tPf/qTa9ztdcAjRoxwnev2+twXXnjBde4XX3zhGndz/Phxx9iRI0dc57p9T3/729/CzinaQqn7Zp/519TUaMCAAVq8eHGj8UcffVSLFi3S4sWLtXnzZvl8Po0ePdq1SQGIX9Q8kHya/YS/vLw85eXlNRozxuiJJ57Q7NmzNXbsWEnS8uXLlZGRoVdffVWTJ08+vWwBtDhqHkg+Ef3Mv7S0VBUVFcrNzQ0s83q9GjFihDZu3NjonNraWlVVVQUNAIkhnJqXqHsg1iLa/CsqKiRJGRkZQcszMjICsVMVFBQoLS0tMLp37x7JlABEUTg1L1H3QKxF5Wr/U1+WYIxxfIHCrFmz5Pf7A6OsrCwaKQGIoubUvETdA7EW0bf6+Xw+SV+fDWRmZgaWV1ZWNjgzOMnr9crr9UYyDQAtJJyal6h7INYi2vxzcnLk8/lUVFSk888/X5JUV1en4uJiLVy4MJK7ssbpfBbq9/vDnjtp0iTX+MqVKx1jJ06cCHu/SCzUfPj69OnjGHN7zbck19siDxw44Dq3vLzcMbZ8+XLXuYcPH3aMvfnmm65zm4rHQtu2bV3jd911l2Psxz/+caTTaVHNbv6HDx/W7t27A1+XlpZq27ZtSk9PV48ePTRjxgzNnz9fvXv3Vu/evTV//nydccYZuummmyKaOICWQc0DyafZzX/Lli0aNWpU4OuZM2dKkiZMmKAXX3xR9957r44ePaopU6YEHvjxzjvvqEOHDpHLGkCLoeaB5NPs5j9y5Ei5PRTQ4/EoPz9f+fn5p5MXgDhBzQPJh2f7AwBgGZo/AACWofkDAGAZmj8AAJaJ6H3+iC9NXYA1aNAgx1hTr+e88sorHWPvvPOO61zABk09xOjxxx93jF199dWuc93emHjzzTe7zt2yZYtjrKn73m3To0ePWKcQNZz5AwBgGZo/AACWofkDAGAZmj8AAJah+QMAYBmaPwAAluFWvyRWU1PjGnd7be/f/vY317nPP/+8Y2zdunWuc91uNVqyZInrXLdnzAPx5OQrjp00dTufm2uvvdYxVlxcHPZ2YQ/O/AEAsAzNHwAAy9D8AQCwDM0fAADL0PwBALAMzR8AAMvQ/AEAsAz3+Vvs//7v/xxjEydOdJ1bWFjoGPvpT3/qOtct3q5dO9e5L730kmOsvLzcdS7QkhYtWuQa93g8jrGm7tXnXv7QtWrlfI574sSJFswkvnDmDwCAZWj+AABYhuYPAIBlaP4AAFiG5g8AgGVo/gAAWKbZt/pt2LBBjz32mEpKSlReXq7Vq1fruuuuC8QnTpyo5cuXB80ZMmSINm3adNrJouWsXr3aNb5r1y7HWFO3OF1xxRWOsfnz57vOzc7Odow98sgjrnP/8Y9/uMbROGre2ZgxYxxjAwcOdJ3r9nrqNWvWhJsSTuF2O19Trwjftm1bhLOJH80+86+pqdGAAQO0ePFix3WuuuoqlZeXB8batWtPK0kAsUPNA8mn2Wf+eXl5ysvLc13H6/XK5/OFnRSA+EHNA8knKp/5r1+/Xl26dFGfPn00adIkVVZWRmM3AOIENQ8klog/3jcvL0833nijsrOzVVpaqgcffFCXX365SkpK5PV6G6xfW1ur2trawNdVVVWRTglAFDW35iXqHoi1iDf/cePGBf67f//+uvDCC5Wdna0333xTY8eObbB+QUGB5s6dG+k0ALSQ5ta8RN0DsRb1W/0yMzOVnZ3teHX4rFmz5Pf7A6OsrCzaKQGIoqZqXqLugViL+lv9Dh48qLKyMmVmZjYa93q9jn8aBJB4mqp5iboHYq3Zzf/w4cPavXt34OvS0lJt27ZN6enpSk9PV35+vq6//nplZmZqz549uv/++9WpUyf94Ac/iGjiiK2///3vjrEf/vCHrnOvueYax5jbq4IlafLkyY6x3r17u84dPXq0axyNo+adtW3b1jGWmprqOtftosiVK1eGnVMycvtFMT8/P+ztvvfee67xWbNmhb3teNfs5r9lyxaNGjUq8PXMmTMlSRMmTNDTTz+tjz76SC+99JIOHTqkzMxMjRo1SitXrlSHDh0ilzWAFkPNA8mn2c1/5MiRrk9Fevvtt08rIQDxhZoHkg/P9gcAwDI0fwAALEPzBwDAMjR/AAAsE/X7/GGfQ4cOucZ/+9vfOsZeeOEF17lt2jj/yA4fPtx17siRIx1j69evd50LRNo3H298qvLy8hbMJPaaeubDAw884Bi75557XOfu3bvXMfarX/3Kde7hw4dd44mMM38AACxD8wcAwDI0fwAALEPzBwDAMjR/AAAsQ/MHAMAyNH8AACzDff4Iy3nnnecYu+GGG1znXnTRRY4xt/v4m7Jjxw7X+IYNG8LeNhBpa9asiXUKLWrgwIGOsabu1R83bpxj7I033nCde/3117vGbcWZPwAAlqH5AwBgGZo/AACWofkDAGAZmj8AAJah+QMAYBlu9bPYOeec4xibNm2a69yxY8c6xnw+X9g5NeX48eOOsaZeg3rixIlIpwPLeTyesGKSdN111znG7rzzznBTipn/9//+n2v8wQcfdIylpaW5zn3llVccYzfffLN7YmgUZ/4AAFiG5g8AgGVo/gAAWIbmDwCAZWj+AABYhuYPAIBlaP4AAFimWff5FxQUaNWqVdq5c6fatm2roUOHauHChUH3ixtjNHfuXD333HP68ssvNWTIEC1ZskT9+vWLePJwv6d+/PjxrnPd7uXv2bNnuCmdli1btrjGH3nkEceYba9IbSnUvTNjTFgxyb12n3zySde5v/nNbxxjBw8edJ178cUXO8Z++tOfus4dMGCAY6xbt26ucz///HPH2Ntvv+06d+nSpa5xNF+zzvyLi4s1depUbdq0SUVFRaqvr1dubq5qamoC6zz66KNatGiRFi9erM2bN8vn82n06NGqrq6OePIAoo+6B5JPs87833rrraCvCwsL1aVLF5WUlGj48OEyxuiJJ57Q7NmzA0+AW758uTIyMvTqq69q8uTJkcscQIug7oHkc1qf+fv9fklSenq6JKm0tFQVFRXKzc0NrOP1ejVixAht3Lix0W3U1taqqqoqaACIX9Q9kPjCbv7GGM2cOVPDhg1T//79JUkVFRWSpIyMjKB1MzIyArFTFRQUKC0tLTC6d+8ebkoAooy6B5JD2M1/2rRp+vDDD/W73/2uQezUF1oYYxxfcjFr1iz5/f7AKCsrCzclAFFG3QPJIay3+k2fPl1r1qzRhg0bgq7wPHn1akVFhTIzMwPLKysrG5wVnOT1euX1esNJA0ALou6B5NGs5m+M0fTp07V69WqtX79eOTk5QfGcnBz5fD4VFRXp/PPPlyTV1dWpuLhYCxcujFzWScbpH0hJ+t73vuc6d/HixY6xvn37hp3T6fjrX//qGn/sscccY2+88YbrXF7L2/Ko++ho3bq1Y2zKlCmuc6+//nrHWFPXT/Tu3ds9sTA5Xd9x0rp16xxjv/zlLyOdDprQrOY/depUvfrqq3rjjTfUoUOHwOd5aWlpatu2rTwej2bMmKH58+erd+/e6t27t+bPn68zzjhDN910U1S+AQDRRd0DyadZzf/pp5+WJI0cOTJoeWFhoSZOnChJuvfee3X06FFNmTIl8LCPd955Rx06dIhIwgBaFnUPJJ9m/9m/KR6PR/n5+crPzw83JwBxhLoHkg/P9gcAwDI0fwAALEPzBwDAMjR/AAAsE9ZDftDQyeecN+bZZ591nTtw4EDH2He+851wUzotTd2z+6tf/cox1tTrOY8ePRpWTkC8+eCDDxxjmzdvdp170UUXhb1ft9cBuz03pClNvQ54xYoVjrE777wz7P2i5XHmDwCAZWj+AABYhuYPAIBlaP4AAFiG5g8AgGVo/gAAWIZb/b5hyJAhjrF77rnHde7gwYMdY127dg07p9Nx5MgR1/iTTz7pGJs/f77r3JqamrByApLJ3r17HWNjx451nTt58mTH2AMPPBB2Tk359a9/7Rg7+RInJ7t37450OogRzvwBALAMzR8AAMvQ/AEAsAzNHwAAy9D8AQCwDM0fAADL0PwBALCMxxhjYp3EN1VVVSktLS0m+16wYIFjrKn7/E/Hjh07HGN//OMfXefW19c7xtxeuytJhw4dco0j8fn9fnXs2DHWaTQplnUPJJtQ6p4zfwAALEPzBwDAMjR/AAAsQ/MHAMAyNH8AACxD8wcAwDamGebPn28uvPBC0759e9O5c2dz7bXXmp07dwatM2HCBCMpaAwZMiTkffj9/gbzGQxGeMPv9zenxKl7BiMJRih136wz/+LiYk2dOlWbNm1SUVGR6uvrlZub2+Dd7ldddZXKy8sDY+3atc3ZDYA4Qt0DyadNc1Z+6623gr4uLCxUly5dVFJSouHDhweWe71e+Xy+yGQIIKaoeyD5nNZn/n6/X5KUnp4etHz9+vXq0qWL+vTpo0mTJqmysvJ0dgMgjlD3QOIL+/G+xhhde+21+vLLL/X+++8Hlq9cuVLt27dXdna2SktL9eCDD6q+vl4lJSXyer0NtlNbW6va2trA11VVVerevXs4KQE4RaQf70vdA/EvpLpv1pU/3zBlyhSTnZ1tysrKXNfbt2+fSUlJMa+99lqj8Tlz5sT84ggGI1lHJC74o+4ZjMQaodR9WM1/2rRpplu3bubTTz8Naf2zzz7bLFiwoNHYsWPHjN/vD4yysrKYHzgGI1lGJJs/dc9gJMYIpe6bdcGfMUbTp0/X6tWrtX79euXk5DQ55+DBgyorK1NmZmajca/X2+ifBQHEB+oeSELN+MXf3H777SYtLc2sX7/elJeXB8aRI0eMMcZUV1ebu+66y2zcuNGUlpaadevWmUsuucR07drVVFVVhbQP7vdlMCI3InHmT90zGIk1Iv5nf6cdFRYWGmOMOXLkiMnNzTWdO3c2KSkppkePHmbChAnm888/D3kf/CPAYERuRKL5O22bumcw4nOEUvdhX+0fLVVVVUpLS4t1GkBSiPTV/tFC3QORE0rd82x/AAAsQ/MHAMAyNH8AACxD8wcAwDI0fwAALEPzBwDAMjR/AAAsQ/MHAMAyNH8AACxD8wcAwDI0fwAALBN3zT/OXjUAJLREqadEyRNIBKHUU9w1/+rq6linACSNRKmnRMkTSASh1FPcvdXvxIkT2rdvnzp06CCPx6Oqqip1795dZWVlCfF2sljiWIUu2Y+VMUbV1dXKyspSq1Zx9zt+A9R9+DhWoUv2Y9Wcum/TQjmFrFWrVurWrVuD5R07dkzK/1nRwLEKXTIfq0R6RS51f/o4VqFL5mMVat3H/ykBAACIKJo/AACWifvm7/V6NWfOHHm93linEvc4VqHjWMU3/v+EjmMVOo7Vv8TdBX8AACC64v7MHwAARBbNHwAAy9D8AQCwDM0fAADLxH3zX7p0qXJycvStb31LgwYN0vvvvx/rlGJuw4YNuuaaa5SVlSWPx6PXX389KG6MUX5+vrKystS2bVuNHDlS27dvj02yMVRQUKCLLrpIHTp0UJcuXXTdddfpk08+CVqHYxV/qPnGUfehoe5DE9fNf+XKlZoxY4Zmz56trVu36rLLLlNeXp4+//zzWKcWUzU1NRowYIAWL17caPzRRx/VokWLtHjxYm3evFk+n0+jR4+27vnpxcXFmjp1qjZt2qSioiLV19crNzdXNTU1gXU4VvGFmndG3YeGug+RiWODBw82t912W9Cyvn37mvvuuy9GGcUfSWb16tWBr0+cOGF8Pp9ZsGBBYNmxY8dMWlqaeeaZZ2KQYfyorKw0kkxxcbExhmMVj6j50FD3oaPuGxe3Z/51dXUqKSlRbm5u0PLc3Fxt3LgxRlnFv9LSUlVUVAQdN6/XqxEjRlh/3Px+vyQpPT1dEscq3lDz4eNn2Rl137i4bf4HDhzQ8ePHlZGREbQ8IyNDFRUVMcoq/p08Nhy3YMYYzZw5U8OGDVP//v0lcaziDTUfPn6WG0fdO4u7t/qdyuPxBH1tjGmwDA1x3IJNmzZNH374of7yl780iHGs4gv/P8LHsQtG3TuL2zP/Tp06qXXr1g1+E6usrGzwGxv+xefzSRLH7RumT5+uNWvWaN26dUGvjeVYxRdqPnz8LDdE3buL2+afmpqqQYMGqaioKGh5UVGRhg4dGqOs4l9OTo58Pl/Qcaurq1NxcbF1x80Yo2nTpmnVqlV67733lJOTExTnWMUXaj58/Cz/C3UfolhdaRiKFStWmJSUFLNs2TKzY8cOM2PGDNOuXTuzZ8+eWKcWU9XV1Wbr1q1m69atRpJZtGiR2bp1q/nss8+MMcYsWLDApKWlmVWrVpmPPvrIjB8/3mRmZpqqqqoYZ96ybr/9dpOWlmbWr19vysvLA+PIkSOBdThW8YWad0bdh4a6D01cN39jjFmyZInJzs42qamp5oILLgjcrmGzdevWGUkNxoQJE4wxX9/KMmfOHOPz+YzX6zXDhw83H330UWyTjoHGjpEkU1hYGFiHYxV/qPnGUfehoe5Dwyt9AQCwTNx+5g93L774ojwej7Zs2RKR7Xk8Hk2bNi0i2/rmNvPz8yOyrXfffVcej0cej0cHDhyIyDaBRGND3T/wwAMaM2aMunbtKo/Ho4kTJ0YsN/wLzR9x7/Dhw5o0aZKysrJinQqAKPuP//gPHTx4UN///veVmpoa63SSFs0fce++++7TmWeeqVtuuSXWqQCIsurqan3wwQd6+umnlZKSEut0khbNP4kdO3ZMd911lwYOHKi0tDSlp6frkksu0RtvvOE459lnn1WfPn3k9Xr1ve99TytWrGiwTkVFhSZPnqxu3bopNTVVOTk5mjt3rurr6yP+Pbz//vt67rnn9MILL6h169YR3z6QbBK97lu1oi21hLh/wh/CV1tbq3/+85+6++671bVrV9XV1endd9/V2LFjVVhYqJtvvjlo/ZMPxHjooYfUrl07LV26VOPHj1ebNm10ww03SPr6H4DBgwerVatW+uUvf6levXrpgw8+0Lx587Rnzx4VFha65tSzZ09J0p49e5rM/+jRo7r11ls1Y8YMXXDBBVqzZk1YxwGwSaLXPVpIrG83QHgKCwuNJLN58+aQ59TX15uvvvrK3Hrrreb8888Pikkybdu2NRUVFUHr9+3b15x99tmBZZMnTzbt27cP3Ft80uOPP24kme3btwdtc86cOUHr9erVy/Tq1SukfO+66y7zne98J3B/7pw5c4wk88UXX4Q0H0g2NtT9N7Vr1y5wKyMii7+vJLnf//73uvTSS9W+fXu1adNGKSkpWrZsmT7++OMG615xxRVBj7ds3bq1xo0bp927d2vv3r2SpD/+8Y8aNWqUsrKyVF9fHxh5eXmSvn6Xtpvdu3dr9+7dTeb93//933riiSf07LPPqm3bts35lgHrJWrdo+XQ/JPYqlWr9MMf/lBdu3bVyy+/rA8++ECbN2/WLbfcomPHjjVY/+QzrxtbdvDgQUnS/v379Yc//EEpKSlBo1+/fpIUsdvwbrnlFo0dO1YXXnihDh06pEOHDgVyrqqqUnV1dUT2AySbRK57tBw+809iL7/8snJycrRy5cqgt1XV1tY2un5jr7M8ueyss86S9PXLV8477zw98sgjjW4jUrfjbd++Xdu3b9fvf//7BrFevXppwIAB2rZtW0T2BSSTRK57tByafxLzeDxKTU0N+gegoqLC8arfP//5z9q/f3/gT4DHjx/XypUr1atXr8BbscaMGaO1a9eqV69eOvPMM6OW+7p16xose/HFF7V8+XK9/vrr6tq1a9T2DSSyRK57tByaf4J77733Gr2C9uqrr9aYMWO0atUqTZkyRTfccIPKysr08MMPKzMzU7t27Wowp1OnTrr88sv14IMPBq763blzZ9BtPw899FDgLWt33HGHzjnnHB07dkx79uzR2rVr9cwzzwS9PvNUZ599tiQ1+fnfyJEjGyxbv369JOnSSy9Vp06dXOcDySxZ6176+vqBL774QtLXv4h89tln+q//+i9J0ogRI9S5c+cmt4EQxPqKQ4Tn5FW/TqO0tNQY8/Xbq3r27Gm8Xq/57ne/a55//vnAVfPfJMlMnTrVLF261PTq1cukpKSYvn37mldeeaXBvr/44gtzxx13mJycHJOSkmLS09PNoEGDzOzZs83hw4eDtnnqVb/Z2dkmOzs7rO+Zq/1hOxvqfsSIEY7f37p165pzuOCCF/sAAGAZrvYHAMAyNH8AACxD8wcAwDI0fwAALEPzBwDAMjR/AAAsE3cP+Tlx4oT27dunDh06BD2hCkDojDGqrq5WVlZWQrwfnboHTl+z6j5aDxBYsmRJ4CETF1xwgdmwYUNI88rKylwfYsFgMEIfZWVl0SrxBsKteWOoewYjkiOUuo9K81+xYoVJSUkxzz//vNmxY4e58847Tbt27Rq8C7oxhw4divmBYzCSZRw6dCgaJd7A6dS8MdQ9gxHJEUrdR6X5Dx482Nx2221By/r27Wvuu+++Juf6/f6YHzgGI1mG3++PRok3cDo1bwx1z2BEcoRS9xH/MLCurk4lJSXKzc0NWp6bm6uNGzc2WL+2tlZVVVVBA0DiaG7NS9Q9EGsRb/4HDhzQ8ePHA6+HPCkjI6PR90YXFBQoLS0tMLp37x7plABEUXNrXqLugViL2mXAp16xa4xp9CreWbNmye/3B0ZZWVm0UgIQRaHWvETdA7EW8Vv9OnXqpNatWzf4jb+ysrLBmYEkeb1eeb3eSKcBoIU0t+Yl6h6ItYif+aempmrQoEEqKioKWl5UVKShQ4dGencAYoyaBxJQGBf2NunkbT/Lli0zO3bsMDNmzDDt2rUze/bsaXIuV/0yGJEbLXW1/+nUvDHUPYMRyRFK3UflCX/jxo3TwYMH9dBDD6m8vFz9+/fX2rVrlZ2dHY3dAYgxah5ILB5jjIl1Et9UVVWltLS0WKcBJAW/36+OHTvGOo0mUfdA5IRS9/H/0G8AABBRNH8AACxD8wcAwDI0fwAALEPzBwDAMjR/AAAsQ/MHAMAyNH8AACxD8wcAwDI0fwAALEPzBwDAMjR/AAAsQ/MHAMAyNH8AACxD8wcAwDI0fwAALEPzBwDAMjR/AAAsQ/MHAMAyNH8AACxD8wcAwDI0fwAALEPzBwDAMjR/AAAsQ/MHAMAyNH8AACzTJtIbzM/P19y5c4OWZWRkqKKiItK7AprtiiuucIy98sorrnNHjBjhGPvkk0/CzikZUPeIpgceeMAxdurP3alatXI+xx05cqTr3OLiYtd4Iot485ekfv366d133w183bp162jsBkAcoe6BxBGV5t+mTRv5fL5obBpAnKLugcQRlc/8d+3apaysLOXk5OhHP/qRPv3002jsBkAcoe6BxBHxM/8hQ4bopZdeUp8+fbR//37NmzdPQ4cO1fbt23XWWWc1WL+2tla1tbWBr6uqqiKdEoAoo+6BxBLxM/+8vDxdf/31Ovfcc3XllVfqzTfflCQtX7680fULCgqUlpYWGN27d490SgCijLoHEkvUb/Vr166dzj33XO3atavR+KxZs+T3+wOjrKws2ikBiDLqHohvUbng75tqa2v18ccf67LLLms07vV65fV6o50GgBZE3QPxLeLN/+6779Y111yjHj16qLKyUvPmzVNVVZUmTJgQ6V1F3PDhw13jjX12edLq1asjnQ6i4KKLLnKMbd68uQUzSS6JXPeIvYkTJ7rGf/GLXzjGTpw4EfZ+jTFhz010EW/+e/fu1fjx43XgwAF17txZF198sTZt2qTs7OxI7wpAnKDugcQS8ea/YsWKSG8SQJyj7oHEwrP9AQCwDM0fAADL0PwBALAMzR8AAMtE/T7/RNLU6x179+7tGONWv/jg9vpOScrJyXGMNXVlusfjCSsnAO6aqr1vfetbLZSJPTjzBwDAMjR/AAAsQ/MHAMAyNH8AACxD8wcAwDI0fwAALEPzBwDAMtzn/w0333yza/yDDz5ooUwQrszMTNf4pEmTHGMvv/yy69ydO3eGlRMA6corr3SMTZ8+PeztNlWXY8aMcYzt378/7P0mOs78AQCwDM0fAADL0PwBALAMzR8AAMvQ/AEAsAzNHwAAy3Cr3zc09TpYxL8XXngh7Lm7du2KYCaAXYYNG+YaLywsdIylpaWFvd/HHnvMNf7ZZ5+Fve1kRrcDAMAyNH8AACxD8wcAwDI0fwAALEPzBwDAMjR/AAAsQ/MHAMAyzb7Pf8OGDXrsscdUUlKi8vJyrV69Wtddd10gbozR3Llz9dxzz+nLL7/UkCFDtGTJEvXr1y+SeYftvPPOc4xlZGS0YCaIhtO5X7ioqCiCmSSPRK95tIwJEya4xrOyssLe9vr16x1jL730UtjbtVmzz/xramo0YMAALV68uNH4o48+qkWLFmnx4sXavHmzfD6fRo8ererq6tNOFkDLo+aB5NPsM/+8vDzl5eU1GjPG6IknntDs2bM1duxYSdLy5cuVkZGhV199VZMnTz69bAG0OGoeSD4R/cy/tLRUFRUVys3NDSzzer0aMWKENm7c2Oic2tpaVVVVBQ0AiSGcmpeoeyDWItr8KyoqJDX87DwjIyMQO1VBQYHS0tICo3v37pFMCUAUhVPzEnUPxFpUrvb3eDxBXxtjGiw7adasWfL7/YFRVlYWjZQARFFzal6i7oFYi+hb/Xw+n6SvzwYyMzMDyysrKx2vpPd6vfJ6vZFMA0ALCafmJeoeiLWINv+cnBz5fD4VFRXp/PPPlyTV1dWpuLhYCxcujOSuwnb11Vc7xtq2bduCmSBcbk0lJycn7O3+4x//CHuurRKh5hE5nTp1cozdcsstrnNPnDjhGDt06JDr3Hnz5rnG0XzNbv6HDx/W7t27A1+XlpZq27ZtSk9PV48ePTRjxgzNnz9fvXv3Vu/evTV//nydccYZuummmyKaOICWQc0DyafZzX/Lli0aNWpU4OuZM2dK+voBDy+++KLuvfdeHT16VFOmTAk88OOdd95Rhw4dIpc1gBZDzQPJp9nNf+TIkTLGOMY9Ho/y8/OVn59/OnkBiBPUPJB8eLY/AACWofkDAGAZmj8AAJah+QMAYJmI3uefCM4555yw527fvj2CmSBcjz/+uGOsqdcy/+///q9jjLfQwXY9e/Z0jb/22mtR2e9TTz3lGl+3bl1U9mszzvwBALAMzR8AAMvQ/AEAsAzNHwAAy9D8AQCwDM0fAADLWHer3+nYvHlzrFNIGB07dnSNX3XVVY6xn/zkJ65zc3Nzw8pJkh5++GHHWFOvFQWSnVtdStJ5550X9rb//Oc/O8Z+/etfh71dhIczfwAALEPzBwDAMjR/AAAsQ/MHAMAyNH8AACxD8wcAwDI0fwAALMN9/s2Qnp4ek/0OGDDAMebxeFznXnnllY6xbt26uc5NTU11jP34xz92nduqlfvvlUePHnWM/fWvf3WdW1tb6xhr08b9R7qkpMQ1DiS76667zjG2YMGCsLf7l7/8xTU+YcIEx5jf7w97vwgPZ/4AAFiG5g8AgGVo/gAAWIbmDwCAZWj+AABYhuYPAIBlmn2r34YNG/TYY4+ppKRE5eXlWr16ddCtIxMnTtTy5cuD5gwZMkSbNm067WQjwe0WM2OM69xnnnnGMXb//feHnVNT3F6j2dStfvX19Y6xI0eOuM7dsWOHY+w3v/mN69wtW7a4xouLix1j+/fvd527d+9ex1jbtm1d5+7cudM1joYSveZt07NnT9f4a6+9FpX9fvrpp67xpuoaLavZZ/41NTUaMGCAFi9e7LjOVVddpfLy8sBYu3btaSUJIHaoeSD5NPvMPy8vT3l5ea7reL1e+Xy+sJMCED+oeSD5ROUz//Xr16tLly7q06ePJk2apMrKymjsBkCcoOaBxBLxx/vm5eXpxhtvVHZ2tkpLS/Xggw/q8ssvV0lJibxeb4P1a2trgx7XWlVVFemUAERRc2teou6BWIt48x83blzgv/v3768LL7xQ2dnZevPNNzV27NgG6xcUFGju3LmRTgNAC2luzUvUPRBrUb/VLzMzU9nZ2dq1a1ej8VmzZsnv9wdGWVlZtFMCEEVN1bxE3QOxFvW3+h08eFBlZWXKzMxsNO71eh3/NAgg8TRV8xJ1D8Ras5v/4cOHtXv37sDXpaWl2rZtm9LT05Wenq78/Hxdf/31yszM1J49e3T//ferU6dO+sEPfhDRxMM1ZcoUx9hnn33mOnfo0KGRTickn3/+uWPs9ddfd5378ccfO8bi9T7sn//8567xzp07O8aautcYzZfoNW+bX/ziF67xEydORGW/p/M6YLS8Zjf/LVu2aNSoUYGvZ86cKenrdzU//fTT+uijj/TSSy/p0KFDyszM1KhRo7Ry5Up16NAhclkDaDHUPJB8mt38R44c6fokvLfffvu0EgIQX6h5IPnwbH8AACxD8wcAwDI0fwAALEPzBwDAMlG/zz+RLFy4MNYpQNIVV1wR9txova4UiCcDBw50jOXm5kZtv2+88YZj7JNPPonafhF5nPkDAGAZmj8AAJah+QMAYBmaPwAAlqH5AwBgGZo/AACWofkDAGAZ7vNHUlm9enWsUwCi7p133nGMnXnmmWFvt6nXfE+cODHsbSO+cOYPAIBlaP4AAFiG5g8AgGVo/gAAWIbmDwCAZWj+AABYhlv9ACDBnHXWWY6xEydOhL3dpUuXusYPHz4c9rYRXzjzBwDAMjR/AAAsQ/MHAMAyNH8AACxD8wcAwDI0fwAALEPzBwDAMs26z7+goECrVq3Szp071bZtWw0dOlQLFy7UOeecE1jHGKO5c+fqueee05dffqkhQ4ZoyZIl6tevX8STh508Ho9jrE+fPq5zm3plKRqi7lteYWGha7xVq+ict23cuDEq20X8adZPUHFxsaZOnapNmzapqKhI9fX1ys3NVU1NTWCdRx99VIsWLdLixYu1efNm+Xw+jR49WtXV1RFPHkD0UfdA8mnWmf9bb70V9HVhYaG6dOmikpISDR8+XMYYPfHEE5o9e7bGjh0rSVq+fLkyMjL06quvavLkyZHLHECLoO6B5HNafzvy+/2SpPT0dElSaWmpKioqlJubG1jH6/VqxIgRjn9Oqq2tVVVVVdAAEL+oeyDxhd38jTGaOXOmhg0bpv79+0uSKioqJEkZGRlB62ZkZARipyooKFBaWlpgdO/ePdyUAEQZdQ8kh7Cb/7Rp0/Thhx/qd7/7XYPYqRdkGWMcL9KaNWuW/H5/YJSVlYWbEoAoo+6B5BDWW/2mT5+uNWvWaMOGDerWrVtguc/nk/T1mUBmZmZgeWVlZYOzgpO8Xq+8Xm84aQBoQdQ9kDya1fyNMZo+fbpWr16t9evXKycnJyiek5Mjn8+noqIinX/++ZKkuro6FRcXa+HChZHLGlYzxjjGonULlM2o++gYOHCgY+zKK690nev22t66ujrXuUuWLHGM7d+/33Uukkezmv/UqVP16quv6o033lCHDh0Cn+elpaWpbdu28ng8mjFjhubPn6/evXurd+/emj9/vs444wzddNNNUfkGAEQXdQ8kn2Y1/6efflqSNHLkyKDlhYWFmjhxoiTp3nvv1dGjRzVlypTAwz7eeecddejQISIJA2hZ1D2QfJr9Z/+meDwe5efnKz8/P9ycAMQR6h5IPnxACgCAZWj+AABYhuYPAIBlaP4AAFgmrIf8APHqkksucY2/+OKLLZMI0IRvf/vbjrGTD04Kxz/+8Q/X+N133x32tpE8OPMHAMAyNH8AACxD8wcAwDI0fwAALEPzBwDAMjR/AAAsw61+SDgejyfWKQBAQuPMHwAAy9D8AQCwDM0fAADL0PwBALAMzR8AAMvQ/AEAsAzNHwAAy3CfP+LOn/70J9f4jTfe2EKZANGzc+dOx9jGjRtd5w4bNizS6cAynPkDAGAZmj8AAJah+QMAYBmaPwAAlqH5AwBgGZo/AAC2Mc0wf/58c+GFF5r27dubzp07m2uvvdbs3LkzaJ0JEyYYSUFjyJAhIe/D7/c3mM9gMMIbfr+/OSVO3TMYSTBCqftmnfkXFxdr6tSp2rRpk4qKilRfX6/c3FzV1NQErXfVVVepvLw8MNauXduc3QCII9Q9kHya9ZCft956K+jrwsJCdenSRSUlJRo+fHhgudfrlc/ni0yGAGKKugeSz2l95u/3+yVJ6enpQcvXr1+vLl26qE+fPpo0aZIqKytPZzcA4gh1DyQ+jzHGhDPRGKNrr71WX375pd5///3A8pUrV6p9+/bKzs5WaWmpHnzwQdXX16ukpERer7fBdmpra1VbWxv4uqqqSt27dw8nJQCn8Pv96tixY8S2R90D8S+kum/WlT/fMGXKFJOdnW3Kyspc19u3b59JSUkxr732WqPxOXPmxPziCAYjWUckLvij7hmMxBqh1H1YzX/atGmmW7du5tNPPw1p/bPPPtssWLCg0dixY8eM3+8PjLKyspgfOAYjWUYkmz91z2Akxgil7pt1wZ8xRtOnT9fq1au1fv165eTkNDnn4MGDKisrU2ZmZqNxr9fb6J8FAcQH6h5IQs34xd/cfvvtJi0tzaxfv96Ul5cHxpEjR4wxxlRXV5u77rrLbNy40ZSWlpp169aZSy65xHTt2tVUVVWFtA/u92UwIjciceZP3TMYiTUi/md/px0VFhYaY4w5cuSIyc3NNZ07dzYpKSmmR48eZsKECebzzz8PeR/8I8BgRG5Eovk7bZu6ZzDic4RS92Ff7R8tVVVVSktLi3UaQFKI9NX+0ULdA5ETSt3zbH8AACxD8wcAwDI0fwAALEPzBwDAMjR/AAAsQ/MHAMAyNH8AACxD8wcAwDI0fwAALEPzBwDAMjR/AAAsE3fNP85eNQAktESpp0TJE0gEodRT3DX/6urqWKcAJI1EqadEyRNIBKHUU9y91e/EiRPat2+fOnToII/Ho6qqKnXv3l1lZWUJ8XayWOJYhS7Zj5UxRtXV1crKylKrVnH3O34D1H34OFahS/Zj1Zy6b9NCOYWsVatW6tatW4PlHTt2TMr/WdHAsQpdMh+rRHpFLnV/+jhWoUvmYxVq3cf/KQEAAIgomj8AAJaJ++bv9Xo1Z84ceb3eWKcS9zhWoeNYxTf+/4SOYxU6jtW/xN0FfwAAILri/swfAABEFs0fAADL0PwBALAMzR8AAMvEffNfunSpcnJy9K1vfUuDBg3S+++/H+uUYm7Dhg265pprlJWVJY/Ho9dffz0oboxRfn6+srKy1LZtW40cOVLbt2+PTbIxVFBQoIsuukgdOnRQly5ddN111+mTTz4JWodjFX+o+cZR96Gh7kMT181/5cqVmjFjhmbPnq2tW7fqsssuU15enj7//PNYpxZTNTU1GjBggBYvXtxo/NFHH9WiRYu0ePFibd68WT6fT6NHj7bu+enFxcWaOnWqNm3apKKiItXX1ys3N1c1NTWBdThW8YWad0bdh4a6D5GJY4MHDza33XZb0LK+ffua++67L0YZxR9JZvXq1YGvT5w4YXw+n1mwYEFg2bFjx0xaWpp55plnYpBh/KisrDSSTHFxsTGGYxWPqPnQUPeho+4bF7dn/nV1dSopKVFubm7Q8tzcXG3cuDFGWcW/0tJSVVRUBB03r9erESNGWH/c/H6/JCk9PV0SxyreUPPh42fZGXXfuLht/gcOHNDx48eVkZERtDwjI0MVFRUxyir+nTw2HLdgxhjNnDlTw4YNU//+/SVxrOINNR8+fpYbR907i7u3+p3K4/EEfW2MabAMDXHcgk2bNk0ffvih/vKXvzSIcaziC/8/wsexC0bdO4vbM/9OnTqpdevWDX4Tq6ysbPAbG/7F5/NJEsftG6ZPn641a9Zo3bp1Qa+N5VjFF2o+fPwsN0Tdu4vb5p+amqpBgwapqKgoaHlRUZGGDh0ao6ziX05Ojnw+X9Bxq6urU3FxsXXHzRijadOmadWqVXrvvfeUk5MTFOdYxRdqPnz8LP8LdR+iWF1pGIoVK1aYlJQUs2zZMrNjxw4zY8YM065dO7Nnz55YpxZT1dXVZuvWrWbr1q1Gklm0aJHZunWr+eyzz4wxxixYsMCkpaWZVatWmY8++siMHz/eZGZmmqqqqhhn3rJuv/12k5aWZtavX2/Ky8sD48iRI4F1OFbxhZp3Rt2HhroPTVw3f2OMWbJkicnOzjapqanmggsuCNyuYbN169YZSQ3GhAkTjDFf38oyZ84c4/P5jNfrNcOHDzcfffRRbJOOgcaOkSRTWFgYWIdjFX+o+cZR96Gh7kPDK30BALBM3H7mDwAAooPmDwCAZWj+AABYhuYPAIBlaP4AAFiG5g8AgGVo/gAAWIbmDwCAZWj+AABYhuYPAIBlaP4AAFiG5g8AgGX+P2gOSsmydmYzAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 600x600 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist(flatten=False)\n",
        "\n",
        "plt.figure(figsize=[6,6])\n",
        "for i in range(4):\n",
        "    plt.subplot(2,2,i+1)\n",
        "    plt.title(\"Label: %i\"%y_train[i])\n",
        "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGXtnvX4OoE7",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "В нашей реализации сеть - просто список (Python-list) слоев.\n",
        "\n",
        "\n",
        "\n",
        "> Обратите внимание, что у нас нет глобального пулинга. При изменении архитектуры сети вы должны поменять входую размерность в последнем Dense слое\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "VicezF_TOoE8",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "network = []\n",
        "network.append(Conv2d(1, 4, 5))\n",
        "network.append(MaxPool2d(2))\n",
        "network.append(ReLU())\n",
        "network.append(Conv2d(4, 8, 5))\n",
        "network.append(MaxPool2d(2))\n",
        "network.append(ReLU())\n",
        "network.append(Flatten())\n",
        "network.append(Dense(3200, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "1sUrd667ZDuK"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "\n",
        "optimizer = SGDOptimizer()\n",
        "scheduler = LRScheduler(learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "482hIf_UOoE9",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Реализуйте прямой проход по целой сети, последовательно вызывая .forward() для каждого слоя."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "OKRyUyj5OoE9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def forward(network, X):\n",
        "    \"\"\"\n",
        "    Compute activations of all network layers by applying them sequentially.\n",
        "    Return a list of activations for each layer.\n",
        "    Make sure last activation corresponds to network logits.\n",
        "    \"\"\"\n",
        "    activations = []\n",
        "    input = X\n",
        "\n",
        "    for layer in network:\n",
        "        input = layer.forward(input)\n",
        "        activations.append(input)\n",
        "\n",
        "    assert len(activations) == len(network)\n",
        "    return activations\n",
        "\n",
        "def predict(network, X):\n",
        "    \"\"\"\n",
        "    Use network to predict the most likely class for each sample.\n",
        "    \"\"\"\n",
        "    logits = forward(network, X)[-1]\n",
        "    return logits.argmax(axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "UfUyHKPyOoE-",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def train(network, X, y, optimizer, scheduler):\n",
        "   \"\"\"\n",
        "   Train your network on a given batch of X and y.\n",
        "   You first need to run forward to get all layer activations.\n",
        "   Then you can run layer.backward going from last to first layer.\n",
        "\n",
        "   After you called backward for all layers, all Dense layers have already made one gradient step.\n",
        "   \"\"\"\n",
        "\n",
        "   # Get the layer activations\n",
        "   layer_activations = forward(network,X)\n",
        "   layer_inputs = [X] + layer_activations  #layer_input[i] is an input for network[i]\n",
        "   logits = layer_activations[-1]\n",
        "\n",
        "   # Compute the loss and the initial gradient\n",
        "   loss = softmax_crossentropy_with_logits(logits,y)\n",
        "   loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
        "\n",
        "   gradients = []\n",
        "   for i in range(0, len(network)):\n",
        "      cur_pos = len(network) - 1 - i\n",
        "      gradients.append(loss_grad)\n",
        "      loss_grad = network[cur_pos].backward(layer_inputs[cur_pos], loss_grad)\n",
        "\n",
        "   # update weights and biases with optimizer\n",
        "   for i in range(0, len(network)):\n",
        "      cur_pos = len(network) - 1 - i\n",
        "      grad_weights = gradients[i]\n",
        "      lr = scheduler.get_lr()\n",
        "      optimizer.step(network[cur_pos].weights, grad_weights, lr)\n",
        "\n",
        "   # update learning rate\n",
        "   #<YOUR CODE>\n",
        "\n",
        "   return np.mean(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJG4VMsROoE_",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Все готово для запуска обучения. Если все реализовано корректно, то точность классификации на валидационном множестве **должна быть около** 99%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "kq5XTDNNOoE_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
        "    assert len(inputs) == len(targets)\n",
        "    if shuffle:\n",
        "        indices = np.random.permutation(len(inputs))\n",
        "    for start_idx in tqdm(range(0, len(inputs) - batchsize + 1, batchsize)):\n",
        "        if shuffle:\n",
        "            excerpt = indices[start_idx:start_idx + batchsize]\n",
        "        else:\n",
        "            excerpt = slice(start_idx, start_idx + batchsize)\n",
        "        yield inputs[excerpt], targets[excerpt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "7q2KcwkTOoFA",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "train_log = []\n",
        "val_log = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "kaxQu9WsOoFB",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14992bca4b5f4c5ab787fc2efdc33f92",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1562 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (3200,10) (32,10) ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[77], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x_batch,y_batch \u001b[38;5;129;01min\u001b[39;00m iterate_minibatches(X_train, y_train, batchsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m----> 4\u001b[0m         train(network, x_batch, y_batch, optimizer, scheduler)\n\u001b[1;32m      6\u001b[0m     train_log\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(predict(network, X_train) \u001b[38;5;241m==\u001b[39m y_train))\n\u001b[1;32m      7\u001b[0m     val_log\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(predict(network, X_val) \u001b[38;5;241m==\u001b[39m y_val))\n",
            "Cell \u001b[0;32mIn[74], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(network, X, y, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     28\u001b[0m    grad_weights \u001b[38;5;241m=\u001b[39m gradients[i]\n\u001b[1;32m     29\u001b[0m    lr \u001b[38;5;241m=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39mget_lr()\n\u001b[0;32m---> 30\u001b[0m    optimizer\u001b[38;5;241m.\u001b[39mstep(network[cur_pos]\u001b[38;5;241m.\u001b[39mweights, grad_weights, lr)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# update learning rate\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#<YOUR CODE>\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(loss)\n",
            "Cell \u001b[0;32mIn[66], line 28\u001b[0m, in \u001b[0;36mSGDOptimizer.step\u001b[0;34m(self, weights, grad_weights, lr)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum_buffer \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     25\u001b[0m                             (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdampening) \u001b[38;5;241m*\u001b[39m grad_weights\n\u001b[1;32m     26\u001b[0m     grad_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum_buffer\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weights \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum_buffer\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3200,10) (32,10) "
          ]
        }
      ],
      "source": [
        "for epoch in range(15):\n",
        "\n",
        "    for x_batch,y_batch in iterate_minibatches(X_train, y_train, batchsize=32, shuffle=True):\n",
        "        train(network, x_batch, y_batch, optimizer, scheduler)\n",
        "\n",
        "    train_log.append(np.mean(predict(network, X_train) == y_train))\n",
        "    val_log.append(np.mean(predict(network, X_val) == y_val))\n",
        "\n",
        "    clear_output()\n",
        "    print(\"Epoch\",epoch)\n",
        "    print(\"Train accuracy:\",train_log[-1])\n",
        "    print(\"Val accuracy:\",val_log[-1])\n",
        "    plt.plot(train_log,label='train accuracy')\n",
        "    plt.plot(val_log,label='val accuracy')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
