{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ3DJzu7FDX_"
      },
      "source": [
        "# Домашнее задание 2.1. Сверточные сети\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQVARDpwNsOa"
      },
      "source": [
        "В этом задании вы должны:\n",
        "1. Написать слой Conv2d на Numpy и определить в нем forward-backward методы\n",
        "2. Определить слой MaxPool2d\n",
        "3. Написать всю необходимую обвязку для обучения: оптимизатор с адаптивным шагом и класс, позволяющий изменять расписание для learning rate'а\n",
        "\n",
        "\n",
        "\n",
        "> Обратите внимание, что в этом задании больше нет тестов.\n",
        "> Вы должны сами проверять свой код.  \n",
        "> Это можно сделать так:\n",
        "> 1. Написать юнит-тесты с помощью Pytorch. То есть, ваш модудь должен повторять поведение torch'а\n",
        "> 2. Проверять архитектуру не на всем датасете, а на подвыборке: при наивной имплементации слоев одна эпоха на всем датасете будет занимать около двух часов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMVkqpoEOoD3",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Numpy-имплементация сверточной нейронной сети\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmTxOp7KOoEG",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Вставьте сюда имплементацию из первого домашнего задания.\n",
        "\n",
        "\n",
        "\n",
        "> Обратите внимание, что обновление весов теперь производится с помощью специального класса **Optimizer**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lmfLBp4tOoEN",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "def load_mnist(flatten=False):\n",
        "    \"\"\"taken from https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py\"\"\"\n",
        "    # We first define a download function, supporting both Python 2 and 3.\n",
        "    if sys.version_info[0] == 2:\n",
        "        from urllib import urlretrieve\n",
        "    else:\n",
        "        from urllib.request import urlretrieve\n",
        "\n",
        "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
        "        print(\"Downloading %s\" % filename)\n",
        "        urlretrieve(source + filename, filename)\n",
        "\n",
        "    # We then define functions for loading MNIST images and labels.\n",
        "    # For convenience, they also download the requested files if needed.\n",
        "    import gzip\n",
        "\n",
        "    def load_mnist_images(filename):\n",
        "        if not os.path.exists(filename):\n",
        "            download(filename)\n",
        "        # Read the inputs in Yann LeCun's binary format.\n",
        "        with gzip.open(filename, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
        "        # following the shape convention: (examples, channels, rows, columns)\n",
        "        data = data.reshape(-1, 1, 28, 28)\n",
        "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
        "        # (Actually to range [0, 255/256], for compatibility to the version\n",
        "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
        "        return data / np.float32(256)\n",
        "\n",
        "    def load_mnist_labels(filename):\n",
        "        if not os.path.exists(filename):\n",
        "            download(filename)\n",
        "        # Read the labels in Yann LeCun's binary format.\n",
        "        with gzip.open(filename, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "        # The labels are vectors of integers now, that's exactly what we want.\n",
        "        return data\n",
        "\n",
        "    # We can now download and read the training and test set images and labels.\n",
        "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
        "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
        "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
        "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "    # We reserve the last 10000 training examples for validation.\n",
        "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
        "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
        "\n",
        "    if flatten:\n",
        "        X_train = X_train.reshape([X_train.shape[0], -1])\n",
        "        X_val = X_val.reshape([X_val.shape[0], -1])\n",
        "        X_test = X_test.reshape([X_test.shape[0], -1])\n",
        "\n",
        "    # We just return all the arrays in order, as expected in main().\n",
        "    # (It doesn't matter how we do this as long as we can read them again.)\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jYZ3ptaiOoER",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    \"\"\"\n",
        "    A building block. Each layer is capable of performing two things:\n",
        "\n",
        "    - Process input to get output:           output = layer.forward(input)\n",
        "\n",
        "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
        "\n",
        "    Some layers also have learnable parameters which they update during layer.backward.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"Here you can initialize layer parameters (if any) and auxiliary stuff.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n",
        "        \"\"\"\n",
        "        return input\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the layer, with respect to the given input.\n",
        "\n",
        "        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n",
        "\n",
        "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
        "\n",
        "        Luckily, you already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
        "\n",
        "        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss / d layer\n",
        "        \"\"\"\n",
        "        input_dim = input.shape[1]\n",
        "\n",
        "        d_layer_d_input = np.eye(input_dim)\n",
        "\n",
        "        return np.dot(grad_output, d_layer_d_input) # chain rule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Qe246l61OoEe",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
        "        pass\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Apply elementwise ReLU to [batch, input_units] matrix\"\"\"\n",
        "        output = np.array(input)\n",
        "        output[output < 0] = 0\n",
        "        return output\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"Compute gradient of loss w.r.t. ReLU input\"\"\"\n",
        "        relu_grad_mask = np.zeros_like(input)\n",
        "        relu_grad_mask[input > 0] = 1\n",
        "        return grad_output * relu_grad_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "g22Gzs_2OoEl",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class Dense(Layer):\n",
        "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        A dense layer is a layer which performs a learned affine transformation:\n",
        "        f(x) = <W*x> + b\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # initialize weights with small random numbers from normal distribution\n",
        "        # you can change the intializtion method\n",
        "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
        "        self.biases = np.zeros(output_units)\n",
        "\n",
        "    def forward(self,input):\n",
        "        \"\"\"\n",
        "        Perform an affine transformation:\n",
        "        f(x) = <W*x> + b\n",
        "\n",
        "        input shape: [batch, input_units]\n",
        "        output shape: [batch, output_units]\n",
        "        \"\"\"\n",
        "        return np.dot(input, self.weights) + self.biases\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "\n",
        "        # compute d f / d x = d f / d dense * d dense / d x\n",
        "        # where d dense/ d x = weights transposed\n",
        "        # grad_output is a derivative of the next (in forward) layer of prediction\n",
        "        # this result is needed for the next layer\n",
        "        grad_input = np.dot(grad_output, np.transpose(self.weights))\n",
        "\n",
        "        # compute gradient w.r.t. weights and biases\n",
        "        grad_weights = np.dot(np.transpose(input), grad_output)\n",
        "        grad_biases = np.sum(grad_output, axis=0)\n",
        "\n",
        "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
        "        # Here we perform a stochastic gradient descent step.\n",
        "        # or step of another gradient method\n",
        "        self.weights = self.weights - self.learning_rate * grad_weights\n",
        "        self.biases = self.biases - self.learning_rate * grad_biases\n",
        "\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "A1vN8h41ILY9"
      },
      "outputs": [],
      "source": [
        "class Conv2d(Layer):\n",
        "    def __init__(self, input_channels, output_channels, kernel_size, learning_rate=0.1):\n",
        "        self.weights = np.random.randn(input_channels, output_channels, kernel_size, kernel_size)*0.01\n",
        "        self.biases = np.zeros(output_channels)\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        self.grad_weights = None\n",
        "        self.grad_biases = None\n",
        "\n",
        "    def _conv(input, weights, biases):\n",
        "        kernel_height = np.shape(weights)[2]\n",
        "        kernel_width = np.shape(weights)[3]\n",
        "        output_chanels = np.shape(weights)[1]\n",
        "        # Output dimention is determined by the number of filters in the layer.\n",
        "        # Each filter is a tensor with the size (input_channels, kernel_size, kernel_size)\n",
        "        # (In this example second axis is an index of filter)\n",
        "        # Such a filter produces one number per convolution \n",
        "        #   and this convolution is performed for each part of the input tensor.\n",
        "        batch_size = np.shape(input)[0]\n",
        "        output_height = np.shape(input)[2] - kernel_height + 1\n",
        "        output_width = np.shape(input)[3] - kernel_width + 1\n",
        "\n",
        "        # This is the result of convollution of each filter\n",
        "        filters = np.empty((output_chanels, output_height, output_width, batch_size))\n",
        "        for filter_idx in range(output_chanels):\n",
        "            for i in range(output_height):\n",
        "                for j in range(output_width):\n",
        "                    filters[filter_idx, i, j] = \\\n",
        "                        np.tensordot(input[:, :, i:i+kernel_height, j:j+kernel_width],\n",
        "                                     weights[:, filter_idx, :, :],\n",
        "                                     axes=([1, 2, 3], [0, 1, 2]))\n",
        "        output = np.rollaxis(filters, 3, 0)\n",
        "        if biases is not None:\n",
        "        # Here values from biases are broadcasted\n",
        "            output += np.reshape(biases, (1, output_chanels, 1, 1))\n",
        "        return output\n",
        "\n",
        "    def _pad(tensor, val, new_height, new_width):\n",
        "        pad_h = (new_height - np.shape(tensor)[2]) // 2\n",
        "        pad_w = (new_width - np.shape(tensor)[3]) // 2\n",
        "        padding = ((0, 0), (0, 0), (pad_h, pad_h), (pad_w, pad_w))\n",
        "        res = np.pad(tensor, padding, 'constant', constant_values=(val, val))\n",
        "        assert np.shape(res)[2] == new_height and np.shape(res)[3] == new_width\n",
        "        return res\n",
        "    \n",
        "    def _flip_transpose(tensor):\n",
        "        tensor = np.flip(tensor, (2, 3))\n",
        "        tensor = np.moveaxis(tensor, 2, 3) # this should work\n",
        "        return tensor\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Perform an convolution:\n",
        "\n",
        "        output_height = input_height - kernel_size + 1\n",
        "        output_width = input_width - kernel_size + 1\n",
        "\n",
        "        input shape: [batch, input_channels, input_height, input_width]\n",
        "        output shape: [batch, output_channels, output_height, output_width]\n",
        "        \"\"\"\n",
        "        return Conv2d._conv(input, self.weights, self.biases)\n",
        "        \n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        # Grad output has the shape [batch, output_channels, output_height, output_width]\n",
        "        # I don't know how this is possible, but if has these axises, convolution just works\n",
        "        input = np.moveaxis(input, 0, 1)\n",
        "        grad_weights = Conv2d._conv(input, grad_output, None)\n",
        "        input = np.moveaxis(input, 0, 1)\n",
        "        # dldb = dldz * dzdb \n",
        "        # dzdb = 1\n",
        "        # dldz = grad_output\n",
        "        # Each biases' component is applied to each element with the fixed output_chanels axis,\n",
        "        #   so input gradient is the sum of all output gradients\n",
        "        grad_biases = np.sum(grad_output, axis=(0, 2, 3))\n",
        "\n",
        "        kernel_size = np.shape(self.weights)[2]\n",
        "        # Probably I shouldn't change input tensor, but whatever\n",
        "        grad_padded = Conv2d._pad(grad_output, 0,\n",
        "                                   np.shape(input)[2] + kernel_size - 1,\n",
        "                                   np.shape(input)[3] + kernel_size - 1)\n",
        "        tmp_weights = np.copy(self.weights)\n",
        "        tmp_weights = Conv2d._flip_transpose(tmp_weights)\n",
        "        tmp_weights = np.moveaxis(tmp_weights, 0, 1)\n",
        "        grad_input = Conv2d._conv(grad_padded, tmp_weights, None)\n",
        "\n",
        "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
        "\n",
        "        self.weights = self.weights - self.learning_rate * grad_weights\n",
        "        self.biases = self.biases - self.learning_rate * grad_biases\n",
        "\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test shapes:\n",
            "torch.Size([10, 2, 3, 4])\n",
            "(10, 2, 3, 4)\n"
          ]
        }
      ],
      "source": [
        "# Conv2D test\n",
        "from torch import nn\n",
        "import torch\n",
        "test_batch_size = 10\n",
        "test_input_chanels = 3\n",
        "test_output_chanels = 2\n",
        "test_input_height = 4\n",
        "test_input_width = 5\n",
        "test_kernel_size = 2\n",
        "\n",
        "test_conv = Conv2d(input_channels=test_input_chanels, \n",
        "                   output_channels=test_output_chanels, \n",
        "                   kernel_size=test_kernel_size)\n",
        "test_input = np.random.randn(test_batch_size, \n",
        "                             test_input_chanels, \n",
        "                             test_input_height,\n",
        "                             test_input_width)\n",
        "test_output = test_conv.forward(test_input)\n",
        "test_layer_torch = nn.Conv2d(in_channels=test_input_chanels,\n",
        "                     out_channels=test_output_chanels,\n",
        "                     kernel_size=(test_kernel_size, test_kernel_size))\n",
        "test_ref = test_layer_torch(torch.tensor(test_input, dtype=torch.float))\n",
        "print(\"Test shapes:\")\n",
        "print(test_ref.shape)\n",
        "print(test_output.shape)\n",
        "#assert torch.equal(torch.tensor(test_output, dtype=torch.float), test_ref)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shapes (10, 3, 4, 5), (10, 2, 3, 4)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[[[ 2.24702367e-05, -1.16830755e-05,  2.37812791e-05,\n",
              "           2.28300702e-04,  9.92701917e-05],\n",
              "         [-5.37874401e-05,  6.86330510e-04, -6.10457418e-04,\n",
              "           5.60375481e-05, -4.43312883e-04],\n",
              "         [ 2.95569568e-05,  2.37547187e-06, -5.99972479e-04,\n",
              "           4.45742165e-04, -4.36253578e-04],\n",
              "         [-5.54364400e-04, -3.62907787e-04, -1.64161757e-04,\n",
              "           2.89329017e-04, -9.36220411e-05]],\n",
              "\n",
              "        [[ 1.75075782e-05,  1.81104070e-05, -1.11406634e-04,\n",
              "           4.56588450e-04,  1.48581325e-04],\n",
              "         [-3.26136590e-05,  7.97330193e-04, -1.22660906e-03,\n",
              "           1.84797607e-03, -5.97651677e-04],\n",
              "         [ 1.06640711e-04,  6.25362387e-04,  7.07925046e-04,\n",
              "           1.21144829e-03, -2.20997219e-03],\n",
              "         [-2.27280149e-04,  1.37829692e-03,  1.07728708e-03,\n",
              "           4.20618478e-04, -1.08205335e-03]],\n",
              "\n",
              "        [[-6.28755586e-05, -3.62226713e-05,  2.95142848e-04,\n",
              "          -7.16575938e-04, -7.81467686e-05],\n",
              "         [-3.04218744e-04,  5.64512621e-04, -1.84229453e-03,\n",
              "           9.10464401e-04,  7.63468641e-04],\n",
              "         [-2.11442755e-04, -6.57357595e-04,  3.81674587e-05,\n",
              "           1.56995814e-03, -5.80450988e-04],\n",
              "         [-5.34443881e-04,  7.85356182e-04,  9.58941555e-04,\n",
              "           7.98967021e-04, -6.58303829e-04]]],\n",
              "\n",
              "\n",
              "       [[[ 3.49357230e-04,  7.96620186e-04,  1.15721078e-04,\n",
              "          -7.19973414e-04, -6.66569353e-04],\n",
              "         [-1.61792389e-03,  1.69782626e-04,  6.66606682e-04,\n",
              "           5.81754922e-04, -3.08466346e-05],\n",
              "         [ 7.37692757e-04, -5.09989224e-04,  1.61946725e-04,\n",
              "           1.40926401e-04,  5.08051570e-04],\n",
              "         [-1.92727938e-04, -5.47264576e-04, -4.84165383e-04,\n",
              "          -2.61871915e-05,  1.83002976e-04]],\n",
              "\n",
              "        [[ 3.32114445e-04,  1.39872174e-03,  1.14301067e-04,\n",
              "          -9.29337315e-04, -9.93140771e-04],\n",
              "         [-7.36275798e-04,  3.24953285e-03, -5.48904697e-04,\n",
              "          -5.19978330e-04, -9.64949788e-04],\n",
              "         [ 3.41843265e-04, -1.78597484e-03,  1.87162942e-03,\n",
              "           1.22937319e-03,  1.66981829e-03],\n",
              "         [-8.10122206e-05,  2.91077976e-04,  1.41136390e-03,\n",
              "           1.57688091e-03,  5.32443109e-04]],\n",
              "\n",
              "        [[-6.62256764e-04, -1.82807248e-03,  9.48899411e-04,\n",
              "           1.03166612e-03,  5.09220407e-04],\n",
              "         [-1.00226532e-03,  3.39856547e-03, -1.15855409e-03,\n",
              "          -2.54511387e-04, -9.66624944e-04],\n",
              "         [ 7.51380866e-04, -1.78759915e-03, -2.42754471e-04,\n",
              "          -1.08863642e-03,  4.74155903e-04],\n",
              "         [-2.10615822e-04, -2.97062997e-04,  4.18465830e-04,\n",
              "           6.83874769e-04,  7.08768919e-05]]],\n",
              "\n",
              "\n",
              "       [[[-3.70577181e-06, -2.84611757e-04, -3.14731440e-04,\n",
              "           3.66569628e-04,  3.01940910e-04],\n",
              "         [-6.93439402e-05,  6.93114798e-05, -9.54099099e-05,\n",
              "           4.22391261e-04,  3.31648234e-04],\n",
              "         [ 2.11984984e-04,  1.32470016e-03, -2.30081608e-04,\n",
              "           2.59920923e-04,  2.20139697e-04],\n",
              "         [-2.77844788e-04, -6.33689011e-04,  6.50779204e-04,\n",
              "          -2.30287078e-04,  5.15495586e-05]],\n",
              "\n",
              "        [[ 3.05451349e-05, -2.92752500e-04, -5.72908790e-04,\n",
              "           4.37175757e-04,  4.88322635e-04],\n",
              "         [-2.41440084e-05, -7.30041753e-05, -1.21294841e-03,\n",
              "           1.17957614e-03,  1.29407819e-03],\n",
              "         [ 4.88248781e-05,  1.06751315e-03, -1.85036607e-04,\n",
              "           3.02708744e-03,  8.98231816e-04],\n",
              "         [-1.33417703e-04,  3.89363245e-04,  2.87463290e-03,\n",
              "          -8.23585919e-04,  8.98340371e-04]],\n",
              "\n",
              "        [[ 1.86310416e-04,  2.37077590e-04,  1.57313928e-04,\n",
              "          -3.16368696e-04, -3.62102233e-04],\n",
              "         [ 1.78562365e-04,  8.86897522e-04, -7.68617452e-04,\n",
              "          -1.77172357e-03,  2.69662930e-04],\n",
              "         [-5.64119398e-04, -7.26078681e-04, -3.11811097e-03,\n",
              "           1.73970639e-03, -2.92674189e-05],\n",
              "         [-5.10230769e-04, -1.21363057e-03,  1.56269302e-03,\n",
              "          -1.03104489e-03,  5.94910771e-04]]],\n",
              "\n",
              "\n",
              "       [[[ 3.91784918e-05,  3.29938700e-04, -3.44674992e-04,\n",
              "           2.22519258e-04,  5.26724639e-04],\n",
              "         [ 7.33066048e-05,  8.99888478e-04, -4.57276950e-05,\n",
              "          -3.20717035e-04, -1.67903683e-04],\n",
              "         [-7.46883565e-04, -3.22599641e-04, -5.00206009e-04,\n",
              "           7.72438334e-04,  3.48337191e-05],\n",
              "         [ 8.62867139e-04, -1.08834110e-04, -2.31770801e-04,\n",
              "          -4.65609448e-04,  8.37589297e-05]],\n",
              "\n",
              "        [[-3.14510412e-05,  4.90214565e-04, -5.61700435e-04,\n",
              "           3.06733559e-04,  8.08750422e-04],\n",
              "         [ 1.04792634e-04,  8.35155625e-04, -6.34386181e-04,\n",
              "           1.09312927e-03,  6.67623116e-04],\n",
              "         [-4.62434554e-04,  9.37867394e-04,  1.18959721e-04,\n",
              "           1.82324128e-03, -9.98599336e-04],\n",
              "         [ 3.54226978e-04, -2.40429175e-03,  3.20544579e-05,\n",
              "           1.62711176e-04,  1.35309515e-03]],\n",
              "\n",
              "        [[-4.35785836e-04,  3.42921526e-04,  3.64763137e-04,\n",
              "          -8.33551561e-04, -4.84315224e-04],\n",
              "         [-3.58145540e-04, -6.93495684e-04, -1.20329477e-03,\n",
              "          -1.39058385e-04,  1.11968068e-03],\n",
              "         [-4.69534419e-04,  1.89077507e-03, -5.15144891e-04,\n",
              "           1.17341997e-03, -1.28968075e-03],\n",
              "         [ 8.37649346e-04, -1.66430246e-03,  1.19290468e-04,\n",
              "          -2.58998257e-04,  8.84765645e-04]]],\n",
              "\n",
              "\n",
              "       [[[ 3.23692590e-05,  8.44737626e-05, -6.34268487e-05,\n",
              "           2.69405821e-04,  2.10010933e-04],\n",
              "         [ 1.04252385e-04,  7.29415104e-04, -2.41425341e-04,\n",
              "          -5.56714927e-05,  3.10299751e-04],\n",
              "         [-8.09093016e-04,  7.44422055e-04, -2.00381682e-04,\n",
              "          -2.43404500e-04,  2.44816385e-04],\n",
              "         [-4.17276538e-04,  4.43362878e-05,  2.94647497e-04,\n",
              "          -1.04181363e-03,  3.42421087e-05]],\n",
              "\n",
              "        [[ 2.62529946e-05,  1.73988918e-04, -1.10689636e-04,\n",
              "           4.36100430e-04,  3.62013950e-04],\n",
              "         [ 1.43049094e-04,  1.43292940e-03, -7.20703086e-05,\n",
              "           1.18946681e-03,  1.21446537e-03],\n",
              "         [-2.91126612e-04,  3.11302980e-03, -7.17541764e-04,\n",
              "           7.29976886e-05,  1.12598500e-03],\n",
              "         [-1.75639087e-04,  1.14423805e-03,  3.18051760e-04,\n",
              "          -1.33303057e-03,  2.56430543e-03]],\n",
              "\n",
              "        [[-8.51403613e-05,  2.04985778e-05, -1.11319953e-04,\n",
              "          -5.90614472e-04, -3.28312350e-04],\n",
              "         [-6.99788570e-04, -9.06273829e-04, -2.33027952e-04,\n",
              "           2.68754764e-05,  2.63817430e-04],\n",
              "         [-1.24359336e-03,  1.17419455e-03, -5.83071101e-04,\n",
              "           2.42349194e-04, -7.13080218e-04],\n",
              "         [-4.58975263e-04,  6.70244147e-04,  6.11600132e-04,\n",
              "          -1.38566521e-03,  1.90681591e-03]]],\n",
              "\n",
              "\n",
              "       [[[-1.35835497e-04, -5.22181202e-04,  6.86203316e-05,\n",
              "           1.72726916e-04,  1.03018502e-04],\n",
              "         [ 3.45950471e-04, -3.97128489e-04, -8.64495562e-06,\n",
              "           1.09040415e-03,  4.61367703e-04],\n",
              "         [ 7.07356836e-04, -4.81644048e-04, -1.47689691e-04,\n",
              "          -3.00960897e-04, -1.03462049e-04],\n",
              "         [-2.13584423e-04, -4.08128469e-04,  1.79788320e-04,\n",
              "           2.81731240e-04, -1.24404175e-04]],\n",
              "\n",
              "        [[-9.76000423e-05, -8.17539105e-04,  1.05657992e-05,\n",
              "           3.04490033e-04,  4.67886708e-05],\n",
              "         [ 7.86212474e-05, -2.00447799e-03, -5.09652232e-07,\n",
              "           2.20217701e-03, -2.49904104e-05],\n",
              "         [ 3.12673065e-04, -1.61534629e-03,  2.21025283e-03,\n",
              "           1.47896288e-03,  8.79372536e-04],\n",
              "         [-9.28615959e-05,  3.84773658e-04,  1.44508109e-03,\n",
              "           1.32793667e-04, -8.38675867e-04]],\n",
              "\n",
              "        [[ 4.23431094e-04,  5.27864840e-04, -2.14952867e-06,\n",
              "          -5.30115481e-04,  2.86025541e-04],\n",
              "         [ 1.00876371e-03,  2.32290941e-05, -1.96222695e-03,\n",
              "          -1.93055898e-04, -1.12465574e-03],\n",
              "         [ 5.30956008e-04, -2.22277227e-03, -3.25025029e-05,\n",
              "           5.24240603e-04,  1.00077737e-03],\n",
              "         [-2.71708913e-04, -3.32642733e-04,  7.82863129e-04,\n",
              "           3.94416945e-04, -4.14437834e-04]]],\n",
              "\n",
              "\n",
              "       [[[ 9.32262761e-05, -2.38710630e-05,  2.00387642e-05,\n",
              "          -4.62784430e-04, -3.31562323e-04],\n",
              "         [-4.81058057e-04,  8.73076388e-04,  8.84256110e-04,\n",
              "           7.53966665e-04,  1.84719000e-04],\n",
              "         [ 4.19702294e-04, -8.07016695e-04, -9.22337401e-05,\n",
              "           5.79206157e-04,  1.09769688e-03],\n",
              "         [-6.17959059e-04,  2.96250393e-04,  1.11583944e-04,\n",
              "          -1.48216181e-03,  3.96947423e-04]],\n",
              "\n",
              "        [[ 1.11974261e-04,  7.13682995e-05, -1.16819224e-04,\n",
              "          -5.12072062e-04, -4.48171355e-04],\n",
              "         [-2.32496994e-04,  1.47110052e-03, -5.03624226e-04,\n",
              "           1.23850525e-03,  1.52825374e-04],\n",
              "         [ 2.32651563e-04, -4.20376454e-04,  3.52055270e-03,\n",
              "           1.79628099e-03,  2.15113907e-03],\n",
              "         [-2.64654087e-04,  1.75771444e-03, -1.37176105e-04,\n",
              "          -7.14062527e-04,  4.18164796e-03]],\n",
              "\n",
              "        [[-5.38465679e-05, -5.99464759e-04,  1.18524957e-03,\n",
              "           2.67660268e-04,  9.66292348e-05],\n",
              "         [-2.95484599e-04,  1.16160845e-03, -3.48563874e-03,\n",
              "          -2.44666114e-04, -1.25871335e-04],\n",
              "         [-1.79668256e-04, -2.63356211e-03,  1.53313281e-03,\n",
              "          -4.78019908e-04, -1.50591620e-03],\n",
              "         [-7.36175398e-04,  8.88571777e-04, -2.43313933e-04,\n",
              "          -1.73489279e-03,  2.47910823e-03]]],\n",
              "\n",
              "\n",
              "       [[[-7.33922591e-05, -5.60237645e-04, -1.16942197e-03,\n",
              "          -6.99285243e-04, -3.59004026e-04],\n",
              "         [ 2.01936657e-04,  8.05691940e-04,  2.10873757e-04,\n",
              "           3.20403221e-04,  1.83634123e-05],\n",
              "         [ 3.59799335e-04,  4.33621092e-04, -2.51898479e-04,\n",
              "           2.83213183e-04,  2.98281543e-04],\n",
              "         [-1.78261149e-04, -7.78637934e-05,  7.00670494e-05,\n",
              "          -2.45974068e-04,  9.07023730e-05]],\n",
              "\n",
              "        [[-5.41890394e-05, -6.49454190e-04, -1.74969940e-03,\n",
              "          -1.04599522e-03, -5.58425674e-04],\n",
              "         [ 5.05747600e-05, -6.64435849e-04, -3.13262021e-03,\n",
              "          -1.40117023e-03, -7.01598319e-04],\n",
              "         [ 1.50720687e-04, -3.38322019e-04, -2.83551519e-04,\n",
              "           1.72362119e-03,  8.00622694e-04],\n",
              "         [-7.99930695e-05,  4.17125305e-04,  5.61870603e-04,\n",
              "           2.71159945e-05,  1.03427745e-03]],\n",
              "\n",
              "        [[ 2.21121238e-04,  1.12006853e-03,  1.73597124e-03,\n",
              "           1.05245860e-03,  3.54706301e-04],\n",
              "         [ 4.95170739e-04,  1.46477792e-03, -8.39931321e-04,\n",
              "          -1.29103494e-03, -5.95736197e-04],\n",
              "         [ 7.84330581e-05, -7.17378030e-04, -1.19912836e-03,\n",
              "           4.95479634e-04, -2.31095357e-04],\n",
              "         [-2.57702449e-04, -1.35487973e-04,  6.96490288e-05,\n",
              "          -4.91184648e-04,  6.26994235e-04]]],\n",
              "\n",
              "\n",
              "       [[[ 1.21719856e-05, -3.30392247e-04, -6.75846911e-04,\n",
              "          -3.09394073e-04, -2.61474182e-04],\n",
              "         [-2.76192191e-04,  5.03736252e-04, -7.35078914e-04,\n",
              "           7.06763261e-04,  1.99585051e-04],\n",
              "         [ 8.26203874e-04,  6.94020598e-04, -6.31577367e-04,\n",
              "          -1.62378550e-04, -4.94772430e-04],\n",
              "         [ 1.30028034e-04,  5.36628842e-05, -2.62878215e-05,\n",
              "           5.70718456e-04, -2.88566569e-04]],\n",
              "\n",
              "        [[ 3.08775378e-05, -3.01517802e-04, -1.14480423e-03,\n",
              "          -3.23418108e-04, -4.22836063e-04],\n",
              "         [-2.30243931e-04, -1.97388183e-04, -3.71703886e-03,\n",
              "           1.01534407e-03, -4.04691707e-04],\n",
              "         [ 2.62770725e-04, -1.60748998e-03, -1.44033112e-03,\n",
              "           6.29358697e-04, -5.71892318e-04],\n",
              "         [ 4.08492880e-05, -3.91387062e-04,  4.15855044e-04,\n",
              "           1.86202717e-04, -1.70442382e-03]],\n",
              "\n",
              "        [[ 7.85272471e-05,  4.36333004e-04,  1.39069928e-03,\n",
              "           1.20267528e-04,  3.13433816e-04],\n",
              "         [ 2.57119979e-04,  2.72281492e-03, -1.75103211e-03,\n",
              "           1.03674684e-04, -5.72065110e-04],\n",
              "         [ 4.48676533e-04, -6.13745742e-04, -1.10801990e-03,\n",
              "           6.35855240e-04,  6.73301170e-04],\n",
              "         [-2.94656116e-05, -7.26628787e-04,  4.53381781e-04,\n",
              "           7.61240097e-04, -7.76201977e-04]]],\n",
              "\n",
              "\n",
              "       [[[-2.55786863e-04, -7.61676498e-04, -1.07089165e-03,\n",
              "          -7.78805369e-04, -2.65508632e-04],\n",
              "         [ 1.01774726e-03,  1.75968884e-04, -3.39213819e-05,\n",
              "          -2.16991698e-04,  5.33231787e-04],\n",
              "         [ 6.40819976e-05, -6.13351540e-04, -1.18797124e-04,\n",
              "          -1.12291911e-03, -1.17793336e-04],\n",
              "         [ 3.94350774e-04,  2.18125900e-04,  9.76385942e-05,\n",
              "           3.20925972e-04, -1.82645737e-04]],\n",
              "\n",
              "        [[-2.58896798e-04, -1.15919363e-03, -1.47946208e-03,\n",
              "          -1.13912395e-03, -4.96279307e-04],\n",
              "         [ 4.37651697e-04, -3.13523235e-03, -2.06518657e-03,\n",
              "          -2.21469532e-03, -4.27711304e-04],\n",
              "         [-2.23269143e-06, -1.12908544e-03, -2.52981025e-04,\n",
              "          -1.12457164e-03,  1.05848569e-03],\n",
              "         [ 1.68817053e-04, -9.55438611e-04, -9.56675011e-04,\n",
              "          -3.22663725e-04, -1.07949560e-03]],\n",
              "\n",
              "        [[ 4.02076730e-04,  1.96215377e-03,  1.34877215e-03,\n",
              "           8.27455605e-04,  5.47011695e-04],\n",
              "         [ 1.47140106e-03, -5.79849964e-04,  4.21030157e-04,\n",
              "          -6.44812765e-04, -1.50450087e-03],\n",
              "         [ 5.62342701e-04,  5.22478731e-04,  1.07296095e-03,\n",
              "          -1.29021365e-04,  1.15675218e-03],\n",
              "         [ 4.68895455e-04, -1.82049530e-04, -4.48548490e-04,\n",
              "           2.18478292e-04, -4.91824454e-04]]]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"Input shapes {np.shape(test_input)}, {np.shape(test_output)}\")\n",
        "test_conv.backward(test_input, test_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "p1iE85PpMm0Z"
      },
      "outputs": [],
      "source": [
        "class MaxPool2d(Layer):\n",
        "    def __init__(self, kernel_size):\n",
        "        self.kernel_size = kernel_size\n",
        "        self.max_elem_mask = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Perform an pooling:\n",
        "\n",
        "        output_height = input_height // kernel_size\n",
        "        output_width = input_width // kernel_size\n",
        "\n",
        "        input shape: [batch, input_channels, input_height, input_width]\n",
        "        output shape: [batch, input_channels, output_height, output_width]\n",
        "        \"\"\"\n",
        "        type = input.dtype\n",
        "        kernel_size = self.kernel_size\n",
        "        batch = np.shape(input)[0]\n",
        "        input_chanels = np.shape(input)[1]\n",
        "        output_height = np.shape(input)[2]\n",
        "        output_width = np.shape(input)[3]\n",
        "        output = np.empty((output_height, output_width, batch, input_chanels))\n",
        "        self.max_elem_mask = np.empty((batch, input_chanels, np.shape(input)[2], np.shape(input)[3]))\n",
        "        for i in range(output_height):\n",
        "            for j in range(output_width):\n",
        "                input_slice = input[:, :, i:i+kernel_size, j:j+kernel_size]\n",
        "                max_tensor = np.max(input_slice, axis=(2, 3))\n",
        "# How should I handlel such indexes convertions?\n",
        "                output[i, j] = max_tensor\n",
        "                self.max_elem_mask[:, :, i:i+kernel_size, j:j+kernel_size] = \\\n",
        "                    np.equal(input_slice, np.reshape(max_tensor, \n",
        "                                                     (np.shape(max_tensor)[0],\n",
        "                                                      np.shape(max_tensor)[1],\n",
        "                                                     1, 1)))\n",
        "        return np.moveaxis(output, [0, 1], [2, 3])\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        type = input.dtype\n",
        "        kernel_size = self.kernel_size\n",
        "        grad_input = self.max_elem_mask\n",
        "        for i in range(np.shape(grad_output)[2]):\n",
        "            for j in range(np.shape(grad_output)[3]):\n",
        "                grad_input[:, :, i:i+kernel_size, j:j+kernel_size] = \\\n",
        "                    np.multiply(grad_output[:, :, i:i+1, j:j+1], \n",
        "                                self.max_elem_mask[:, :, i:i+kernel_size, j:j+kernel_size],\n",
        "                                dtype=type)\n",
        "        self.max_elem_mask = None\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10, 3, 4, 5)\n",
            "input_shape (10, 3, 4, 5) grad shape: (10, 3, 4, 5)\n"
          ]
        }
      ],
      "source": [
        "test_maxpool = MaxPool2d(2)\n",
        "test_output = test_maxpool.forward(test_input)\n",
        "print(np.shape(test_maxpool.max_elem_mask))\n",
        "test_backward = test_maxpool.backward(test_input, test_output)\n",
        "print(f\"input_shape {np.shape(test_input)} grad shape: {np.shape(test_backward)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W-GM45xSqJD"
      },
      "outputs": [],
      "source": [
        "class Flatten(Layer):\n",
        "    def forward(self, input):\n",
        "          \"\"\"\n",
        "          Perform an flatten operation:\n",
        "\n",
        "          input shape: [batch, input_channels, input_height, input_width]\n",
        "          output shape: [batch, input_channels * output_height * output_width]\n",
        "          \"\"\"\n",
        "          #<your code here>\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        #grad_input = <your code here>\n",
        "\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iCuNsUikOoE1",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def softmax_crossentropy_with_logits(logits,reference_answers):\n",
        "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
        "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
        "\n",
        "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
        "\n",
        "    return xentropy\n",
        "\n",
        "def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n",
        "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
        "    ones_for_answers = np.zeros_like(logits)\n",
        "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
        "\n",
        "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
        "\n",
        "    return (- ones_for_answers + softmax) / logits.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muokQIA3UcnL"
      },
      "source": [
        "## Имплементация оптимизатора и изменения learning rate'a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBzMVan_XzFM"
      },
      "source": [
        "В имплементации этих двух классов есть небольшие неточности.\n",
        "Посмотрите, как сделана имплементация метода моментов в Pytorch и добавьте пропущенное.\n",
        "\n",
        "> Добавлять моменты Нестерова не нужно!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4hyzuIcUqLd"
      },
      "outputs": [],
      "source": [
        "class SGDOptimizer:\n",
        "    def __init__(self, momentum=0.9, dampening=0.0, weight_decay=0.0):\n",
        "        \"\"\"\n",
        "        Wrapper which perfoms weights update\n",
        "        \"\"\"\n",
        "        self.momentum = momentum\n",
        "        self.dampening = dampening\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        # здесь будем копить моментумы\n",
        "        self.momentum_buffer = 0\n",
        "\n",
        "    def step(self, weights, grad_weights, lr=0.1):\n",
        "      \"\"\"\n",
        "      Update weights\n",
        "      \"\"\"\n",
        "      self.momentum_buffer = self.momentum * self.momentum_buffer + (1 - self.dampening) * grad_weights\n",
        "\n",
        "      #<your code here>\n",
        "\n",
        "      return weights - lr * self.momentum_buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DX-leurYblp"
      },
      "outputs": [],
      "source": [
        "class LRScheduler:\n",
        "    def __init__(self, lr):\n",
        "        \"\"\"\n",
        "        Wrapper which perfoms learning rate updates\n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "        self.current_step = 0\n",
        "\n",
        "    def get_lr(self, step):\n",
        "      \"\"\"\n",
        "      Update learing rate for current iteration\n",
        "      \"\"\"\n",
        "      current_lr = self.lr / (self.current_step + 1)\n",
        "      #<your code here>\n",
        "      self.current_step += 1\n",
        "      return current_lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlEURTE9OoE5",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Итоговая нейросеть\n",
        "\n",
        "Все готово для запуска нейросети. Нейросеть будем тестировать на классическом датасете MNIST. Код ниже визуализирует несколько примеров из этого датасета."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KKQ81e0OoE6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist(flatten=False)\n",
        "\n",
        "plt.figure(figsize=[6,6])\n",
        "for i in range(4):\n",
        "    plt.subplot(2,2,i+1)\n",
        "    plt.title(\"Label: %i\"%y_train[i])\n",
        "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGXtnvX4OoE7",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "В нашей реализации сеть - просто список (Python-list) слоев.\n",
        "\n",
        "\n",
        "\n",
        "> Обратите внимание, что у нас нет глобального пулинга. При изменении архитектуры сети вы должны поменять входую размерность в последнем Dense слое\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VicezF_TOoE8",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "network = []\n",
        "network.append(Conv2d(1, 4, 5))\n",
        "network.append(MaxPool2d(2))\n",
        "network.append(ReLU())\n",
        "network.append(Conv2d(4, 8, 5))\n",
        "network.append(MaxPool2d(2))\n",
        "network.append(ReLU())\n",
        "network.append(Flatten())\n",
        "network.append(Dense(5 * 5 * 8, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sUrd667ZDuK"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "\n",
        "optimizer = SGDOptimizer()\n",
        "scheduler = LRScheduler(learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "482hIf_UOoE9",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Реализуйте прямой проход по целой сети, последовательно вызывая .forward() для каждого слоя."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKRyUyj5OoE9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def forward(network, X):\n",
        "    \"\"\"\n",
        "    Compute activations of all network layers by applying them sequentially.\n",
        "    Return a list of activations for each layer.\n",
        "    Make sure last activation corresponds to network logits.\n",
        "    \"\"\"\n",
        "    activations = []\n",
        "    input = X\n",
        "\n",
        "    # <your code here>\n",
        "\n",
        "    assert len(activations) == len(network)\n",
        "    return activations\n",
        "\n",
        "def predict(network, X):\n",
        "    \"\"\"\n",
        "    Use network to predict the most likely class for each sample.\n",
        "    \"\"\"\n",
        "    logits = forward(network, X)[-1]\n",
        "    return logits.argmax(axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfUyHKPyOoE-",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def train(network,X,y):\n",
        "    \"\"\"\n",
        "    Train your network on a given batch of X and y.\n",
        "    You first need to run forward to get all layer activations.\n",
        "    Then you can run layer.backward going from last to first layer.\n",
        "\n",
        "    After you called backward for all layers, all Dense layers have already made one gradient step.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the layer activations\n",
        "    layer_activations = forward(network,X)\n",
        "    layer_inputs = [X] + layer_activations  #layer_input[i] is an input for network[i]\n",
        "    logits = layer_activations[-1]\n",
        "\n",
        "    # Compute the loss and the initial gradient\n",
        "    loss = softmax_crossentropy_with_logits(logits,y)\n",
        "    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
        "\n",
        "    # propagate gradients through network layers using .backward\n",
        "    # hint: start from last layer and move to earlier layers\n",
        "    #<YOUR CODE>\n",
        "\n",
        "    # update weights and biases with optimizer\n",
        "    #<YOUR CODE>\n",
        "\n",
        "    # update learning rate\n",
        "    #<YOUR CODE>\n",
        "\n",
        "    return np.mean(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJG4VMsROoE_",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Все готово для запуска обучения. Если все реализовано корректно, то точность классификации на валидационном множестве **должна быть около** 99%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kq5XTDNNOoE_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
        "    assert len(inputs) == len(targets)\n",
        "    if shuffle:\n",
        "        indices = np.random.permutation(len(inputs))\n",
        "    for start_idx in tqdm(range(0, len(inputs) - batchsize + 1, batchsize)):\n",
        "        if shuffle:\n",
        "            excerpt = indices[start_idx:start_idx + batchsize]\n",
        "        else:\n",
        "            excerpt = slice(start_idx, start_idx + batchsize)\n",
        "        yield inputs[excerpt], targets[excerpt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q2KcwkTOoFA",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "train_log = []\n",
        "val_log = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaxQu9WsOoFB",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "for epoch in range(15):\n",
        "\n",
        "    for x_batch,y_batch in iterate_minibatches(X_train, y_train, batchsize=32, shuffle=True):\n",
        "        train(network, x_batch, y_batch)\n",
        "\n",
        "    train_log.append(np.mean(predict(network, X_train) == y_train))\n",
        "    val_log.append(np.mean(predict(network, X_val) == y_val))\n",
        "\n",
        "    clear_output()\n",
        "    print(\"Epoch\",epoch)\n",
        "    print(\"Train accuracy:\",train_log[-1])\n",
        "    print(\"Val accuracy:\",val_log[-1])\n",
        "    plt.plot(train_log,label='train accuracy')\n",
        "    plt.plot(val_log,label='val accuracy')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
